{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:13:09.338880Z",
     "start_time": "2024-07-24T05:13:04.308473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import Point, LineString\n",
    "# from geopandas import GeoDataFrame\n",
    "from Code.general_functions import *\n",
    "from sklearn.cluster import DBSCAN\n",
    "from momepy import extend_lines\n",
    "import os\n",
    "\n",
    "project_crs = 'epsg:3857'\n",
    "\n",
    "place ='Turin_Italy'\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "data_folder= os.path.join(pjr_loc, f'places/{place}')\n",
    "test_data_folder = os.path.join(data_folder, 'test/Aggregation')\n",
    "\n",
    "# if you want to save the files\n",
    "def save_points_file(data, path):\n",
    "    \"\"\"\n",
    "    The function get a data, arrange columns, convert list to string and export  it into a shpfile\n",
    "    :param data:\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    col_of_lists_as_str = 'col_of_lists_as_str'\n",
    "    data[col_of_lists_as_str] = data[index_right].apply(str)\n",
    "    data.drop(columns=[index_right]).to_file(path)\n",
    "    data.drop(columns=[col_of_lists_as_str], inplace=True)"
   ],
   "id": "b455c5742a2a6268",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T04:35:48.350854Z",
     "start_time": "2024-07-24T04:35:45.906957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggregation\n",
    "print('Aggregate intersections')\n",
    "network_0 = gpd.read_file(f'{data_folder}/network.shp')\n",
    "network_0"
   ],
   "id": "87cfa1be23e8f7d7",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T04:36:55.767359Z",
     "start_time": "2024-07-24T04:36:55.746335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aggregate only streets have been simplified\n",
    "network = network_0[network_0['is_simplif']==1]\n",
    "network_not_simplified = network_0[network_0['is_simplif']==0]"
   ],
   "id": "c09744ef1055fbc1",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T04:41:37.544748Z",
     "start_time": "2024-07-24T04:41:34.891530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Get the first/start of each line\n",
    "# Extract unique start and end points from all LineStrings\n",
    "geometry = 'geometry'\n",
    "index_right = 'index_right'\n",
    "all_points = network[geometry].apply(lambda line: [Point(line.coords[0]), Point(line.coords[-1])]).explode()\n",
    "# # Create a GeoSeries of unique points\n",
    "unique_points = GeoDataFrame(geometry=gpd.GeoSeries(all_points).unique(), crs=project_crs)\n",
    "# save data\n",
    "\n",
    "# 2. Make sure I have the name of the lines associated with these lines\n",
    "pnts_line_name = unique_points.sjoin(network)[[index_right, geometry]].reset_index().dissolve(by='index',\n",
    "                                                                                              aggfunc=lambda\n",
    "                                                                                                  x: x.tolist())\n",
    "pnts_line_name['num_of_lines'] = pnts_line_name[index_right].apply(len)  # count the number of lines for each point\n",
    "\n",
    "# 3. Use DBSCAN with 20 meters threshold\n",
    "# Extract coordinates for DBSCAN\n",
    "coordinates = pnts_line_name.geometry.apply(lambda point: (point.x, point.y)).tolist()\n",
    "dbscan = DBSCAN(eps=40, min_samples=2)\n",
    "pnts_line_name['group'] = dbscan.fit_predict(coordinates)\n",
    "lines_to_update = pnts_line_name[pnts_line_name['group'] > -1]\n",
    "\n",
    "\n",
    "# if you want to save the files\n",
    "def save_points_file(data, path):\n",
    "    \"\"\"\n",
    "    The function get a data, arrange columns, convert list to string and export  it into a shpfile\n",
    "    :param data:\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    col_of_lists_as_str = 'col_of_lists_as_str'\n",
    "    data[col_of_lists_as_str] = data[index_right].apply(str)\n",
    "    data.drop(columns=[index_right]).to_file(path)\n",
    "    data.drop(columns=[col_of_lists_as_str], inplace=True)\n",
    "\n",
    "\n",
    "# 4.1.Find the point with the max number of connected lines, if it is one use it otherwise uses the average\n",
    "# Find the maximum 'num' value for each group\n",
    "num = 'num_of_lines'\n",
    "group_name = 'group'\n",
    "new_geometry = 'new_geometry'\n",
    "max_values_per_group = lines_to_update.groupby('group')['num_of_lines'].max()\n",
    "# Filter rows with the maximum 'num' value for each group\n",
    "result_gdf = lines_to_update[\n",
    "    lines_to_update.set_index([group_name, num]).index.isin(list(max_values_per_group.items()))]\n",
    "\n",
    "\n",
    "# Custom aggregation function to calculate the average point for each group\n",
    "def calculate_average_point(group):\n",
    "    x_mean = group.x.mean()\n",
    "    y_mean = group.y.mean()\n",
    "    return Point(x_mean, y_mean)\n",
    "\n",
    "\n",
    "# Apply the custom aggregation function to calculate average points per group\n",
    "lines_to_update2 = lines_to_update.set_index(group_name)\n",
    "lines_to_update2['new_geometry'] = result_gdf.groupby(group_name)[geometry].apply(calculate_average_point)\n",
    "\n",
    "# 4.2 Among whom are updated remove every line the start and last point are the same\n",
    "# Get all the lines going to be deleted\n",
    "lines_to_delete = []\n",
    "\n",
    "\n",
    "def update_lines_to_delete(row):\n",
    "    # explode the lines names within each row list to separate rows\n",
    "    lines_to_update_tmep = row[index_right].explode()\n",
    "\n",
    "    # Identify rows with duplicate values\n",
    "    lines_to_delete.extend(lines_to_update_tmep[lines_to_update_tmep.duplicated()].tolist())\n",
    "\n",
    "\n",
    "lines_to_update2.groupby(level=group_name).apply(update_lines_to_delete)\n",
    "\n",
    "# remove lines their geometry not going to change\n",
    "lines_to_update3 = lines_to_update2[lines_to_update2[geometry] != lines_to_update2[new_geometry]]\n",
    "\n",
    "# 4.3 Change the point of each line with new point\n",
    "network_new = network[~network.index.isin(lines_to_delete)]\n",
    "\n",
    "\n",
    "def update_network_with_aggregated_point(group):\n",
    "    lines_in_group = group.explode(index_right)\n",
    "\n",
    "    def update_one_line(points_data):\n",
    "        if points_data.name not in lines_to_delete:\n",
    "            updated_line_geo = network_new.loc[points_data.name]\n",
    "            line_coords = updated_line_geo.geometry.coords\n",
    "            if Point(line_coords[0]) == points_data.geometry:\n",
    "                network_new.at[points_data.name, geometry] = LineString(\n",
    "                    [points_data[new_geometry]] + line_coords[1:])\n",
    "            elif Point(line_coords[-1]) == points_data.geometry:\n",
    "                network_new.at[points_data.name, geometry] = LineString(\n",
    "                    line_coords[:-1] + [points_data[new_geometry]])\n",
    "            else:\n",
    "                print(points_data)\n",
    "                print(lines_in_group)\n",
    "\n",
    "    lines_in_group.set_index(index_right).apply(update_one_line, axis=1)\n",
    "\n",
    "\n",
    "lines_to_update3.groupby(level=group_name).apply(update_network_with_aggregated_point)"
   ],
   "id": "b0c13d99bfb690d5",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T04:42:03.128245Z",
     "start_time": "2024-07-24T04:42:01.556436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "combined_gdf = pd.concat([network_new, network_not_simplified], ignore_index=True)\n",
    "\n",
    "combined_gdf.to_file(f'{test_data_folder}/network_new.shp')"
   ],
   "id": "94edbde9d3854f8f",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:32:17.017011Z",
     "start_time": "2024-07-24T05:31:48.034432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extend_lines_f = extend_lines(combined_gdf, 100)\n",
    "extend_lines_f['length'] = extend_lines_f.length\n",
    "\n",
    "obj_intersection_1 = Intersection(extend_lines_f.copy(), 1)\n",
    "obj_intersection_1.intersection_network()\n",
    "obj_intersection_1.my_network.rename(columns={'str_name': 'name'},\n",
    "                                     inplace=True)  # 'str_name' become 'str to be compatible with other previous networks\n",
    "obj_intersection_1.update_names(org_gpd=extend_lines_f.copy().rename(columns={'str_name': 'name'}))\n",
    "\n",
    "# Clear short segments\n",
    "final2 = EnvEntity(obj_intersection_1.my_network.reset_index())\n",
    "final2.update_the_current_network(final2.get_deadend_gdf(delete_short=100))\n",
    "combined_gdf.to_file(f'{test_data_folder}/network_new2.shp')"
   ],
   "id": "e5b82361c87563e",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "cb7eed4785924926",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
