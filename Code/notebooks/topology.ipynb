{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import pickle\n",
    "project_crs = 'epsg:3857'\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "from shapely.geometry import  Point, LineString, MultiPolygon, MultiPoint\n",
    "from momepy import remove_false_nodes\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Choose locations and create folders if necessary\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder = f'{pjr_loc}/places/{place.replace(\",\", \"_\").replace(\" \", \"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes', exist_ok=True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection', exist_ok=True)\n",
    "if place == 'Tel Aviv':\n",
    "    useful_tags_path = ['name:en', 'highway', 'length', 'bearing', 'tunnel', 'junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "# find and store roundabout\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',layer = 'edges')# Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=True)\n",
    "is_junction= True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf= my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "# region\n",
    "# Classes to be employed during the execution of this code.\n",
    "#Intersection\n",
    "#Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self,network:GeoDataFrame,number:int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry':[],'name':[]}\n",
    "        self.lines_to_delete =[]\n",
    "        self.num = number\n",
    "    def delete_false_intersection(self,name_to_splt='name'):\n",
    "\n",
    "        if 'length' in self.my_network.columns: # To run the code smoothly we need to remove 'length' col if exist\n",
    "            self.my_network.drop(columns='length',inplace= True)\n",
    "\n",
    "        # It should be executed twice in order to clean all\n",
    "        for _  in range(2):\n",
    "            # First clean all the false node\n",
    "            self.my_network = remove_false_nodes(self.my_network)\n",
    "            # the previous function has changed the topology so the length should be updated\n",
    "            self.my_network['length'] =self.my_network.length\n",
    "            self.my_network  =self.my_network.drop_duplicates(subset='length') # remove false intersection duplicate many polyline which should be removed\n",
    "            self.my_network.reset_index(drop=True,inplace= True) # Changes has been made to the geometry so the index should be reset\n",
    "\n",
    "\n",
    "    def intersection_network(self):\n",
    "\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines= self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines,crs=project_crs),right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index!=s_join_0['index_right']]\n",
    "\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time =len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x,pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic)==0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic,crs=project_crs)\n",
    "\n",
    "        # Split string line by points\n",
    "        segments = {'geometry':[],'org_id':[]}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups =  inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups :\n",
    "                pbar.update(1)\n",
    "                points  = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = points_line_gdf.append(points)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg =[]\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg)>1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments,crs=project_crs)\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] =self.my_network[cols_no_geometry]\n",
    "\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split =self.my_network.drop(index=network_split_final.index.unique()).append(network_split_final).drop(index= self.lines_to_delete)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True,inplace= True)\n",
    "\n",
    "    def find_intersection_points(self,row,pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 =  self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,LineString):\n",
    "                return\n",
    "            if isinstance(pnt,MultiPoint):\n",
    "                temp_line= line_1.name if line_1.length< line_2.length else line_2.name\n",
    "                if temp_line not in self.lines_to_delete:\n",
    "                    self.lines_to_delete.append(temp_line)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords)==0 or pnt.coords[0]==line_1.geometry.coords[0] or pnt.coords[0]==line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "    def update_names(self, org_gpd:GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        # df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "        # df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index  ='old_index'\n",
    "\n",
    "        df = gpd.sjoin(df4, org_gpd).reset_index(names='old_index')\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = {}\n",
    "        # Define the relevant columns to store\n",
    "        rel_col  =[col for col in df.columns if col.endswith(\"right\")]+['geometry']\n",
    "        rel_col.remove('index_right')\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name_right'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names)==1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1]- count_names[0]>1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name_right'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name_right'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "                streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data[my_data['old_index']] = my_data[rel_col].to_list()\n",
    "        _ =df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(index=dic_str_data.keys(), data= dic_str_data.values(),columns=[x.replace('_right', '',) for x in rel_col],crs=project_crs)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = df3.append(updated_df)\n",
    "\n",
    "#Roundabout\n",
    "class EnvEntity:\n",
    "        def __init__(self,network):\n",
    "            self.dead_end_fd = None\n",
    "            self.pnt_dead_end = None\n",
    "            self.pnt_dic = {}\n",
    "            self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "            self.network = network\n",
    "\n",
    "\n",
    "        def __populate_pnt_dic(self,point: type, name_of_line: str):\n",
    "            \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if not point in self.pnt_dic:\n",
    "                self.pnt_dic[point] = []\n",
    "            self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "        def __send_pnts(self,temp_line: GeoSeries):\n",
    "            \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            my_geom = temp_line['geometry']\n",
    "            self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "            self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "        def get_deadend_gdf(self,delete_short:int =30)-> GeoDataFrame:\n",
    "            self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "            deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "            pnt_dead_end_0 = [item for item in self.pnt_dic.items() if len(item[1]) == 1] # Retain all the line points with deadened\n",
    "            self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "            # Create shp file of deadened_pnts\n",
    "            geometry,line_name = 'geometry','line_name'\n",
    "            pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "            pnt_dead_end_df[geometry]= pnt_dead_end_df[0].apply(lambda x:Point(x))\n",
    "            pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x:x[0])\n",
    "            pnt_dead_end_df.crs = project_crs\n",
    "            self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "            if delete_short>0:\n",
    "                # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "                deadend_gdf =self.network.loc[deadend_list]\n",
    "                self.network.drop(index=deadend_gdf[deadend_gdf.length<delete_short].index,inplace=True)\n",
    "                return deadend_gdf[deadend_gdf.length>delete_short]\n",
    "            return self.network.loc[deadend_list]\n",
    "\n",
    "        def update_the_current_network(self,temp_network):\n",
    "            r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            new_network_temp = self.network.drop(index=temp_network.index)\n",
    "            self.network = new_network_temp.append(temp_network)\n",
    "            self.network['length'] = self.network.length\n",
    "            self.network  = self.network[self.network['length']>1]\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self,network: GeoDataFrame):\n",
    "       EnvEntity.__init__(self,network)\n",
    "       self.pnt_dic ={}\n",
    "       self.centroid =self.__from_roundabout_to_centroid()\n",
    "       self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type=='Polygon': # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid =GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "        # GeoDataFrame(dic_data,crs=project_crs).to_file(f'{path_round_about}/roundabout_union.shp')\n",
    "\n",
    "    def __first_last_pnt_of_line(self,row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "    def __update_geometry(self,cur,s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows()if item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                0]\n",
    "        return LineString(geo_cur)\n",
    "    def my_spatial_join(self,deadend_lines, deadend_pnts,line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "        # centroid = gpd.read_file(f'{path_round_about}/centroid.shp')\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                   distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique() # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side =deads_both_side[deads_both_side==2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete=deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length<300))]\n",
    "        deadend_lines = deadend_lines[~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length<300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x:self.__update_geometry(x,s_join), axis=1)\n",
    "\n",
    "        return change_geo\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                         name      highway  bearing  group  is_simplified  \\\n0                          11         path   124.50     -1              0   \n1                          11         path   124.50     -1              0   \n2                          11         path   124.50     -1              0   \n3                          14  residential   106.10     -1              0   \n4                          18         path    40.85     -1              0   \n...                       ...          ...      ...    ...            ...   \n14562      Vicolo Santa Maria   pedestrian    25.20     -1              0   \n14563         Vicolo Valtorta  residential    26.50     -1              0   \n14564       Viottolo del Cral      footway    48.40     -1              0   \n14565   strada alle sei ville      footway    96.10     -1              0   \n14566  via Edoardo Perroncito      service    26.90     -1              0   \n\n           lenght                                           geometry  \n0      563.914540  LINESTRING (857888.020 5626761.570, 857902.213...  \n1      309.186141  LINESTRING (857638.976 5626560.639, 857618.671...  \n2      291.082651  LINESTRING (854914.954 5625472.999, 854917.670...  \n3      237.840926  LINESTRING (856136.574 5627425.278, 856138.912...  \n4      194.525862  LINESTRING (860173.665 5627099.066, 860141.427...  \n...           ...                                                ...  \n14562   53.578238  LINESTRING (854753.997 5632849.499, 854774.402...  \n14563   62.615859  LINESTRING (854694.542 5629305.469, 854722.494...  \n14564  108.529998  LINESTRING (859390.699 5636638.352, 859320.123...  \n14565  264.079574  LINESTRING (857998.760 5631175.272, 858020.668...  \n14566  269.364144  LINESTRING (846757.429 5633375.691, 846856.748...  \n\n[14567 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>highway</th>\n      <th>bearing</th>\n      <th>group</th>\n      <th>is_simplified</th>\n      <th>lenght</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>563.914540</td>\n      <td>LINESTRING (857888.020 5626761.570, 857902.213...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>309.186141</td>\n      <td>LINESTRING (857638.976 5626560.639, 857618.671...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>291.082651</td>\n      <td>LINESTRING (854914.954 5625472.999, 854917.670...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14</td>\n      <td>residential</td>\n      <td>106.10</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>237.840926</td>\n      <td>LINESTRING (856136.574 5627425.278, 856138.912...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18</td>\n      <td>path</td>\n      <td>40.85</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>194.525862</td>\n      <td>LINESTRING (860173.665 5627099.066, 860141.427...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14562</th>\n      <td>Vicolo Santa Maria</td>\n      <td>pedestrian</td>\n      <td>25.20</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>53.578238</td>\n      <td>LINESTRING (854753.997 5632849.499, 854774.402...</td>\n    </tr>\n    <tr>\n      <th>14563</th>\n      <td>Vicolo Valtorta</td>\n      <td>residential</td>\n      <td>26.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>62.615859</td>\n      <td>LINESTRING (854694.542 5629305.469, 854722.494...</td>\n    </tr>\n    <tr>\n      <th>14564</th>\n      <td>Viottolo del Cral</td>\n      <td>footway</td>\n      <td>48.40</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>108.529998</td>\n      <td>LINESTRING (859390.699 5636638.352, 859320.123...</td>\n    </tr>\n    <tr>\n      <th>14565</th>\n      <td>strada alle sei ville</td>\n      <td>footway</td>\n      <td>96.10</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>264.079574</td>\n      <td>LINESTRING (857998.760 5631175.272, 858020.668...</td>\n    </tr>\n    <tr>\n      <th>14566</th>\n      <td>via Edoardo Perroncito</td>\n      <td>service</td>\n      <td>26.90</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>269.364144</td>\n      <td>LINESTRING (846757.429 5633375.691, 846856.748...</td>\n    </tr>\n  </tbody>\n</table>\n<p>14567 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "new_network = gpd.read_file(f'{data_folder}/simplification/simp.shp').rename(columns={'is_simplif':'is_simplified'})\n",
    "new_network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n",
    "obj_intersection.delete_false_intersection()\n",
    "obj_intersection.my_network.reset_index().to_file(f'{data_folder}/delete_2_nodes/delete_false_intersection.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37215/37215 [00:06<00:00, 6003.11it/s]\n",
      "100%|██████████| 1787/1787 [00:10<00:00, 173.57it/s]\n"
     ]
    }
   ],
   "source": [
    "obj_intersection.intersection_network()\n",
    "obj_intersection.my_network.reset_index().to_file(f'{data_folder}/delete_2_nodes/intersection_network.shp')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Test update_names\n",
    "\n",
    "\n",
    "org_gpd= new_gpd\n",
    "df1 = obj_intersection.my_network\n",
    "\n",
    "\n",
    "# Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "df3 = df1[df1['name'].notna()]\n",
    "# df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "df4 = df1[df1['name'].isna()]\n",
    "# df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "# use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "old_index  ='old_index'\n",
    "\n",
    "df = gpd.sjoin(df4, org_gpd,predicate='crosses').reset_index(names='old_index')\n",
    "# Create a new dictionary to store the updated data.\n",
    "dic_str_data = {}\n",
    "# Define the relevant columns to store\n",
    "rel_col  =[col for col in df.columns if col.endswith(\"right\")]+['geometry']\n",
    "rel_col.remove('index_right')\n",
    "\n",
    "def return_street_name(aplcnts_tst):\n",
    "    \"\"\"\n",
    "    1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "    2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "    3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "    4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "    :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_names = aplcnts_tst['name_right'].value_counts().sort_values(ascending=False)\n",
    "    if len(count_names)==1:\n",
    "        # there is only one name\n",
    "        my_data = aplcnts_tst.iloc[0]\n",
    "    elif count_names[1]- count_names[0]>1:\n",
    "        # The highest number of polylines with the same name are bigger at least in 2:\n",
    "        my_data = aplcnts_tst[aplcnts_tst['name_right'] == count_names.index[0]].iloc[0]\n",
    "    else:\n",
    "        # otherwise filter those with the most popular name or close to (-1)\n",
    "        str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name_right'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "        buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "        streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "        buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "        streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "        groupy = streets_right_geo.groupby('name')\n",
    "        my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "        # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "        my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "    # Populate the new dictionary with relevant data\n",
    "    dic_str_data[my_data['old_index']] = my_data[rel_col].to_list()\n",
    "df.reset_index().to_file(f'{data_folder}/delete_2_nodes/s_join_with_org_gpd.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "       old_index name_left highway_left  bearing_left  group_left  \\\n19948      10707       NaN          NaN           NaN         NaN   \n\n       is_simplified_left  lenght_left      length  \\\n19948                 NaN          NaN  172.431015   \n\n                                                geometry  index_right  \\\n19948  LINESTRING (855475.810 5631053.967, 855630.183...         9411   \n\n                name_right highway_right  bearing_right  group_right  \\\n19948  Via Madama Cristina      tertiary       26.35641           -1   \n\n       is_simplified_right  lenght_right  \n19948                    0     14.681307  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>old_index</th>\n      <th>name_left</th>\n      <th>highway_left</th>\n      <th>bearing_left</th>\n      <th>group_left</th>\n      <th>is_simplified_left</th>\n      <th>lenght_left</th>\n      <th>length</th>\n      <th>geometry</th>\n      <th>index_right</th>\n      <th>name_right</th>\n      <th>highway_right</th>\n      <th>bearing_right</th>\n      <th>group_right</th>\n      <th>is_simplified_right</th>\n      <th>lenght_right</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19948</th>\n      <td>10707</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>172.431015</td>\n      <td>LINESTRING (855475.810 5631053.967, 855630.183...</td>\n      <td>9411</td>\n      <td>Via Madama Cristina</td>\n      <td>tertiary</td>\n      <td>26.35641</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>14.681307</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_old_index = df.groupby(old_index)\n",
    "one_example = groupby_old_index.get_group(10707)\n",
    "count_names = one_example['name_right'].value_counts().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "obj_intersection.update_names(new_gpd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test intersection - update names\n",
    "# Start from here\n",
    "with open(f'{data_folder}/intersection/network_2.pkl', \"rb\") as f:\n",
    "    df1= pickle.load(f)\n",
    "with open(f'{data_folder}/intersection/extend_lines_f.pkl', \"rb\") as f:\n",
    "    org_gpd= pickle.load(f)\n",
    "\n",
    "# Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "df3 = df1[df1['name'].notna()]\n",
    "# df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "df4 = df1[df1['name'].isna()]\n",
    "# df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "# use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "old_index  ='old_index'\n",
    "\n",
    "df = gpd.sjoin(df4, org_gpd).reset_index(names='old_index')\n",
    "# Create a new dictionary to store the updated data.\n",
    "dic_str_data = {}\n",
    "# Define the relevant columns to store\n",
    "rel_col  =[col for col in df.columns if col.endswith(\"right\")]+['geometry']\n",
    "rel_col.remove('index_right')\n",
    "\n",
    "# nore code should be added here to update name"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code to consolidate nearest intersections\n",
    "1. Get the first/start of each line\n",
    "2. Make sure I have the name of the lines associated with these lines\n",
    "3. Use DBSCAN with 20 meters threshold\n",
    "4. For each group\n",
    "   4.1.dffds\n",
    "   4.2 fj;ldsfads\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}