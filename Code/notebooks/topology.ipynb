{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import pickle\n",
    "project_crs = 'epsg:3857'\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "from shapely.geometry import  Point, LineString, MultiPolygon, MultiPoint\n",
    "from momepy import remove_false_nodes\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Choose locations and create folders if necessary\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder = f'{pjr_loc}/places/{place.replace(\",\", \"_\").replace(\" \", \"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes', exist_ok=True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection', exist_ok=True)\n",
    "if place == 'Tel Aviv':\n",
    "    useful_tags_path = ['name:en', 'highway', 'length', 'bearing', 'tunnel', 'junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Run when initialise the code\n",
    "# find and store roundabout\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',layer = 'edges')# Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=Trueb)\n",
    "is_junction= True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf= my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Classes to be employed during the execution of this code.\n",
    "# Intersection\n",
    "# Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self, network: GeoDataFrame, number: int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry': [], 'name': []}\n",
    "        self.lines_to_delete = []\n",
    "        self.num = number\n",
    "        print('Update topology')\n",
    "\n",
    "    def intersection_network(self):\n",
    "        \"\"\"\n",
    "        This function fix topology (add or remove vertices) where needed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # First remove_false_nodes\n",
    "        self.my_network = remove_false_nodes(self.my_network).reset_index(drop=True)\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines = self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 = gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines, crs=project_crs),\n",
    "                             right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index != s_join_0['index_right']]\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time = len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x, pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic) == 0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic, crs=project_crs)\n",
    "        # Split string line by points\n",
    "        segments = {'geometry': [], 'org_id': []}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups = inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups:\n",
    "                pbar.update(1)\n",
    "                points = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "                # if group_pnts[0]==588:\n",
    "                #     print(points)\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line, crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = GeoDataFrame(pd.concat([points_line_gdf, points]), crs=project_crs)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(\n",
    "                    lambda x: row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start', inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg = []\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg) > 1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments, crs=project_crs)\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] = self.my_network[cols_no_geometry]\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split = GeoDataFrame(pd.concat([self.my_network.drop(index=network_split_final.index.unique()),\n",
    "                                                network_split_final]), crs=project_crs)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def find_intersection_points(self, row, pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 = self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,\n",
    "                          LineString):  # The intersection is only between the buffer and the point\n",
    "                return\n",
    "            if isinstance(pnt, MultiPoint):\n",
    "                for single_pnt in pnt.geoms:\n",
    "                    self.inter_pnt_dic['geometry'].append(single_pnt)\n",
    "                    self.inter_pnt_dic['name'].append(row.name)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords) == 0 or pnt.coords[0] == line_1.geometry.coords[0] or pnt.coords[0] == \\\n",
    "                    line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "\n",
    "    def update_names(self, org_gpd: GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index = 'old_index'\n",
    "        df4_as_buffer = GeoDataFrame(geometry=df4['geometry'].buffer(distance=2, cap_style=2), crs=project_crs)\n",
    "        df = gpd.sjoin(df4_as_buffer,\n",
    "                       org_gpd)  # for spatial join use buffer around each polyline.that provide better result\n",
    "        df.index.name = old_index\n",
    "        df['geometry'] = df4['geometry']  # bring the dataframe into linestring format\n",
    "        df.reset_index(inplace=True)  # To be consistent with the following code and other dataframe\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = []\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names) == 1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1] - count_names[0] > 1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on = aplcnts_tst[\n",
    "                    aplcnts_tst['name'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(\n",
    "                    geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance=20, cap_style=2)],\n",
    "                    crs=project_crs)  # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on[\n",
    "                                                                   'index_right'])].reset_index()  # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 = GeoDataFrame(geometry=streets_right_geo.buffer(distance=20, cap_style=2))\n",
    "                streets_right_geo['area'] = gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = \\\n",
    "                    groupy.get_group(groupy['area'].sum().sort_values(ascending=False).index[0]).sort_values(\n",
    "                        by='area',\n",
    "                        ascending=False).iloc[\n",
    "                        0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data.append(my_data.to_list())\n",
    "\n",
    "        _ = df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(data=dic_str_data, columns=df.columns, crs=project_crs).drop(\n",
    "            columns='index_right').set_index(old_index)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = GeoDataFrame(pd.concat([df3, updated_df]), crs=project_crs)\n",
    "\n",
    "\n",
    "# Roundabout\n",
    "class EnvEntity:\n",
    "    def __init__(self, network):\n",
    "        self.dead_end_fd = None\n",
    "        self.pnt_dead_end = None\n",
    "        self.pnt_dic = {}\n",
    "        self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "        self.network = network\n",
    "\n",
    "    def __populate_pnt_dic(self, point: type, name_of_line: str):\n",
    "        \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        if not point in self.pnt_dic:\n",
    "            self.pnt_dic[point] = []\n",
    "        self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "    def __send_pnts(self, temp_line: GeoSeries):\n",
    "        \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        my_geom = temp_line['geometry']\n",
    "        self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "        self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "    def get_deadend_gdf(self, delete_short: int = 30) -> GeoDataFrame:\n",
    "        self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "        deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "        pnt_dead_end_0 = [item for item in self.pnt_dic.items() if\n",
    "                          len(item[1]) == 1]  # Retain all the line points with deadened\n",
    "        self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "        # Create shp file of deadened_pnts\n",
    "        geometry, line_name = 'geometry', 'line_name'\n",
    "        pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "        pnt_dead_end_df[geometry] = pnt_dead_end_df[0].apply(lambda x: Point(x))\n",
    "        pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x: x[0])\n",
    "        pnt_dead_end_df.crs = project_crs\n",
    "        self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "        if delete_short > 0:\n",
    "            # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "            deadend_gdf = self.network.loc[deadend_list]\n",
    "            self.network.drop(index=deadend_gdf[deadend_gdf.length < delete_short].index, inplace=True)\n",
    "            return deadend_gdf[deadend_gdf.length > delete_short]\n",
    "        return self.network.loc[deadend_list]\n",
    "\n",
    "    def update_the_current_network(self, temp_network):\n",
    "        r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        new_network_temp = self.network.drop(index=temp_network.index)\n",
    "        self.network = GeoDataFrame(pd.concat([new_network_temp, temp_network]), crs=project_crs)\n",
    "        self.network['length'] = self.network.length\n",
    "        self.network = self.network[self.network['length'] > 1]\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self, network: GeoDataFrame):\n",
    "        EnvEntity.__init__(self, network)\n",
    "        self.pnt_dic = {}\n",
    "        self.centroid = self.__from_roundabout_to_centroid()\n",
    "        self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type == 'Polygon':  # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer.geoms):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid = GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "\n",
    "    def __first_last_pnt_of_line(self, row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "\n",
    "    def __update_geometry(self, cur, s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows() if\n",
    "                        item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = \\\n",
    "                self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                    0]\n",
    "        return LineString(geo_cur)\n",
    "\n",
    "    def my_spatial_join(self, deadend_lines, deadend_pnts, line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                   distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique()  # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side = deads_both_side[deads_both_side == 2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete = deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[\n",
    "            ~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length < 300))]\n",
    "        deadend_lines = deadend_lines[\n",
    "            ~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length < 300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x: self.__update_geometry(x, s_join), axis=1)\n",
    "\n",
    "        return change_geo"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# starting point\n",
    "new_network = gpd.read_file(f'{data_folder}/simplification/simp.shp').rename(columns={'is_simplif':'is_simplified'})\n",
    "new_network"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Test intersection network\n",
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# First remove_false_nodes\n",
    "my_network = remove_false_nodes(obj_intersection.my_network).reset_index(drop=True)\n",
    "my_network.to_file(f'{data_folder}/delete_2_nodes/delete_false_intersection.shp')\n",
    "# Create buffer around each element\n",
    "buffer_around_lines= my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "\n",
    "# s_join between buffer to lines\n",
    "s_join_0 =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines,crs=project_crs),right_df=my_network)\n",
    "\n",
    "# delete lines belong to the buffer\n",
    "s_join = s_join_0[s_join_0.index!=s_join_0['index_right']]\n",
    "s_join.reset_index().to_file(f'{data_folder}/delete_2_nodes/s_join.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "intersect_by_line =[]\n",
    "#  find_intersection_points by function\n",
    "inter_pnt_dic = {'geometry':[],'name':[]}\n",
    "# Find new intersections that are not at the beginning or end of the line\n",
    "def find_intersection_points(row):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            line_1 = my_network.loc[row.name]\n",
    "            line_2 =  my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            if isinstance(pnt,LineString): # The intersection is only between the buffer and the point ToDo - when the situation is a overlay line\n",
    "                intersect_by_line.append(row)\n",
    "                return\n",
    "            if isinstance(pnt,MultiPoint):\n",
    "                for single_pnt in pnt:\n",
    "                    inter_pnt_dic['geometry'].append(single_pnt)\n",
    "                    inter_pnt_dic['name'].append(row.name)\n",
    "                return\n",
    "            # If it is first or end continue\n",
    "            if len(pnt.coords)==0 or pnt.coords[0]==line_1.geometry.coords[0] or pnt.coords[0]==line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            inter_pnt_dic['geometry'].append(pnt)\n",
    "            inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            intersect_by_line.append(row)\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "s_join.apply(lambda x: find_intersection_points(x), axis=1)\n",
    "inter_pnt_gdf = GeoDataFrame(inter_pnt_dic,crs=project_crs)\n",
    "inter_pnt_gdf.to_file(f'{data_folder}/delete_2_nodes/inter_pnt_dic.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Split string line by one point\n",
    "\n",
    "segments = {'geometry':[],'org_id':[]}\n",
    "# Groupby points name (which is the line they should split)\n",
    "line_name = 588\n",
    "group_pnts =  inter_pnt_gdf.groupby('name').get_group(line_name)\n",
    "points  = group_pnts\n",
    "points['is_split'] = True\n",
    "row = my_network.loc[line_name]\n",
    "current = list(row.geometry.coords)\n",
    "points_line = [Point(x) for x in current]\n",
    "points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "points_line_gdf['is_split'] = False\n",
    "line_all_pnts = points_line_gdf.append(points)\n",
    "line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "seg =[]\n",
    "for point in line_all_pnts.iterrows():\n",
    "    prop = point[1]\n",
    "    seg.append(prop['geometry'])\n",
    "    if prop['is_split']:\n",
    "        segments['geometry'].append(LineString(seg))\n",
    "        segments['org_id'].append(row.name)\n",
    "        seg = [prop['geometry']]\n",
    "# if the split point is the last one, you don't need to create new segment\n",
    "if len(seg)>1:\n",
    "    segments['geometry'].append(LineString(seg))\n",
    "    segments['org_id'].append(row.name)\n",
    "segments"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "line_all_pnts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Split string line by all\n",
    "segments = {'geometry':[],'org_id':[]}\n",
    "# Groupby points name (which is the line they should split)\n",
    "\n",
    "my_groups =  inter_pnt_gdf.groupby('name')\n",
    "for group_pnts in my_groups :\n",
    "    points  = group_pnts[1]\n",
    "    points['is_split'] = True\n",
    "\n",
    "    # get the line to split by comparing the name\n",
    "    row = my_network.loc[group_pnts[0]]\n",
    "    current = list(row.geometry.coords)\n",
    "    points_line = [Point(x) for x in current]\n",
    "    points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "    points_line_gdf['is_split'] = False\n",
    "\n",
    "    # append all the points together (line points and split points)\n",
    "    line_all_pnts = points_line_gdf.append(points)\n",
    "\n",
    "    # Find the distance of each point form the begining of the line on the line.\n",
    "    line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "    line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "\n",
    "    # split the line\n",
    "    seg =[]\n",
    "    for point in line_all_pnts.iterrows():\n",
    "        prop = point[1]\n",
    "        seg.append(prop['geometry'])\n",
    "        if prop['is_split']:\n",
    "            segments['geometry'].append(LineString(seg))\n",
    "            segments['org_id'].append(row.name)\n",
    "            seg = [prop['geometry']]\n",
    "    # if the split point is the last one, you don't need to create new segment\n",
    "    if len(seg)>1:\n",
    "        segments['geometry'].append(LineString(seg))\n",
    "        segments['org_id'].append(row.name)\n",
    "network_split = GeoDataFrame(segments,crs=project_crs)\n",
    "network_split.to_file(f'{data_folder}/delete_2_nodes/segments.shp')\n",
    "cols_no_geometry = my_network.columns[:-1]\n",
    "network_split_final = network_split.set_index('org_id')\n",
    "network_split_final[cols_no_geometry] =my_network[cols_no_geometry]\n",
    "# remove old and redundant line from our network and update with new one\n",
    "network_split =my_network.drop(index=network_split_final.index.unique()).append(network_split_final)\n",
    "network_split['length'] = network_split.length\n",
    "my_network = network_split\n",
    "my_network.reset_index(drop=True,inplace= True)\n",
    "my_network.to_file(f'{data_folder}/delete_2_nodes/intersection_network.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Call to intersection_network as part of its class\n",
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n",
    "obj_intersection.intersection_network()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Test update_names\n",
    "org_gpd= new_network.copy()\n",
    "df1 = obj_intersection.my_network\n",
    "df1.to_file(f'{data_folder}/intersection/source.shp')\n",
    "\n",
    "\n",
    "# Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "df3 = df1[df1['name'].notna()]\n",
    "# df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "df4 = df1[df1['name'].isna()]\n",
    "# df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "# use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "old_index  ='old_index'\n",
    "\n",
    "df4_as_buffer= GeoDataFrame(geometry=df4['geometry'].buffer(distance  = 2, cap_style=2),crs=project_crs)\n",
    "df = gpd.sjoin(df4_as_buffer, org_gpd) # for spatial join use buffer around each polyline.that provide better result\n",
    "df.index.name = old_index\n",
    "df['geometry'] = df4['geometry'] # bring the dataframe into linestring format\n",
    "df.reset_index(inplace=True) # To be consistent with the following code and other dataframe"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Create a new dictionary to store the updated data.\n",
    "dic_str_data = []\n",
    "\n",
    "\n",
    "def return_street_name(aplcnts_tst):\n",
    "    \"\"\"\n",
    "    1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "    2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "    3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "    4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "    :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_names = aplcnts_tst['name'].value_counts().sort_values(ascending=False)\n",
    "    if len(count_names)==1:\n",
    "        # there is only one name\n",
    "        my_data = aplcnts_tst.iloc[0]\n",
    "    elif count_names[1]- count_names[0]>1:\n",
    "        # The highest number of polylines with the same name are bigger at least in 2:\n",
    "        my_data = aplcnts_tst[aplcnts_tst['name'] == count_names.index[0]].iloc[0]\n",
    "    else:\n",
    "        # otherwise filter those with the most popular name or close to (-1)\n",
    "        str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "        buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "        streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "        buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "        streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "        groupy = streets_right_geo.groupby('name')\n",
    "        my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "        # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "        my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "    # Populate the new dictionary with relevant data\n",
    "    dic_str_data.append(my_data.to_list())\n",
    "_ =df.groupby(old_index).apply(return_street_name)\n",
    "updated_df = GeoDataFrame(data= dic_str_data,columns=df.columns,crs=project_crs).drop(columns='index_right').set_index(old_index)\n",
    "updated_df['length'] = updated_df.length\n",
    "my_network = df3.append(updated_df)\n",
    "my_network.reset_index().to_file(f'{data_folder}/intersection/after_name_update.shp')\n",
    "# convert the dictionary into a dataframe."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Run intersection\n",
    "intersection_agg_folder = f'{data_folder}/intersection_agg/'\n",
    "\n",
    "num = 2\n",
    "new_gpd = gpd.read_file(f'{intersection_agg_folder}network_new.shp')\n",
    "obj_intersection = Intersection(new_gpd, num)\n",
    "obj_intersection.intersection_network()\n",
    "obj_intersection.update_names(new_gpd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Code to consolidate nearest intersections\n",
    "1. Get the first/start of each line\n",
    "2. Make sure I have the name of the lines associated with these lines\n",
    "3. Use DBSCAN with 20 meters threshold\n",
    "4. For each group\n",
    "   4.1.Find the point with the max number of connected lines, if it is one use it otherwise use the avarage\n",
    "   4.2 Among whom are updated remove every line the start and lase point are the same\n",
    "   4.3 Change the point of each line with new point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "param = 0\n",
    "if param:\n",
    "    \n",
    "intersection_agg_folder = f'{data_folder}/intersection_agg/'\n",
    "network = gpd.read_file(f'{data_folder}/final_test.shp')\n",
    "network"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 1. Get the first/start of each line\n",
    "# Extract unique start and end points from all LineStrings\n",
    "geometry= 'geometry'\n",
    "index_right= 'index_right'\n",
    "all_points = network[geometry].apply(lambda line: [Point(line.coords[0]), Point(line.coords[-1])]).explode()\n",
    "# # Create a GeoSeries of unique points\n",
    "unique_points = GeoDataFrame(geometry=gpd.GeoSeries(all_points).unique(),crs=project_crs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# save data\n",
    "unique_points.to_file(f'{intersection_agg_folder}unique_points.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 2. Make sure I have the name of the lines associated with these lines\n",
    "pnts_line_name = unique_points.sjoin(network)[[index_right,geometry]].reset_index().dissolve(by='index',aggfunc=lambda x: x.tolist())\n",
    "pnts_line_name['num_of_lines']= pnts_line_name[index_right].apply(len) # count the number of lines for each point"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 3. Use DBSCAN with 20 meters threshold\n",
    "# Extract coordinates for DBSCAN\n",
    "coordinates = pnts_line_name.geometry.apply(lambda point: (point.x, point.y)).tolist()\n",
    "dbscan = DBSCAN(eps=40, min_samples=2)\n",
    "pnts_line_name['group'] = dbscan.fit_predict(coordinates)\n",
    "lines_to_update = pnts_line_name[pnts_line_name['group']>-1]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# if you want to save the files\n",
    "def save_points_file(data,path):\n",
    "    \"\"\"\n",
    "    The function get a data, arrange columns, convert list to string and export  it into a shpfile\n",
    "    :param data:\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    col_of_lists_as_str= 'col_of_lists_as_str'\n",
    "    data[col_of_lists_as_str] = data[index_right].apply(str)\n",
    "    data.drop(columns= [index_right]).to_file(path)\n",
    "    data.drop(columns= [col_of_lists_as_str],inplace=True)\n",
    "save_points_file(pnts_line_name,f'{intersection_agg_folder}pnts_line_name.shp')\n",
    "save_points_file(lines_to_update,f'{intersection_agg_folder}lines_to_update.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 4.1.Find the point with the max number of connected lines, if it is one use it otherwise uses the average\n",
    "# Find the maximum 'num' value for each group\n",
    "num = 'num_of_lines'\n",
    "group_name  = 'group'\n",
    "new_geometry= 'new_geometry'\n",
    "max_values_per_group =lines_to_update.groupby('group')['num_of_lines'].max()\n",
    "# Filter rows with the maximum 'num' value for each group\n",
    "result_gdf = lines_to_update[lines_to_update.set_index([group_name, num]).index.isin(max_values_per_group.items())]\n",
    "\n",
    "# Custom aggregation function to calculate the average point for each group\n",
    "def calculate_average_point(group):\n",
    "    x_mean = group.x.mean()\n",
    "    y_mean = group.y.mean()\n",
    "    return Point(x_mean, y_mean)\n",
    "\n",
    "# Apply the custom aggregation function to calculate average points per group\n",
    "lines_to_update2= lines_to_update.set_index(group_name)\n",
    "lines_to_update2['new_geometry'] = result_gdf.groupby(group_name )[geometry].apply(calculate_average_point)\n",
    "lines_to_update2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 4.2 Among whom are updated remove every line the start and last point are the same\n",
    "# Get all the lines going to be deleted\n",
    "lines_to_delete =[]\n",
    "\n",
    "def update_lines_to_delete(row):\n",
    "    # explode the lines names within each row list to separate rows\n",
    "    lines_to_update_tmep = row[index_right].explode()\n",
    "\n",
    "    # Identify rows with duplicate values\n",
    "    lines_to_delete.extend(lines_to_update_tmep[lines_to_update_tmep.duplicated()].tolist())\n",
    "\n",
    "lines_to_update2.groupby(level=group_name).apply(update_lines_to_delete)\n",
    "\n",
    "# remove lines their geometry not going to change\n",
    "lines_to_update3= lines_to_update2[lines_to_update2[geometry]!=lines_to_update2[new_geometry]]\n",
    "lines_to_update3"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Save files\n",
    "lines_to_update2['new_geometry'].to_file(f'{intersection_agg_folder}new_geometry.shp')\n",
    "lines_to_update3['new_geometry'].to_file(f'{intersection_agg_folder}new_geometry_1.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# For test only\n",
    "network_delete = network[network.index.isin(lines_to_delete)]\n",
    "network_delete.reset_index().to_file(f'{intersection_agg_folder}network_delete.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 4.3 Change the point of each line with new point\n",
    "network_new = network[~network.index.isin(lines_to_delete)]\n",
    "def update_network_with_aggregated_point(group):\n",
    "    lines_in_group = group.explode(index_right)\n",
    "\n",
    "    def update_one_line(points_data):\n",
    "        if points_data.name not in lines_to_delete:\n",
    "            updated_line_geo =network_new.loc[points_data.name]\n",
    "            line_coords = updated_line_geo.geometry.coords\n",
    "            if Point(line_coords[0])==points_data.geometry:\n",
    "                network_new.at[points_data.name,geometry] = LineString([points_data[new_geometry]] + line_coords[1:])\n",
    "            elif Point(line_coords[-1])==points_data.geometry:\n",
    "                network_new.at[points_data.name,geometry] = LineString(line_coords[:-1]+[points_data[new_geometry]])\n",
    "            else:\n",
    "                print(points_data)\n",
    "                print(lines_in_group)\n",
    "    lines_in_group.set_index(index_right).apply(update_one_line,axis=1)\n",
    "\n",
    "lines_to_update3.groupby(level=group_name).apply(update_network_with_aggregated_point)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 4.3 on one point and one group\n",
    "lines_to_update3.groupby(level=group_name).get_group(338)\n",
    "network_new.loc[9018]['geometry'].coords[0]\n",
    "if 5952 in lines_to_delete:\n",
    "    print(True)\n",
    "lines_to_update3\n",
    "network_new.to_file(f'{intersection_agg_folder}network_new.shp')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# This code take the network and delete polylines with the same \n",
    "intersection_agg_folder = f'{data_folder}/intersection_agg/'\n",
    "gpd.read_file(f'{intersection_agg_folder}network_new.shp')\n",
    "all_points = network[geometry].apply(lambda line: [Point(line.coords[0]), Point(line.coords[-1])]).explode()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Create two polylines (LineString objects)\n",
    "line1 = LineString([(0, 0), (1, 1), (2, 0)])\n",
    "line2 = LineString([(0, 1), (1, 1), (2, 1)])\n",
    "\n",
    "# Create a GeoDataFrame with the two polylines\n",
    "gdf = gpd.GeoDataFrame(geometry=[line1, line2])\n",
    "\n",
    "# Display the original GeoDataFrame\n",
    "print(\"Original GeoDataFrame:\")\n",
    "print(gdf)\n",
    "\n",
    "# Compute the difference between the two polylines\n",
    "result = gdf.difference(gdf.iloc[1].geometry)\n",
    "\n",
    "# Create a new GeoDataFrame with the result\n",
    "result_gdf = gpd.GeoDataFrame(geometry=result)\n",
    "\n",
    "# Display the resulting GeoDataFrame\n",
    "print(\"\\nGeoDataFrame after removing overlay:\")\n",
    "print(result_gdf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
