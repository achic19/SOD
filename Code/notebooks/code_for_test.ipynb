{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "import glob\n",
    "\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import Polygon, Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes,extend_lines\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast # to convert str with list to list of string\n",
    "\n",
    "def get_five_largest_streets(simplified_network:GeoDataFrame,name_col:str,path:str):\n",
    "    \"\"\"\n",
    "    FOR TEST PURPOSES - get the five largest streets\n",
    "    :param simplified_network:\n",
    "    :param name_col: the column that stores the street name\n",
    "    :param path: the file name to store the longest street\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Dissolve the GeoDataFrame based on 'street_id' to aggregate polylines of the same street\n",
    "    dissolved_gdf = simplified_network[simplified_network['is_simplified']==1].dissolve(by=name_col)\n",
    "    dissolved_gdf['street_length'] = dissolved_gdf['geometry'].length\n",
    "    dissolved_gdf.sort_values(by='street_length',ascending=False).head(5).to_file(f'{path}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Green;font-size: 30px\">Module 1:Preliminary work</span>\n",
    "<ul> <li>Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.</li>\n",
    " <li>Identify roundabout elements, if any exist, and store them in a separate DataFrame.</li>\n",
    "  <li>Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.</li>\n",
    "   <li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li>\n",
    "   </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Red;font-size: 30px\">Module 2 -3:Detect  parallel streets segments and merge them </span>\n",
    "<ul> <li>For each group of streets with the same name search for parallel segments</li>\n",
    " <li> Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed</li>\n",
    "  <li>The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "    <ul><li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li></ul>\n",
    "    </li>\n",
    "\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turin\n"
     ]
    },
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Achituv\\\\My_Drive\\\\current_projects\\\\SOD\\\\Code/places/Turin_test'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run when initialise the code\n",
    "# In this example, the data is extracted from OSM by specifying a location's name, but you can also download data using a specified polygon. The code is designed to handle multiple polygons or location names seamlessly.\n",
    "\n",
    "# Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder  = f'{pjr_loc}/places/{place.replace(\",\",\"_\").replace(\" \",\"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes',exist_ok = True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection',exist_ok = True)\n",
    "data_folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Download data from OSM\n",
    "if place =='Tel Aviv':\n",
    "    useful_tags_path = ['name:en','highway','length','bearing','tunnel','junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n",
    "graph = ox.graph_from_place(place, network_type='all')\n",
    "graph = ox.bearing.add_edge_bearings(graph, precision=1)\n",
    "graph_pro = ox.projection.project_graph(graph, to_crs=project_crs)\n",
    "io.save_graph_geopackage(graph_pro, filepath=f'{data_folder}/osm_data.gpkg', encoding='utf-8', directed=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "# find and store roundabout\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',layer = 'edges')# Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=True)\n",
    "is_junction= True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf= my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "# Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.\n",
    "# if 'tunnel' in my_gdf.columns:\n",
    "#     my_gdf = my_gdf[~((my_gdf['tunnel'] == 'building_passage') | (my_gdf['tunnel'] == 'yes'))]\n",
    "to_remove = my_gdf[~((my_gdf['highway'] == 'motorway') | (my_gdf['highway'] == 'trunk')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'trunk_link'))]\n",
    "\n",
    "\n",
    "# Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.\n",
    "df_pro = to_remove.to_crs(project_crs).dropna(subset=['name'])\n",
    "df_pro = df_pro[df_pro['name']!='']\n",
    "df_pro['angle'] = df_pro['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "df_pro['length'] = df_pro.length\n",
    "\n",
    "# Function to convert valid list strings to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)[0]\n",
    "    except (ValueError, SyntaxError,TypeError):\n",
    "        return s  # Return the original string if conversion fails\n",
    "\n",
    "# Apply the function to the DataFrame column so polylines with several street names will return the first name.\n",
    "df_pro['name'] = df_pro['name'].apply(convert_to_list)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# region\n",
    "# Functions and classes to be utilized - Module 2\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    # to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left'))\n",
    "    new_data_0 = to_translate.sjoin(GeoDataFrame(geometry=my_buffer, crs=project_crs), how='inner').reset_index()\n",
    "    if not len(new_data_0):\n",
    "        # There is no overlay\n",
    "        return False\n",
    "    new_data_1= new_data_0[new_data_0['index'] != new_data_0['index_right']] # Remove overlay of polylines with its buffer\n",
    "    for translated_line in new_data_1.iterrows(): # Iterate over the ovelay polylines\n",
    "        translated_line = translated_line[1]\n",
    "        geo_tr_line =GeoDataFrame(data= pd.DataFrame([translated_line]),crs=project_crs)\n",
    "        # check if the the overlay is more than 10%\n",
    "        overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=my_buffer.loc[geo_tr_line['index_right']], crs=project_crs), how='intersection')\n",
    "        if (overlay.length/translated_line.length).iloc[0]*100>10:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Functions and classes to be utilized - Module 3\n",
    "def update_list(line_local):\n",
    "    \"\"\"\n",
    "    add the first start/end point into the list\n",
    "    :param line_local:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    This method calculate new line between the farthest points of the simplified polygon\n",
    "    :param one_poly:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pnt_list = one_poly.exterior.coords[:-1]\n",
    "    list_shp = [Point(item) for item in pnt_list]\n",
    "    dis, dis_2, dis_3 = 0, 0, 0\n",
    "    third_dis = (-1, -1)\n",
    "    # The new line will be determined by the third-farthest points\n",
    "    for k, point in enumerate(list_shp):\n",
    "        j = k\n",
    "        for point2 in list_shp[k + 1:]:\n",
    "            j += 1\n",
    "            temp_dis = point.distance(point2)\n",
    "            if temp_dis > dis:\n",
    "                dis = point.distance(point2)\n",
    "            elif temp_dis > dis_2:\n",
    "                dis_2 = point.distance(point2)\n",
    "            elif temp_dis > dis_3:\n",
    "                third_dis = (k, j)\n",
    "                dis_3 = point.distance(point2)\n",
    "    max_dist['name'].extend([id_pol + 2, id_pol + 2])\n",
    "    max_dist['geometry'].extend([list_shp[third_dis[0]], list_shp[third_dis[1]]])\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    length_to_check  =10\n",
    "    loops = int(dist /length_to_check)\n",
    "\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + length_to_check  * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + length_to_check  * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        s_join_loc = one_pnt_df.sjoin_nearest(data, distance_col='dis').iloc[0]\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            pnt_med = s_join_loc['geometry']\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            line_pnts.append(line.interpolate(line.project(pnt_med)))\n",
    "            line_pnts = add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts)\n",
    "            return line_pnts\n",
    "    return line_pnts\n",
    "def update_df_with_center_line(new_line,is_simplified=0,group_name= -1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "# group the street segments by street name\n",
    "my_groupby = df_pro.groupby('name')\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [],'is_simplified':[]}\n",
    "\n",
    "for_time = len(my_groupby)\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2352/2352 [10:13<00:00,  3.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10762\n",
      "create new files\n"
     ]
    }
   ],
   "source": [
    "smplfcton_fldr = f'{data_folder}/simplification'\n",
    "number_of_parallel = 0 # count the number of polylines were refined\n",
    "with tqdm(total=for_time) as pbar: #  It is used in order to visualise the progress by progress bar\n",
    "    for i, street in enumerate(my_groupby):\n",
    "        res = street[1] # it holds all the streets\n",
    "        name = street[0] # It holds the streets name\n",
    "        pbar.update(1) # for the progress bar\n",
    "        # Remove segments without angle. If less than two segments being left move to the next group.\n",
    "        res = res.dropna(subset=['angle'], axis=0)\n",
    "        if len(res) < 2:\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "        res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "        # if all is -1, don't touch the element\n",
    "        if (res['group']== -1).all():\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # cur_group = res[(res['group'] > -1) | (res.length>20)].groupby('group') # Remove short segments with -1 classification values\n",
    "        # The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "        for group in res.groupby('group'):\n",
    "            data = group[1]\n",
    "            if group[0] ==-1: # No need to check if is parallel\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "                continue\n",
    "            if check_parallelism(data.copy()):\n",
    "                number_of_parallel+=len(data) # Update the number of parallel polylines\n",
    "                # if among of lines with same angles some are parallel:\n",
    "                # new points DataFrame of start/end line of each group\n",
    "                list_pnts_of_line_group = []\n",
    "                data['geometry'].apply(update_list)\n",
    "                df_pnts = GeoDataFrame(geometry=list_pnts_of_line_group, crs=project_crs).drop_duplicates()\n",
    "\n",
    "                # unify lines to one polygon\n",
    "                buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "                one_buffer = buffers.unary_union\n",
    "\n",
    "                max_dist = {'name': [], 'geometry': []}\n",
    "                # simplify polygon with simplify function. If one_buffer is multipolygon object simplify each one them separately\n",
    "                if isinstance(one_buffer, MultiPolygon):\n",
    "                    for id_pol, polygon in enumerate(one_buffer):\n",
    "                        create_center_line(polygon)\n",
    "                else:\n",
    "                    id_pol = -1\n",
    "                    create_center_line(one_buffer)\n",
    "                max_df = GeoDataFrame(max_dist, crs=project_crs)\n",
    "                # max_df.to_file(f'{smplfcton_fldr}/simplimax_df.shp')\n",
    "                # find for each points the closet point from the oribinal data. the closet points will create the new line\n",
    "                s_join = max_df.sjoin_nearest(df_pnts).groupby('name')\n",
    "                for geo in s_join:\n",
    "                    same_name = geo[1]\n",
    "                    if same_name.iloc[0]['index_right'] == same_name.iloc[1]['index_right']:\n",
    "                        continue\n",
    "                    in_0 = same_name.iloc[0]['index_right']\n",
    "                    in_1 = same_name.iloc[1]['index_right']\n",
    "                    # These points will be served to be initial reference in order to find more points\n",
    "                    pnt_f = df_pnts.loc[in_0]['geometry']\n",
    "                    pnt_l = df_pnts.loc[in_1]['geometry']\n",
    "                    lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f])\n",
    "                    lines_pnt_geo.append(pnt_l)\n",
    "                    # Update dic_final\n",
    "                    update_df_with_center_line(LineString(lines_pnt_geo),1,group[0])\n",
    "\n",
    "            else:\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "\n",
    "print(number_of_parallel)\n",
    "print('create new files')\n",
    "# remove short lines\n",
    "final_cols = ['name', 'geometry', 'highway', 'bearing', 'length']\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "\n",
    "# create network\n",
    "new_network.to_file(f'{smplfcton_fldr}/simp.shp')\n",
    "# save the data as a pickle format\n",
    "\n",
    "# save another file for test\n",
    "get_five_largest_streets(new_network,'name',f'{smplfcton_fldr}/test_simp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "# region\n",
    "# Classes to be employed during the execution of this code.\n",
    "#Intersection\n",
    "#Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self,network:GeoDataFrame,number:int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry':[],'name':[]}\n",
    "        self.lines_to_delete =[]\n",
    "        self.num = number\n",
    "    def delete_false_intersection(self,name_to_splt='name'):\n",
    "\n",
    "        if 'length' in self.my_network.columns: # To run the code smoothly we need to remove 'length' col if exist\n",
    "            self.my_network.drop(columns='length',inplace= True)\n",
    "\n",
    "        # It should be executed twice in order to clean all\n",
    "        for _  in range(2):\n",
    "            # First clean all the false node\n",
    "            self.my_network = remove_false_nodes(self.my_network)\n",
    "            # the previous function has changed the topology so the length should be updated\n",
    "            self.my_network['length'] =self.my_network.length\n",
    "            self.my_network  =self.my_network.drop_duplicates(subset='length') # remove false intersection duplicate many polyline which should be removed\n",
    "            self.my_network.reset_index(drop=True,inplace= True) # Changes has been made to the geometry so the index should be reset\n",
    "\n",
    "\n",
    "    def intersection_network(self):\n",
    "\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines= self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines,crs=project_crs),right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index!=s_join_0['index_right']]\n",
    "\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time =len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x,pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic)==0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic,crs=project_crs)\n",
    "\n",
    "        # Split string line by points\n",
    "        segments = {'geometry':[],'org_id':[]}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups =  inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups :\n",
    "                pbar.update(1)\n",
    "                points  = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = points_line_gdf.append(points)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg =[]\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg)>1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments,crs=project_crs)\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] =self.my_network[cols_no_geometry]\n",
    "\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split =self.my_network.drop(index=network_split_final.index.unique()).append(network_split_final).drop(index= self.lines_to_delete)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True,inplace= True)\n",
    "\n",
    "    def find_intersection_points(self,row,pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 =  self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,LineString):\n",
    "                return\n",
    "            if isinstance(pnt,MultiPoint):\n",
    "                temp_line= line_1.name if line_1.length< line_2.length else line_2.name\n",
    "                if temp_line not in self.lines_to_delete:\n",
    "                    self.lines_to_delete.append(temp_line)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords)==0 or pnt.coords[0]==line_1.geometry.coords[0] or pnt.coords[0]==line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "    def update_names(self, org_gpd:GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        # df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "        # df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index  ='old_index'\n",
    "\n",
    "        df = gpd.sjoin(df4, org_gpd).reset_index(names='old_index')\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = {}\n",
    "        # Define the relevant columns to store\n",
    "        rel_col  =[col for col in df.columns if col.endswith(\"right\")]+['geometry']\n",
    "        rel_col.remove('index_right')\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name_right'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names)==1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1]- count_names[0]>1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name_right'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name_right'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "                streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data[my_data['old_index']] = my_data[rel_col].to_list()\n",
    "        _ =df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(index=dic_str_data.keys(), data= dic_str_data.values(),columns=[x.replace('_right', '',) for x in rel_col],crs=project_crs)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = df3.append(updated_df)\n",
    "\n",
    "#Roundabout\n",
    "class EnvEntity:\n",
    "        def __init__(self,network):\n",
    "            self.dead_end_fd = None\n",
    "            self.pnt_dead_end = None\n",
    "            self.pnt_dic = {}\n",
    "            self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "            self.network = network\n",
    "\n",
    "\n",
    "        def __populate_pnt_dic(self,point: type, name_of_line: str):\n",
    "            \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if not point in self.pnt_dic:\n",
    "                self.pnt_dic[point] = []\n",
    "            self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "        def __send_pnts(self,temp_line: GeoSeries):\n",
    "            \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            my_geom = temp_line['geometry']\n",
    "            self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "            self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "        def get_deadend_gdf(self,delete_short:int =30)-> GeoDataFrame:\n",
    "            self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "            deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "            pnt_dead_end_0 = [item for item in self.pnt_dic.items() if len(item[1]) == 1] # Retain all the line points with deadened\n",
    "            self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "            # Create shp file of deadened_pnts\n",
    "            geometry,line_name = 'geometry','line_name'\n",
    "            pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "            pnt_dead_end_df[geometry]= pnt_dead_end_df[0].apply(lambda x:Point(x))\n",
    "            pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x:x[0])\n",
    "            pnt_dead_end_df.crs = project_crs\n",
    "            self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "            if delete_short>0:\n",
    "                # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "                deadend_gdf =self.network.loc[deadend_list]\n",
    "                self.network.drop(index=deadend_gdf[deadend_gdf.length<delete_short].index,inplace=True)\n",
    "                return deadend_gdf[deadend_gdf.length>delete_short]\n",
    "            return self.network.loc[deadend_list]\n",
    "\n",
    "        def update_the_current_network(self,temp_network):\n",
    "            r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            new_network_temp = self.network.drop(index=temp_network.index)\n",
    "            self.network = new_network_temp.append(temp_network)\n",
    "            self.network['length'] = self.network.length\n",
    "            self.network  = self.network[self.network['length']>1]\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self,network: GeoDataFrame):\n",
    "       EnvEntity.__init__(self,network)\n",
    "       self.pnt_dic ={}\n",
    "       self.centroid =self.__from_roundabout_to_centroid()\n",
    "       self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type=='Polygon': # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid =GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "        # GeoDataFrame(dic_data,crs=project_crs).to_file(f'{path_round_about}/roundabout_union.shp')\n",
    "\n",
    "    def __first_last_pnt_of_line(self,row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "    def __update_geometry(self,cur,s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows()if item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                0]\n",
    "        return LineString(geo_cur)\n",
    "    def my_spatial_join(self,deadend_lines, deadend_pnts,line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "        # centroid = gpd.read_file(f'{path_round_about}/centroid.shp')\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                   distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique() # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side =deads_both_side[deads_both_side==2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete=deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length<300))]\n",
    "        deadend_lines = deadend_lines[~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length<300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x:self.__update_geometry(x,s_join), axis=1)\n",
    "\n",
    "        return change_geo\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "                         name      highway  bearing  group  is_simplified  \\\n0                          11         path   124.50     -1              0   \n1                          11         path   124.50     -1              0   \n2                          11         path   124.50     -1              0   \n3                          18         path    40.85     -1              0   \n4                          18         path    40.85     -1              0   \n...                       ...          ...      ...    ...            ...   \n17720      Vicolo Santa Maria   pedestrian    25.20     -1              0   \n17721      Vicolo Santa Maria   pedestrian    25.20     -1              0   \n17722         Vicolo Valtorta  residential    26.50     -1              0   \n17723      Viottolo del Crall      footway    48.40     -1              0   \n17724  via Edoardo Perroncito      service    26.90     -1              0   \n\n                                                geometry  \n0      LINESTRING (857888.020 5626761.570, 857902.213...  \n1      LINESTRING (857638.976 5626560.639, 857618.671...  \n2      LINESTRING (854914.954 5625472.999, 854917.670...  \n3      LINESTRING (860173.665 5627099.066, 860141.427...  \n4      LINESTRING (860173.665 5627099.066, 860196.674...  \n...                                                  ...  \n17720  LINESTRING (854750.858 5632842.689, 854753.997...  \n17721  LINESTRING (854753.997 5632849.499, 854774.402...  \n17722  LINESTRING (854694.542 5629305.469, 854722.494...  \n17723  LINESTRING (859390.699 5636638.352, 859320.123...  \n17724  LINESTRING (846757.429 5633375.691, 846856.748...  \n\n[17725 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>highway</th>\n      <th>bearing</th>\n      <th>group</th>\n      <th>is_simplified</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (857888.020 5626761.570, 857902.213...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (857638.976 5626560.639, 857618.671...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11</td>\n      <td>path</td>\n      <td>124.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (854914.954 5625472.999, 854917.670...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>path</td>\n      <td>40.85</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (860173.665 5627099.066, 860141.427...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18</td>\n      <td>path</td>\n      <td>40.85</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (860173.665 5627099.066, 860196.674...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17720</th>\n      <td>Vicolo Santa Maria</td>\n      <td>pedestrian</td>\n      <td>25.20</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (854750.858 5632842.689, 854753.997...</td>\n    </tr>\n    <tr>\n      <th>17721</th>\n      <td>Vicolo Santa Maria</td>\n      <td>pedestrian</td>\n      <td>25.20</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (854753.997 5632849.499, 854774.402...</td>\n    </tr>\n    <tr>\n      <th>17722</th>\n      <td>Vicolo Valtorta</td>\n      <td>residential</td>\n      <td>26.50</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (854694.542 5629305.469, 854722.494...</td>\n    </tr>\n    <tr>\n      <th>17723</th>\n      <td>Viottolo del Crall</td>\n      <td>footway</td>\n      <td>48.40</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (859390.699 5636638.352, 859320.123...</td>\n    </tr>\n    <tr>\n      <th>17724</th>\n      <td>via Edoardo Perroncito</td>\n      <td>service</td>\n      <td>26.90</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>LINESTRING (846757.429 5633375.691, 846856.748...</td>\n    </tr>\n  </tbody>\n</table>\n<p>17725 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "new_network = gpd.read_file(f'{data_folder}/simp.shp').rename(columns={'is_simplif':'is_simplified'})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46672/46672 [00:07<00:00, 6157.49it/s]\n",
      "100%|██████████| 1460/1460 [00:08<00:00, 175.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n",
    "obj_intersection.delete_false_intersection()\n",
    "obj_intersection.intersection_network()\n",
    "obj_intersection.update_names(new_gpd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-75.49043107032776\n"
     ]
    }
   ],
   "source": [
    "# For test\n",
    "obj_intersection.my_network.reset_index().to_file(f'{data_folder}/delete_2_nodes/all_new2.shp')\n",
    "get_five_largest_streets(obj_intersection.my_network,'name','delete_2_nodes/test_all_new2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "\n",
    "line_name ='line_name'\n",
    "if is_junction:\n",
    "\n",
    "    exist_data= obj_intersection.my_network.reset_index().reset_index(names=line_name)\n",
    "    my_roundabout=Roundabout(exist_data)\n",
    "    deadend_lines, deadend_pnts = my_roundabout.deadend()\n",
    "\n",
    "    # update the current network\n",
    "    change_geo = my_roundabout.my_spatial_join(deadend_lines, deadend_pnts,line_name)\n",
    "    my_roundabout.update_the_current_network(change_geo)\n",
    "\n",
    "    my_roundabout.network.drop_duplicates(subset=line_name,inplace=True)\n",
    "    # Improve roundabout\n",
    "    # First buffer around centroid\n",
    "    centr_name= 'centr_name'\n",
    "    buffer_around_centroid= my_roundabout.centroid['geometry'].buffer(cap_style=1, distance=30)\n",
    "\n",
    "    # s_join between buffer to lines (reset index to retain the original centroid name which can apper more than one in the results). always stay with data you need and with understandable name\n",
    "    roundabout_with_lines =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_centroid,crs=project_crs).reset_index(),right_df=my_roundabout.network[['geometry',line_name]]).drop_duplicates(subset=['index',line_name]).rename(columns={\"index\":centr_name})[['geometry',line_name,centr_name]]\n",
    "\n",
    "    # To facilitate the searching process\n",
    "    my_roundabout.network.set_index(line_name,inplace=True)\n",
    "    # To facilitate easy access to point centroid geometry data, it is advisable to store the information in an object that provides efficient retrieval.\n",
    "    pnt_centroid_temp = my_roundabout.centroid['geometry']\n",
    "    #  Group the data by centroid\n",
    "    for center_line in roundabout_with_lines.groupby(centr_name):\n",
    "        #  Iterate over each group after performing a groupby() operation\n",
    "        for center in center_line[1].itertuples():\n",
    "            # Find the line that connects to the current centroid and obtain its vertices\n",
    "            line_to_test = my_roundabout.network.loc[center[2]]\n",
    "            vertices_line = list(line_to_test['geometry'].coords)\n",
    "            pnt_test = [vertices_line[0],vertices_line[-1]]\n",
    "            # To determine if the current line is already connected to the current centroid,.\n",
    "            is_connected = my_roundabout.centroid[my_roundabout.centroid['geometry'].isin([Point(pnt_test[0]),Point(pnt_test[-1])])]\n",
    "            if len(is_connected)>0 and center[3] in is_connected['name']:\n",
    "                continue\n",
    "\n",
    "            if len(vertices_line)==2:\n",
    "                vertices_line.insert(1, pnt_centroid_temp[center[3]])\n",
    "            else:\n",
    "                my_list = [pnt_centroid_temp[center[3]].distance(Point(temp)) for temp in vertices_line]\n",
    "                # Find the minimum index\n",
    "                min_index = min(range(len(my_list)), key=my_list.__getitem__)\n",
    "                if min_index ==0:\n",
    "                    vertices_line.insert(0,pnt_centroid_temp[center[3]])\n",
    "                elif min_index == len(my_list)-1:\n",
    "                    vertices_line.append(pnt_centroid_temp[center[3]])\n",
    "                else:\n",
    "                    vertices_line[min_index] = pnt_centroid_temp[center[3]]\n",
    "            new_geo = LineString(vertices_line)\n",
    "            my_roundabout.network.at[center[2],'geometry'] = new_geo\n",
    "# Extend\n",
    "    new_network2 = my_roundabout.network.reset_index()\n",
    "    new_network2.drop(columns='index',inplace=True)\n",
    "else:\n",
    "    new_network2=  obj_intersection.my_network.reset_index()\n",
    "\n",
    "new_network2.to_file(f'{data_folder}/ra_network.shp')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extend_lines_f= extend_lines(new_network2,100)\n",
    "extend_lines_f['length'] = extend_lines_f.length\n",
    "new_gpd = extend_lines_f.copy()\n",
    "# extend_lines_f.to_file(f'{data_folder}/extend_network.shp')\n",
    "# ToDo - compare between networks to update fields\n",
    "obj_intersection_1 = Intersection(extend_lines_f.copy(),1)\n",
    "obj_intersection_1.delete_false_intersection('str_name')\n",
    "obj_intersection_1.intersection_network()\n",
    "obj_intersection_1.my_network.rename(columns={'str_name':'name'},inplace=True) # 'str_name' become 'str to be compatible with other previous networks\n",
    "new_gpd.rename(columns={'str_name':'name'},inplace=True)\n",
    "obj_intersection_1.update_names(org_gpd=new_gpd)\n",
    "\n",
    "# obj_intersection_1.my_network.to_file(f'{data_folder}/delete_2_nodes/top_20.shp')\n",
    "# Clear short segments\n",
    "final2 = EnvEntity(obj_intersection_1.my_network.reset_index())\n",
    "final2.update_the_current_network(final2.get_deadend_gdf(delete_short=100))\n",
    "final2.network.to_file(f'{data_folder}/final.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# For test\n",
    "get_five_largest_streets(final2.network,'name','delete_2_nodes/test_all_new2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "       line_name                           name    highway     bearing  group  \\\n0            0.0                             11       path  124.500000   -1.0   \n1            1.0  Ciclabile Carducci - Biglieri   cycleway   15.216667   -1.0   \n2            2.0  Ciclabile Carducci - Biglieri   cycleway   15.216667   -1.0   \n3            3.0  Ciclabile Carducci - Biglieri   cycleway   15.216667   -1.0   \n4            4.0  Corso Achille Mario Dogliotti  secondary   11.509091   -1.0   \n...          ...                            ...        ...         ...    ...   \n13135        NaN                            NaN        NaN         NaN    NaN   \n13136        NaN                            NaN        NaN         NaN    NaN   \n13137        NaN                            NaN        NaN         NaN    NaN   \n13138        NaN                            NaN        NaN         NaN    NaN   \n13139        NaN                            NaN        NaN         NaN    NaN   \n\n       is_simplified      length  \\\n0                0.0  291.082651   \n1                0.0    7.058731   \n2                0.0   65.403420   \n3                0.0  123.124385   \n4                0.0  712.781218   \n...              ...         ...   \n13135            NaN    0.000000   \n13136            NaN  175.488721   \n13137            NaN  130.120289   \n13138            NaN  189.644381   \n13139            NaN  286.656929   \n\n                                                geometry  \n0      LINESTRING (854914.954 5625472.999, 854917.670...  \n1      LINESTRING (853879.494 5627910.602, 853877.679...  \n2      LINESTRING (853836.046 5627755.961, 853819.893...  \n3      LINESTRING (853800.691 5627619.739, 853799.733...  \n4      LINESTRING (854546.553 5627313.872, 854546.620...  \n...                                                  ...  \n13135  LINESTRING (849222.214 5630513.777, 849222.214...  \n13136  LINESTRING (853078.416 5638086.161, 853075.581...  \n13137  LINESTRING (853015.589 5637930.905, 852977.554...  \n13138  LINESTRING (855935.521 5638211.301, 855904.240...  \n13139  LINESTRING (855762.103 5638284.859, 855754.012...  \n\n[13140 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>line_name</th>\n      <th>name</th>\n      <th>highway</th>\n      <th>bearing</th>\n      <th>group</th>\n      <th>is_simplified</th>\n      <th>length</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>11</td>\n      <td>path</td>\n      <td>124.500000</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>291.082651</td>\n      <td>LINESTRING (854914.954 5625472.999, 854917.670...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>Ciclabile Carducci - Biglieri</td>\n      <td>cycleway</td>\n      <td>15.216667</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>7.058731</td>\n      <td>LINESTRING (853879.494 5627910.602, 853877.679...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>Ciclabile Carducci - Biglieri</td>\n      <td>cycleway</td>\n      <td>15.216667</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>65.403420</td>\n      <td>LINESTRING (853836.046 5627755.961, 853819.893...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>Ciclabile Carducci - Biglieri</td>\n      <td>cycleway</td>\n      <td>15.216667</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>123.124385</td>\n      <td>LINESTRING (853800.691 5627619.739, 853799.733...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>Corso Achille Mario Dogliotti</td>\n      <td>secondary</td>\n      <td>11.509091</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>712.781218</td>\n      <td>LINESTRING (854546.553 5627313.872, 854546.620...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13135</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>LINESTRING (849222.214 5630513.777, 849222.214...</td>\n    </tr>\n    <tr>\n      <th>13136</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>175.488721</td>\n      <td>LINESTRING (853078.416 5638086.161, 853075.581...</td>\n    </tr>\n    <tr>\n      <th>13137</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>130.120289</td>\n      <td>LINESTRING (853015.589 5637930.905, 852977.554...</td>\n    </tr>\n    <tr>\n      <th>13138</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>189.644381</td>\n      <td>LINESTRING (855935.521 5638211.301, 855904.240...</td>\n    </tr>\n    <tr>\n      <th>13139</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>286.656929</td>\n      <td>LINESTRING (855762.103 5638284.859, 855754.012...</td>\n    </tr>\n  </tbody>\n</table>\n<p>13140 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save file for test\n",
    "with open(f'{data_folder}/intersection/network_2.pkl', \"wb\") as f:\n",
    "    pickle.dump(obj_intersection_1.my_network, f)\n",
    "with open(f'{data_folder}/intersection/extend_lines_f.pkl', \"wb\") as f:\n",
    "    pickle.dump(extend_lines_f, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}