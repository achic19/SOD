{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "project_crs = 'epsg:3857'\n",
    "\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "from Code.general_functions import Preprocessing\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-22T07:02:29.620211Z",
     "start_time": "2024-07-22T07:02:19.735718Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.169142Z",
     "start_time": "2024-07-18T13:56:36.922351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Join status field into the shapefile\n",
    "places = [\n",
    "    ('Turin_Italy','Turin'),\n",
    "    # ('Tel_Aviv','TA'),\n",
    "    #('San_Francisco__California','SF')\n",
    "]\n",
    "\n",
    "# Loop through each place to process the data\n",
    "for place in places:\n",
    "    # Load the selected streets from a CSV file\n",
    "    selected_streets_csv =pd.read_csv(os.path.join(pjr_loc, f'places/{place[0]}/selected_streets_{place[1]}.csv')).set_index('Street name')\n",
    "    selected_streets = gpd.read_file(os.path.join(pjr_loc, f'places/{place[0]}/selected_streets.shp')).set_index('name')\n",
    "    selected_streets['Status'] = selected_streets_csv['Status']\n",
    "    selected_streets.to_file(os.path.join(pjr_loc, f'places/{place[0]}/selected_streets.shp'))\n",
    "\n",
    "    \n",
    "selected_streets\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\18059\\\\OneDrive - ariel.ac.il\\\\Current_research\\\\SOD\\\\Code\\\\places/Turin_Italy/selected_streets_Turin.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Loop through each place to process the data\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m place \u001B[38;5;129;01min\u001B[39;00m places:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# Load the selected streets from a CSV file\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m     selected_streets_csv \u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(pjr_loc, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplaces/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mplace[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/selected_streets_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mplace[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStreet name\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m     selected_streets \u001B[38;5;241m=\u001B[39m gpd\u001B[38;5;241m.\u001B[39mread_file(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(pjr_loc, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplaces/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mplace[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/selected_streets.shp\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     13\u001B[0m     selected_streets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStatus\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m selected_streets_csv[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStatus\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[0;32m   1706\u001B[0m     f,\n\u001B[0;32m   1707\u001B[0m     mode,\n\u001B[0;32m   1708\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1709\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1710\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   1711\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[0;32m   1712\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1713\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m   1714\u001B[0m )\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    864\u001B[0m             handle,\n\u001B[0;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    869\u001B[0m         )\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\18059\\\\OneDrive - ariel.ac.il\\\\Current_research\\\\SOD\\\\Code\\\\places/Turin_Italy/selected_streets_Turin.csv'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.182906Z",
     "start_time": "2024-07-18T13:56:56.182906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The next cell reads SOD network and select only these in the csv file\n",
    "\n",
    "# Define the locations and their corresponding directory names\n",
    "places = [\n",
    "    ('Turin_Italy', 'Turin_Italy'),\n",
    "    ('San_Francisco__California', 'San_Francisco__California'),\n",
    "    ('Tel_Aviv', 'Tel_Aviv')\n",
    "]\n",
    "    #('Tel_Aviv', 'Tel_Aviv')\n",
    "    # ('Turin__Italy', 'Turin'),\n",
    "    # ('San_Francisco__California', 'San_Francisco'),\n",
    "# Loop through each place to process the data\n",
    "for place in places:\n",
    "    # Load the selected streets from a CSV file\n",
    "    selected_streets = pd.read_csv(os.path.join(pjr_loc, f'places/more/{place[0]}/selected_streets.csv'))\n",
    "    \n",
    "    # Load the street network shapefile as a GeoDataFrame\n",
    "    sod_data = gpd.read_file(os.path.join(pjr_loc, f'places/{place[1]}/network_new.shp'))\n",
    "    \n",
    "    # Filter the GeoDataFrame to include only the streets in the selected streets list\n",
    "    filtered_streets = sod_data[sod_data['name'].isin(selected_streets['Street name'])]\n",
    "    \n",
    "    # Dissolve the filtered streets by the 'name' column to aggregate geometries\n",
    "    dissolved_streets = filtered_streets.dissolve(by='name')\n",
    "    \n",
    "    # Select only the 'geometry' column to save to the GeoPackage\n",
    "    dissolved_streets = dissolved_streets[['geometry']]\n",
    "    \n",
    "    # Save the dissolved streets to a GeoPackage file\n",
    "    output_path = os.path.join(pjr_loc, f'places/{place[1]}/selected_streets.shp')\n",
    "    dissolved_streets.to_file(output_path)\n",
    "    print(f\"Processed and saved selected streets for {place[1]} to {output_path}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### This code find the number of polylines being aggregated and the results of the aggregation with the number of totals streets being aggregated\n",
    "# Define the working folder path\n",
    "work_folder = f'{pjr_loc}/places/Turin_Italy'\n",
    "\n",
    "# Read the simplified shapefile into a GeoDataFrame\n",
    "simp = gpd.read_file(f'{work_folder}/simp.shp')\n",
    "\n",
    "# Filter the GeoDataFrame to include only simplified features\n",
    "is_simplified = simp[simp['is_simplif'] == 1]\n",
    "\n",
    "# Group the simplified features by 'name' and count the number of occurrences for each name\n",
    "agg_streets = is_simplified.groupby('name').size()\n",
    "\n",
    "# Read the original shapefile before simplification into a GeoDataFrame\n",
    "db_before = gpd.read_file(f'{work_folder}/before_df.shp')\n",
    "\n",
    "# Filter the original GeoDataFrame to include only the features with names that were aggregated in the simplified GeoDataFrame\n",
    "only_aggregated = db_before[db_before['name'].isin(agg_streets.reset_index()['name'])]\n",
    "\n",
    "# Output the result (optional, for verification purposes)\n",
    "print(only_aggregated)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.189363Z",
     "start_time": "2024-07-18T13:56:56.189363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This code counts the number of roundabouts being aggregated and processes their geometric centroids.\n",
    "\n",
    "# Define the working folder path\n",
    "work_folder = f'{pjr_loc}/places/Turin_Italy'\n",
    "\n",
    "# Initialize preprocessing with the specified place and test mode\n",
    "place = 'Turin_Italy'\n",
    "my_preprocessing = Preprocessing(place, is_test=True)\n",
    "\n",
    "# Create a data folder for preprocessing\n",
    "data_folder = my_preprocessing.create_folder()\n",
    "\n",
    "# Perform the first filtering step\n",
    "df_pro = my_preprocessing.first_filtering()\n",
    "\n",
    "# Read the roundabout file and reproject to the project CRS\n",
    "roundabout_file = my_preprocessing.round_about\n",
    "roundabout_file = roundabout_file.to_crs(project_crs)\n",
    "\n",
    "# Create a buffered geometry around each roundabout\n",
    "round_about_buffer = roundabout_file['geometry'].buffer(cap_style=1, distance=10, join_style=1).unary_union\n",
    "\n",
    "# Initialize a list to store the names and centroids of roundabouts\n",
    "names = []\n",
    "centroids = []\n",
    "\n",
    "# Check if the buffer result is a single Polygon or multiple geometries\n",
    "if round_about_buffer.type == 'Polygon':  # In case we have only one polygon\n",
    "    names.append(0)\n",
    "    centroids.append(round_about_buffer.centroid)\n",
    "else:\n",
    "    for idx, geom in enumerate(round_about_buffer.geoms):\n",
    "        names.append(idx)\n",
    "        centroids.append(geom.centroid)\n",
    "\n",
    "# Create a GeoDataFrame from the lists with the specified CRS\n",
    "centroid = GeoDataFrame({'name': names, 'geometry': centroids}, crs=project_crs)\n",
    "\n",
    "# Output the resulting GeoDataFrame (optional, for verification purposes)\n",
    "centroid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#('Tel_Aviv', 'Tel_Aviv')\n",
    "\n",
    "# ('Turin_Italy', 'Turin'),\n",
    "# ('San_Francisco__California', 'San_Francisco'),\n",
    "## Choose locations and create folders if necessary\n",
    "place  = 'Turin_Italy'\n",
    "data_folder = os.path.join(pjr_loc, f'places/{place}')\n",
    "os.makedirs(f'{data_folder}/csv/', exist_ok=True) # Here the csv files will be stored\n",
    "before_df= gpd.read_file(f'{data_folder}/before_df.shp')\n",
    "after_df = gpd.read_file(f'{data_folder}/network_new.shp').drop(columns='index')\n",
    "# step 1-4 as one def\n",
    "def street_its_connections(network,is_simplified=False):\n",
    "    \"\"\"\n",
    "    Perform a spatial join to identify all streets connected to each street in the dataset.\n",
    "    :param is_simplified:Decide whether to store the status of 'is_simplified'.\n",
    "    :param network:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # vars\n",
    "    str_name = 'name_left'\n",
    "    con_str_name = 'name_right'\n",
    "    # 1.\tCopy network\n",
    "    df_analysis = network.copy()\n",
    "    # 2.\tInternal intersection\n",
    "    s_join_analysis = gpd.sjoin(df_analysis,network)\n",
    "    # 3.\tDelete those with the same name\n",
    "\n",
    "    s_join_analysis2 = s_join_analysis[s_join_analysis[str_name]!=s_join_analysis[con_str_name]]\n",
    "    # 4.\tFor each group: name: list\n",
    "    group_name = s_join_analysis2.groupby(str_name)\n",
    "    dic_data={}\n",
    "    def connected_streets(str_lines):\n",
    "        \"\"\"\n",
    "        Populate the dictionary with the name of the current street and all its connected streets.\n",
    "        :param str_lines:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if is_simplified: # after SOD process we also want to know if all/part of the street was simplified.\n",
    "            dic_data[str_lines[str_name].iloc[0]] =(list(str_lines[con_str_name].unique()),True if (str_lines['is_simplif_left'] == 1).any() else False)\n",
    "        else:\n",
    "            dic_data[str_lines[str_name].iloc[0]] =(list(str_lines[con_str_name].unique()),False)\n",
    "    group_name.apply(connected_streets)\n",
    "    return dic_data\n",
    "# Create dictionary of street: connected streets before and after\n",
    "dic_before = street_its_connections(before_df)\n",
    "print('dic_before')\n",
    "dic_after = street_its_connections(after_df,is_simplified=True)\n",
    "print('dic_after')\n",
    "# 5. Compare between the two dictionaries\n",
    "df1 = pd.DataFrame(index= dic_after.keys(), data=dic_after.values(),columns=['ConnectedStreets','is_simplified']).reset_index(names='StreetName')\n",
    "df2 = pd.DataFrame(index= dic_before.keys(), data=dic_before.values(),columns=['ConnectedStreets','is_simplified']).reset_index(names='StreetName')\n",
    "def common_streets(row):\n",
    "    \"\"\"\n",
    "    This function compares the connected streets of a given street in the current row with those in another DataFrame (df2).\n",
    "    It returns the count of common connected streets.\n",
    "\n",
    "    Parameters:\n",
    "    row (pandas.Series): A row from the original DataFrame containing 'StreetName' and 'ConnectedStreets' columns.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of common connected streets between the given row and the corresponding street in df2.\n",
    "    \"\"\"\n",
    "    # Filter df2 to find the streets with the same name as the current row's street\n",
    "    streets_in_old = df2[df2['StreetName'] == row['StreetName']]\n",
    "    \n",
    "    # Check if there are no matching streets in df2\n",
    "    if streets_in_old.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        # Calculate the intersection of connected streets between the current row and the matched street in df2\n",
    "        return len(set(row['ConnectedStreets']) & set(streets_in_old['ConnectedStreets'].iloc[0]))\n",
    "\n",
    "def unique_streets_in_df1(row):\n",
    "    \"\"\"\n",
    "    This function compares the connected streets of a given street in the current row with those in another DataFrame (df2).\n",
    "    It returns the count of unique connected streets that are present in the current row but not in df2.\n",
    "\n",
    "    Parameters:\n",
    "    row (pandas.Series): A row from the original DataFrame containing 'StreetName' and 'ConnectedStreets' columns.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of unique connected streets that are in the current row but not in the corresponding street in df2.\n",
    "    \"\"\"\n",
    "    # Filter df2 to find the streets with the same name as the current row's street\n",
    "    streets_in_old = df2[df2['StreetName'] == row['StreetName']]\n",
    "    \n",
    "    # Check if there are no matching streets in df2\n",
    "    if streets_in_old.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        # Calculate the unique connected streets in the current row that are not in the matched street in df2\n",
    "        return len(set(row['ConnectedStreets']) - set(streets_in_old['ConnectedStreets'].iloc[0]))\n",
    "\n",
    "\n",
    "def unique_streets_in_df2(row):\n",
    "    \"\"\"\n",
    "    This function compares the connected streets of a given street in the current row with those in another DataFrame (df2).\n",
    "    It returns the count of unique connected streets that are present in df2 but not in the current row.\n",
    "\n",
    "    Parameters:\n",
    "    row (pandas.Series): A row from the original DataFrame containing 'StreetName' and 'ConnectedStreets' columns.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of unique connected streets that are in the corresponding street in df2 but not in the current row.\n",
    "    \"\"\"\n",
    "    # Filter df2 to find the streets with the same name as the current row's street\n",
    "    streets_in_old = df2[df2['StreetName'] == row['StreetName']]\n",
    "    \n",
    "    # Check if there are no matching streets in df2\n",
    "    if streets_in_old.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        # Calculate the unique connected streets in the matched street in df2 that are not in the current row\n",
    "        return len(set(streets_in_old['ConnectedStreets'].iloc[0]) - set(row['ConnectedStreets']))\n",
    "\n",
    "\n",
    "df1['CommonStreetsCount'] = df1.apply(common_streets, axis=1)\n",
    "df1['UniqueStreetsInAfterCount'] = df1.apply(unique_streets_in_df1, axis=1)\n",
    "df1['UniqueStreetsInBeforeCount'] = df1.apply(unique_streets_in_df2, axis=1)\n",
    "\n",
    "df1['rate_success'] =(df1['CommonStreetsCount']/df1[['CommonStreetsCount','UniqueStreetsInAfterCount','UniqueStreetsInBeforeCount']].sum(axis=1)*100).round(0)\n",
    "    \n",
    "# Calculate rate success\n",
    "df1['rate_success'] =(df1['CommonStreetsCount']/df1[['CommonStreetsCount','UniqueStreetsInAfterCount','UniqueStreetsInBeforeCount']].sum(axis=1)*100).round(0)\n",
    "\n",
    "# Update the original data source\n",
    "after_df2 = after_df.set_index('name')\n",
    "after_df2['rate_success'] = df1.set_index('StreetName')['rate_success']\n",
    "after_df2 = after_df2.reset_index().dissolve(by='name')\n",
    "after_df2.to_file(f'{data_folder}/after_df.shp')\n",
    "df1.to_csv(f'{data_folder}/csv/after_df.csv')\n",
    "df2.to_csv(f'{data_folder}/csv/before_df.csv')\n",
    "rate_success= 'rate_success'\n",
    "is_simplified= 'is_simplified'\n",
    "print(f'mean: {df1[rate_success].mean()}')\n",
    "print(f'mean_simplified: {df1[df1[is_simplified]][rate_success].mean()}')\n",
    "print(f'mean_no_simplified: {df1[~df1[is_simplified]][rate_success].mean()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-22T09:20:11.708420Z",
     "start_time": "2024-07-22T09:19:29.521049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dic_before\n",
      "dic_after\n",
      "mean: 80.42657657657658\n",
      "mean_simplified: 75.36992840095465\n",
      "mean_no_simplified: 81.60299833425874\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:18:36.645903Z",
     "start_time": "2024-07-22T07:18:34.480982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "place  = 'Turin_Italy'\n",
    "data_folder = os.path.join(pjr_loc, f'places/{place}')\n",
    "after_df2 = gpd.read_file(f'{data_folder}/after_df.shp')\n",
    "len(after_df2[after_df2['rate_succe']== 100] )/len(after_df2)*100\n",
    "# len(after_df2[(after_df2['is_simplif'] == 1) & (after_df2['rate_succe']== 100)] )/len(after_df2)*100"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.655462184873954"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:13:48.599100Z",
     "start_time": "2024-07-22T07:13:48.493469Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "2256    None\n",
       "2257    None\n",
       "2258    None\n",
       "2259    None\n",
       "2260    None\n",
       "Length: 1940, dtype: geometry"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.208544Z",
     "start_time": "2024-07-18T13:56:56.208544Z"
    }
   },
   "cell_type": "code",
   "source": "df1[df1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.218594Z",
     "start_time": "2024-07-18T13:56:56.218594Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "after_df2[after_df2]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.223618Z",
     "start_time": "2024-07-18T13:56:56.223618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df1.copy()\n",
    "\n",
    "# Define the bins and labels for the groups\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "labels = ['0-25', '25-50', '50-75', '75-100']\n",
    "\n",
    "# Create a new column with the group intervals\n",
    "df['group'] = pd.cut(df['rate_success'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Calculate the percentage of entries in each group for All\n",
    "group_percentages = df['group'].value_counts(normalize=True) * 100\n",
    "group_percentages.name = 'Percentage'\n",
    "\n",
    "# Split the DataFrame based on the boolean_value\n",
    "df_true = df[df['is_simplified'] == True]\n",
    "df_false = df[df['is_simplified'] == False]\n",
    "\n",
    "# Calculate the percentage of entries in each group for True\n",
    "group_percentages_true = df_true['group'].value_counts(normalize=True) * 100\n",
    "group_percentages_true.name = 'Percentage_True'\n",
    "\n",
    "# Calculate the percentage of entries in each group for False\n",
    "group_percentages_false = df_false['group'].value_counts(normalize=True) * 100\n",
    "group_percentages_false.name = 'Percentage_False'\n",
    "\n",
    "# Combine the results into one DataFrame\n",
    "output_df = pd.concat([group_percentages_true, group_percentages_false,group_percentages], axis=1).reset_index()\n",
    "output_df.columns = ['Group','Percentage', 'Percentage_True', 'Percentage_False']\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_df.to_csv(f'{data_folder}/csv/group_percentages_by_boolean.csv', index=False)\n",
    "\n",
    "print(output_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:59:44.168788Z",
     "start_time": "2024-07-18T13:59:44.100801Z"
    }
   },
   "cell_type": "code",
   "source": "output_df.to_csv(f'{data_folder}/csv/group_percentages_by_boolean.csv', index=False)\n",
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "# Create dictionaries - by steps\n",
    "# params\n",
    "is_simplified = True\n",
    "# vars\n",
    "str_name = 'name_left'\n",
    "con_str_name = 'name_right'\n",
    "# 1.\tCopy network\n",
    "df_analysis = before_df.copy()\n",
    "# 2.\tInternal intersection\n",
    "s_join_analysis = gpd.sjoin(df_analysis,before_df)\n",
    "\n",
    "# 3.\tDelete those with the same name\n",
    "s_join_analysis2 = s_join_analysis[s_join_analysis[str_name]!=s_join_analysis[con_str_name]]\n",
    "\n",
    "# 4.\tFor each group: name: list\n",
    "group_name = s_join_analysis2.groupby(str_name)\n",
    "dic_data={}\n",
    "def connected_streets(str_lines):\n",
    "    \"\"\"\n",
    "    Populate the dictionary with the name of the current street and all its connected streets.\n",
    "    :param str_lines:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if is_simplified: # after SOD process we also want to know if all/part of the street was simplified.\n",
    "        dic_data[str_lines[str_name].iloc[0]] =(list(str_lines[con_str_name].unique()),True if (str_lines['is_simplif_left'] == 1).any() else False)\n",
    "    else:\n",
    "        dic_data[str_lines[str_name].iloc[0]] =(list(str_lines[con_str_name].unique()),False)\n",
    "group_name.apply(connected_streets)\n",
    "dic_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.228474Z",
     "start_time": "2024-07-18T13:56:56.228474Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # 4.\tFor each group: name: list - Code for one group\n",
    "dic_data={}\n",
    "# loop on one group\n",
    "str_lines = group_name.get_group('Corso Chieri')\n",
    "dic_data[str_lines[str_name].iloc[0]] =list(str_lines[con_str_name].unique())\n",
    "# Calculate  if it is simplified\n",
    "gg = True if (str_lines['is_simplif_left'] == 1).any() else False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.238605Z",
     "start_time": "2024-07-18T13:56:56.238605Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from csv to shpfile\n",
    "my_gdf = gpd.read_file('places\\San_Francisco__California\\Streets___Active_and_Retired_20231024.csv')\n",
    "# This line build a correct format of the geometry based on the data from the line column\n",
    "my_gdf['geometry'] = my_gdf['line'].apply(lambda x:LineString([tuple(map(float, coords.split())) for coords in  re.split(r'\\(|\\)', x)[1].split(',')]))\n",
    "my_gdf.crs = 'EPSG:4326'\n",
    "my_gdf.to_file(r'places\\San_Francisco__California\\ref_shp')\n",
    "my_gdf.explore()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.239634Z",
     "start_time": "2024-07-18T13:56:56.239634Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"color:red; font-size:20px\"> Get 10% random sample of unique names</span>\n",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This code selects 10% of the streets randomly  to analysis manually  \n",
    "# parameters\n",
    "first_time = False\n",
    "for place in ['Tel_Aviv','San_Francisco__California','Turin_Italy']:\n",
    "    # if place!='San_Francisco__California':\n",
    "    #     continue\n",
    "    name ='name'\n",
    "\n",
    "    if first_time:\n",
    "        # Get Dataframe and choose only the simplified ones\n",
    "        df = gpd.read_file(f'places/{place}/simp.shp')\n",
    "\n",
    "\n",
    "        # Select 20% randomly\n",
    "        unique_names = df[df['is_simplif']==1]['name'].unique()\n",
    "        selected_names = pd.DataFrame(pd.Series(unique_names).sample(frac=0.1, random_state=42))\n",
    "        selected_names['is_same']=''\n",
    "        selected_names.set_index(0, inplace=True,)\n",
    "        selected_names.index.name= 'street'\n",
    "    else:\n",
    "        selected_names = pd.read_csv(f'places/{place}/selected_streets.csv').set_index('Street name')[['Status 1','Status 2']]\n",
    "\n",
    "    # Save the selected original streets in new shp files (regular and dissolve)\n",
    "    df_2 = gpd.read_file(f'places/{place}/osm_data.gpkg',layer = 'edges')\n",
    "    if place=='Tel_Aviv':\n",
    "        df_2[name] = df_2['name:en']\n",
    "        df_2.drop(columns='name:en',inplace=True)\n",
    "    res = df_2[df_2[name].isin(selected_names.index)]\n",
    "    res.to_file(f'places/{place}/selected_streets.shp')\n",
    "    res.dissolve(by=name).sort_values(by=name).to_file(f'places/{place}/selected_diss.shp')\n",
    "    # Create df to be filled in manually\n",
    "    code_to_repeat = \"selected_names.loc['',:]= []\"\n",
    "\n",
    "    # Loop to repeat the code\n",
    "    for street in selected_names.index:\n",
    "        print(f\"selected_names.loc['{street}',:]= []\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.243646Z",
     "start_time": "2024-07-18T13:56:56.243646Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# It is not updated\n",
    "data_dict= {0:'Success',\n",
    "            1: 'A few minor flaws',\n",
    "            2: 'Partially successful',\n",
    "            3: \"Failed\",\n",
    "            4:'The street of the ref network is presented partially/fully by more then one lane',\n",
    "            5: 'It was not mapped into the ref network',\n",
    "            6: 'The distant between the two parallel lanes are more 60 meters',\n",
    "            7: 'Complex street configurations that can be interpreted as either a success or failure, depend on the need',\n",
    "            8: \"If a polyline representing a roundabout lacks the accompanying tag 'roundabout=True', the code will refrain from converting it into a center point representation.\",\n",
    "            9: \"Plazas are not treated as roundabouts in the current version\",\n",
    "            10: 'The algorithm fails to connect nearby polylines to the roundabout due to its excessive size.',\n",
    "            11: 'In certain segments, the algorithm failed to preserve the original shape.',\n",
    "            12: 'Part of the street was not mapped into original OSM network'\n",
    "            }\n",
    "df_dict_data = pd.Series(data_dict)\n",
    "df_dict_data.to_csv(f'places/status_info.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.248613Z",
     "start_time": "2024-07-18T13:56:56.247604Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# data Query\n",
    "# Arrange the tables to align with the corresponding shapefile.\n",
    "for place in [('Turin_Italy','Turin'),('Tel_Aviv','TA'),('San_Francisco__California','SF')]:\n",
    "    print(place)\n",
    "    selected_streets =pd.read_csv(os.path.join(pjr_loc, f'places/{place[0]}/selected_streets_{place[1]}.csv'))\n",
    "    print(selected_streets['Status'].value_counts())\n",
    "    print(selected_streets['Status'].value_counts()/len(selected_streets)*100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.253646Z",
     "start_time": "2024-07-18T13:56:56.248613Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color:red; font-size:20px\"> Update the new table - DONT USE</span>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Arrange the tables to align with the corresponding shapefile.\n",
    "for place in ['Tel_Aviv','San_Francisco__California','Turin__Italy']:\n",
    "    print(place)\n",
    "    selected_streets = pd.read_csv(f'places/{place}/selected_streets.csv').sort_values(by='Street name').reset_index(drop=True).to_csv(f'places/{place}/selected_streets.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-18T13:56:56.255657Z",
     "start_time": "2024-07-18T13:56:56.255657Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Update the current key\n",
    "new_list = [0, 4, None, 5, 6, 7, 2, 1, 8, 9, 3, 10, 11, 12]\n",
    "my_dict = {i:key for i,key in enumerate(new_list)}\n",
    "for place in ['Tel_Aviv','San_Francisco__California','Turin__Italy']:\n",
    "    print(place)\n",
    "    selected_streets = pd.read_csv(f'places/{place}/selected_streets.csv')\n",
    "    # Split the column based on commas and expand into new columns\n",
    "    selected_streets[['status1', 'status2']] = selected_streets['is_same'].str.split(',',n=1, expand=True)\n",
    "\n",
    "    # Drop the original column if needed\n",
    "    selected_streets = selected_streets.drop('is_same', axis=1)\n",
    "\n",
    "    # Update the status with new values\n",
    "    selected_streets['status1']  = selected_streets['status1'].apply(lambda x:my_dict[int(x)])\n",
    "    selected_streets['status2']  = selected_streets['status2'].apply(lambda x:my_dict[int(x)] if x is not None else '' )\n",
    "    selected_streets.to_csv(f'places/{place}/selected_streets.csv')\n",
    "    print(selected_streets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
