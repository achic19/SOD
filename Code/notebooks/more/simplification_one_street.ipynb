{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T10:19:37.872965Z",
     "start_time": "2024-07-14T10:19:37.842350Z"
    }
   },
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "\n",
    "from pandas import DataFrame\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import  Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from general_functions import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes,extend_lines\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast # to convert str with list to list of string\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from math import log2\n",
    "def get_five_largest_streets(simplified_network:GeoDataFrame,name_col:str,path:str):\n",
    "    \"\"\"\n",
    "    FOR TEST PURPOSES - get the five largest streets\n",
    "    :param simplified_network:\n",
    "    :param name_col: the column that stores the street name\n",
    "    :param path: the file name to store the longest street\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Dissolve the GeoDataFrame based on 'street_id' to aggregate polylines of the same street\n",
    "    dissolved_gdf = simplified_network[simplified_network['is_simplified']==1].dissolve(by=name_col)\n",
    "    dissolved_gdf['street_length'] = dissolved_gdf['geometry'].length\n",
    "    dissolved_gdf.sort_values(by='street_length',ascending=False).head(5).to_file(f'{path}.shp')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Run when initialise the code\n",
    "# In this example, the data is extracted from OSM by specifying a location's name, but you can also download data using a specified polygon. The code is designed to handle multiple polygons or location names seamlessly.\n",
    "\n",
    "# Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.\n",
    "\n",
    "place = 'San_Francisco__California'\n",
    "print(place)\n",
    "data_folder =  f'{pjr_loc}/places/{place.replace(\",\",\"_\").replace(\" \",\"_\")}'\n",
    "data_folder_test  = f'{data_folder}/test/simplification'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes',exist_ok = True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection',exist_ok = True)\n",
    "data_folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T10:23:11.776166Z",
     "start_time": "2024-07-14T10:23:11.760154Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_pro = gpd.read_file(f'{data_folder}/before_df.shp')",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T10:44:34.284942Z",
     "start_time": "2024-07-14T10:44:21.374327Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# region\n",
    "# Functions and classes to be utilized - Module 2\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    # See if there are parallel lines\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35,\n",
    "                                                                                               'left'))  # we need to offset by both sides since the parallel lines could be in opposite directions\n",
    "\n",
    "    def is_parallel(my_s_join: GeoDataFrame, the_buffer: GeoSeries, geo_field: str):\n",
    "        my_s_join['geometry'] = my_s_join[geo_field]\n",
    "        new_data_0 = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner').reset_index()\n",
    "        if not len(new_data_0):\n",
    "            return False\n",
    "        new_data_1 = new_data_0[\n",
    "            new_data_0['index'] != new_data_0['index_right']]  # Remove overlay of polylines with its buffer\n",
    "        for translated_line in new_data_1.iterrows():\n",
    "            translated_line = translated_line[1]\n",
    "            geo_tr_line = GeoDataFrame(data=pd.DataFrame([translated_line]), crs=project_crs)\n",
    "            overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=the_buffer.loc[geo_tr_line['index_right']],\n",
    "                                                            crs=project_crs), how='intersection')\n",
    "            if (overlay.length / translated_line.length).iloc[0] * 100 > 10:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if is_parallel(to_translate, my_buffer, 'geometry_right'):\n",
    "        return True\n",
    "    else:\n",
    "        if is_parallel(to_translate, my_buffer, 'geometry_left'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    This method calculate new line between the farthest points of the simplified polygon\n",
    "    :param one_poly:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lines_in_buffer = data.sjoin(GeoDataFrame(geometry=[one_poly], crs=project_crs)).drop(columns='index_right')\n",
    "\n",
    "    list_pnts_of_line_group = []\n",
    "\n",
    "    def update_list(line_local):\n",
    "        \"\"\"\n",
    "        add the first start/end point into the list\n",
    "        :param line_local:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "\n",
    "    # Get the start/end points of these polylines\n",
    "    lines_in_buffer['geometry'].apply(update_list)\n",
    "\n",
    "    # Find all the unidirectional combinations between each two pair of points\n",
    "    point_combinations = list(combinations(list_pnts_of_line_group, 2))\n",
    "\n",
    "    # Save it into DataFrame frame and calculate distance\n",
    "    df_test = DataFrame()\n",
    "    df_test['point_1'] = [pair[0] for pair in point_combinations]\n",
    "    df_test['point_2'] = [pair[1] for pair in point_combinations]\n",
    "    df_test['dist'] = df_test.apply(lambda x: x['point_1'].distance(x['point_2']), axis=1)\n",
    "\n",
    "    # Calculate  angle (0 and 180)\n",
    "    # Calculate angle using vectorized operations\n",
    "    # Vectorized angle calculation using NumPy\n",
    "    dx = df_test['point_2'].apply(lambda p: p.x) - df_test['point_1'].apply(lambda p: p.x)\n",
    "    dy = df_test['point_2'].apply(lambda p: p.y) - df_test['point_1'].apply(lambda p: p.y)\n",
    "    df_test['angle'] = np.degrees(np.arctan2(dy, dx))\n",
    "    df_test['angle'] = np.where(df_test['angle'] > 0, df_test['angle'], df_test['angle'] + 180)\n",
    "\n",
    "    # Calculate the best two points by looking on their distance and angle. we compare the angle to the polylines angles. The angle has less important so the reason for 0.5\n",
    "    avg = lines_in_buffer['angle'].mean()\n",
    "    dis = abs(df_test['angle'] - avg)\n",
    "    df_test['ratio'] = df_test['dist'] / df_test['dist'].max() + 0.5 * dis / dis.max()\n",
    "    max_points = df_test.sort_values(by='ratio', ascending=False).iloc[0]\n",
    "\n",
    "    # These points will be served to be initial reference in order to find more points\n",
    "    pnt_f = max_points['point_1']\n",
    "    pnt_l = max_points['point_2']\n",
    "\n",
    "    angl_rng = lines_in_buffer['angle'].max() - lines_in_buffer['angle'].min()\n",
    "    if angl_rng < 1:  # If the angel range is less than 1 degree the line will be based on the first and last points\n",
    "        lines_pnt_geo = [pnt_f]\n",
    "    else:\n",
    "        if angl_rng > 100:  # Maximum of length to check is every 10 meters\n",
    "            length_to_check = 10\n",
    "        else:\n",
    "            length_to_check = 75 - log2(\n",
    "                angl_rng) * 10  # The range of  length_to_check (logarithm to create more changes at the beginning)\n",
    "        lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f], length_to_check, lines_in_buffer)\n",
    "    lines_pnt_geo.append(pnt_l)\n",
    "    # Update dic_final\n",
    "    return lines_pnt_geo\n",
    "\n",
    "\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list, lngth_chck: float,\n",
    "                               test_poly: GeoDataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :param test_poly: From these polylines find the closet one in each interation\n",
    "    :param lngth_chck: Used latter to find how many checks should be done\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    loops = int(dist / lngth_chck)\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + lngth_chck * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + lngth_chck * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        s_join_loc = one_pnt_df.sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            pnt_med = line.interpolate(line.project(s_join_loc['geometry']))\n",
    "            if pnt_med.distance(pnt_f_loc) < 10:  # Otherwise the code may stack in endless loops\n",
    "                continue\n",
    "            line_pnts.append(pnt_med)\n",
    "            line_pnts = add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts, lngth_chck, test_poly)\n",
    "            return line_pnts\n",
    "    return line_pnts\n",
    "\n",
    "\n",
    "def update_df_with_center_line(new_line, is_simplified=0, group_name=-1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "\n",
    "\n",
    "# Function to calculate circular_distance\n",
    "def circular_distance(angle1, angle2):\n",
    "    diff = np.abs(angle1 - angle2) % 180\n",
    "    return np.minimum(diff, 180 - diff)\n",
    "\n",
    "\n",
    "# Initiate dic_final here for @def update_df_with_center_line\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [], 'is_simplified': []}\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T10:47:29.147057Z",
     "start_time": "2024-07-14T10:47:29.116111Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# this code run only on the street spacified  in @street_name\n",
    "street_name = 'Kongahällavägen'\n",
    "print(street_name)\n",
    "# Make sure @dic_final is empty before execute this code\n",
    "dic_final= {key: [] for key in dic_final}\n",
    "street= df_pro.groupby('name').get_group(street_name) # group the street segments by street name\n",
    "for_time = len(street)\n",
    "\n",
    "smplfcton_fldr = f'{data_folder}/simplification/streets/'\n",
    "number_of_parallel = 0 # count the number of polylines were refined\n",
    "\n",
    "res = street# it holds all the streets\n",
    "name = street_name\n",
    "# Remove segments without angle. If less than two segments being left move to the next group.\n",
    "res = res.dropna(subset=['angle'], axis=0)\n",
    "if len(res) < 2:\n",
    "    data = res\n",
    "    _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "# Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "\n",
    "# if all is -1, don't touch the element\n",
    "if (res['group']== -1).all():\n",
    "    data = res\n",
    "    _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "# cur_group = res[(res['group'] > -1) | (res.length>20)].groupby('group') # Remove short segments with -1 classification values\n",
    "# The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "for group in res.groupby('group'):\n",
    "    data = group[1]\n",
    "    if group[0] ==-1: # No need to check if is parallel\n",
    "        _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "        continue\n",
    "    if check_parallelism(data.copy()):\n",
    "        # print(group[0])\n",
    "        # Remove unimportant streets which appear more than 10% in the group\n",
    "        min_num_of_polylines = len(data) / 15\n",
    "        # Use a single boolean condition for filtering\n",
    "        condition = (data['highway'].isin(['service','unclassified'])) & (data.groupby('highway')['highway'].transform('count') <= min_num_of_polylines)\n",
    "        data = data[~condition]\n",
    "\n",
    "        number_of_parallel+=len(data) # Update the number of parallel polylines\n",
    "\n",
    "        # unify lines to one polygon\n",
    "        buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "        one_buffer = buffers.unary_union\n",
    "        # simplify polygon with simplify function. If one_buffer is multipolygon object simplify each one them separately\n",
    "        if isinstance(one_buffer, MultiPolygon):\n",
    "            for polygon in one_buffer:\n",
    "                lines_pnt_geo_final = create_center_line(polygon)\n",
    "                update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "        else:\n",
    "            lines_pnt_geo_final =create_center_line(one_buffer)\n",
    "            # Update dic_final\n",
    "            update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "\n",
    "    else:\n",
    "        _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "\n",
    "print(number_of_parallel)\n",
    "print('create new files')\n",
    "# remove short lines\n",
    "final_cols = ['name', 'geometry', 'highway', 'bearing', 'length']\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "new_network['lenght']= new_network.length\n",
    "# create network\n",
    "new_network.to_file(f'{smplfcton_fldr}{street_name}.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
