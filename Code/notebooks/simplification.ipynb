{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "import glob\n",
    "import pickle\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import Polygon, Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes, extend_lines\n",
    "from itertools import combinations\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast  # to convert str with list to list of string\n",
    "import sys\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "# NEW\n",
    "import numpy as np\n",
    "from math import log2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## Choose locations and create folders if necessary\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder  = f'{pjr_loc}/places/{place.replace(\",\",\"_\").replace(\" \",\"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes',exist_ok = True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection',exist_ok = True)\n",
    "if place =='Tel Aviv':\n",
    "    useful_tags_path = ['name:en','highway','length','bearing','tunnel','junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download data\n",
    "graph = ox.graph_from_place(place, network_type='all')\n",
    "graph = ox.bearing.add_edge_bearings(graph, precision=1)\n",
    "graph_pro = ox.projection.project_graph(graph, to_crs=project_crs)\n",
    "io.save_graph_geopackage(graph_pro, filepath=f'{data_folder}/osm_data.gpkg', encoding='utf-8', directed=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## Filter out polylines and calculate angles\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',\n",
    "                       layer='edges')  # Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=True)\n",
    "\n",
    "is_junction = True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf = my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]\n",
    "to_remove = my_gdf[~((my_gdf['highway'] == 'motorway') | (my_gdf['highway'] == 'trunk')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'trunk_link'))]\n",
    "# Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.\n",
    "df_pro = to_remove.to_crs(project_crs).dropna(subset=['name'])\n",
    "df_pro = df_pro[df_pro['name']!='']\n",
    "df_pro['angle'] = df_pro['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "df_pro['length'] = df_pro.length\n",
    "\n",
    " # Function to convert valid list strings to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)[0]\n",
    "    except (ValueError, SyntaxError,TypeError):\n",
    "        return s  # Return the original string if conversion fails\n",
    "\n",
    "# Apply the function to the DataFrame column so polylines with several street names will return the first name.\n",
    "df_pro['name'] = df_pro['name'].apply(convert_to_list)\n",
    "df_pro['highway'] = df_pro['highway'].apply(convert_to_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Detect multi lanes' scenario\n",
    "# DBSCAN and first initial filtering\n",
    "# group the street segments by street name\n",
    "my_groupby = df_pro.groupby('name')\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [],'is_simplified':[]}\n",
    "# Street to test - name = 'Corso Orbassano', '18','Corso Vercelli','Piazza Giuseppe Luigi Lagrange','Corso Peschiera',,'Corso Giambattista Beccaria','Corso Re Umberto',\n",
    "# new to test = 'Corso Valdocco','Corso Ottone Rosai','Via Treviso','Strada del Fortino'\n",
    "name = 'Corso Quintino Sella'\n",
    "res = my_groupby.get_group(name)\n",
    "res = res.dropna(subset=['angle'], axis=0)\n",
    "if len(res) < 2:\n",
    "    data = res\n",
    "    _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "# Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# if all is -1, don't touch the element\n",
    "if (res['group']== -1).all():\n",
    "    data = res\n",
    "    _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "# check parallelism for one street. We create buffer around each polyline and also translate the buffer. If 10% of at least  one of  the translated polylines is overlay with the buffer it is multi lanes  scenarios\n",
    "angels_groups = res.groupby('group')\n",
    "data  = angels_groups.get_group(0)\n",
    "data.explore()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# test gruups\n",
    "res.value_counts('group')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# See if there are parallel lines\n",
    "to_translate = data.copy()\n",
    "my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left')) # we need to offset by both sides since the parallel lines could be in opposite directions\n",
    "def length_of_parallel(my_s_join: GeoDataFrame, the_buffer: GeoSeries, geo_field: str):\n",
    "    my_s_join['geometry'] = my_s_join[geo_field]\n",
    "    new_data_0 = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner').reset_index()\n",
    "    if not len(new_data_0):\n",
    "        return False\n",
    "    new_data_1= new_data_0[new_data_0['index'] != new_data_0['index_right']] # Remove overlay of polylines with its buffer\n",
    "    for translated_line in new_data_1.iterrows():\n",
    "        translated_line = translated_line[1]\n",
    "        geo_tr_line =GeoDataFrame(data= pd.DataFrame([translated_line]),crs=project_crs)\n",
    "        overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=the_buffer.loc[geo_tr_line['index_right']], crs=project_crs), how='intersection')\n",
    "        if (overlay.length/translated_line.length).iloc[0]*100>10:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if length_of_parallel(to_translate, my_buffer, 'geometry_right'):\n",
    "    print(1)\n",
    "\n",
    "else:\n",
    "    if length_of_parallel(to_translate, my_buffer, 'geometry_left'):\n",
    "        print(1)\n",
    "    else:\n",
    "        print(-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\n",
    "to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_s_join= to_translate\n",
    "the_buffer=\n",
    "geo_field="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Functions and classes to be utilized - Module 3\n",
    "def update_list(line_local):\n",
    "    \"\"\"\n",
    "    add the first start/end point into the list\n",
    "    :param line_local:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list, lngth_chck: float, test_poly:GeoDataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :param test_poly: From these polylines find the closet one in each interation\n",
    "    :param lngth_chck: Used latter to find how many checks should be done\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    loops = int(dist / lngth_chck)\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + lngth_chck * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + lngth_chck * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        try:\n",
    "            s_join_loc = one_pnt_df.sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "        except RecursionError:\n",
    "            print('error')\n",
    "            return line_pnts\n",
    "\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            pnt_med= line.interpolate(line.project( s_join_loc['geometry']))\n",
    "            if pnt_med.distance(pnt_f_loc)<10: # Otherwise the code may stack in endless loops\n",
    "                continue\n",
    "            line_pnts.append(pnt_med)\n",
    "            return add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts,lngth_chck,test_poly)\n",
    "    return line_pnts\n",
    "def update_df_with_center_line(new_line,is_simplified=0,group_name= -1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Create buffer around the polyline and unify them #\n",
    "min_num_of_polylines = len(data)/10\n",
    "# Use a single boolean condition for filtering\n",
    "condition = (data['highway'].isin(['service','unclassified'])) & (data.groupby('highway')['highway'].transform('count') <= min_num_of_polylines)\n",
    "data = data[~condition]\n",
    "\n",
    "# unify lines to one polygon\n",
    "buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "one_buffer = buffers.unary_union\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "one_buffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "time_00 =time.time()\n",
    "time_0 = time.time()\n",
    "#  Create centerline\n",
    "# Get all the polylines intersected with the new polygon\n",
    "one_poly= one_buffer\n",
    "lines_in_buffer = data.sjoin(GeoDataFrame(geometry=[one_poly],crs=project_crs)).drop(columns='index_right')\n",
    "\n",
    "list_pnts_of_line_group = []\n",
    "\n",
    "# Get the start/end points of these polylines\n",
    "lines_in_buffer['geometry'].apply(update_list)\n",
    "print(f'Get the start/end points of these polylines:{time.time() -time_0} ')\n",
    "time_0 = time.time()\n",
    "\n",
    "# Find all the unidirectional combinations between each two pair of points\n",
    "point_combinations = list(combinations(list_pnts_of_line_group, 2))\n",
    "print(f'Find all the unidirectional combinations between each two pair of points:{time.time() -time_0} ')\n",
    "time_0 = time.time()\n",
    "\n",
    "# Save it into DataFrame frame and calculate distance\n",
    "df_test= DataFrame()\n",
    "df_test['point_1'] = [pair[0] for pair in point_combinations]\n",
    "df_test['point_2'] = [pair[1] for pair in point_combinations]\n",
    "df_test['dist'] = df_test.apply(lambda x:x['point_1'].distance(x['point_2']),axis=1)\n",
    "print(f'Save it into Geodata frame and calculate distance and angle (0 and 180):{time.time() -time_0} ')\n",
    "time_0 = time.time()\n",
    "\n",
    "\n",
    "# Calculate  angle (0 and 180)\n",
    "# Calculate angle using vectorized operations\n",
    "# Vectorized angle calculation using NumPy\n",
    "dx = df_test['point_2'].apply(lambda p: p.x)  - df_test['point_1'].apply(lambda p: p.x)\n",
    "dy = df_test['point_2'].apply(lambda p: p.y)  - df_test['point_1'].apply(lambda p: p.y)\n",
    "df_test['angle'] = np.degrees(np.arctan2(dy, dx))\n",
    "df_test['angle']  = np.where(df_test['angle'] > 0, df_test['angle'], df_test['angle'] + 180)\n",
    "\n",
    "print(f'Calculate distance and angle (0 and 180):{time.time() -time_0} ')\n",
    "time_0 = time.time()\n",
    "# Calculate the best two points by looking on their distance and angle. we compare the angle to the polylines angles\n",
    "avg= lines_in_buffer['angle'].mean()\n",
    "dis = abs(df_test['angle'] -avg)\n",
    "df_test['ratio'] = df_test['dist']/df_test['dist'].max() + 0.5*dis /dis.max()\n",
    "max_points = df_test.sort_values(by='ratio',ascending=False).iloc[0]\n",
    "print(f'Calculate the best two points by looking on their distance and angle:{time.time() -time_0} ')\n",
    "\n",
    "\n",
    "# These points will be served to be initial reference in order to find more points\n",
    "pnt_f = max_points['point_1']\n",
    "pnt_l = max_points['point_2']\n",
    "print(f'tot:{time.time() -time_00} ')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "source": [
    "time_0 = time.time()\n",
    "\n",
    "# Add more vertices\n",
    "angl_rng = lines_in_buffer['angle'].max() - lines_in_buffer['angle'].min()\n",
    "if angl_rng <1: # If the angel range is less than 1 degree the line will be based on the first and last points\n",
    "        lines_pnt_geo = [pnt_f]\n",
    "else:\n",
    "    if angl_rng > 100: # Maximum of length to check is every 10 meters\n",
    "        length_to_check  =10\n",
    "    else:\n",
    "        length_to_check  =75-log2(angl_rng)*10 # The range of  length_to_check (logarithm to create more changes at the beginning)\n",
    "    lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f],length_to_check,lines_in_buffer)\n",
    "lines_pnt_geo.append(pnt_l)\n",
    "print(f'Add more vertices:{time.time() -time_0} ')\n",
    "# Update dic_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "source": [
    "# Draw for test\n",
    "# data.plot(color ='red')\n",
    "# ax= GeoDataFrame(geometry=[one_poly],crs=project_crs).plot(edgecolor='blue', linewidth=2.0, facecolor='none')\n",
    "\n",
    "# GeoDataFrame(geometry=[pnt_f,pnt_l],crs=project_crs).plot(ax= ax, color='red')\n",
    "# GeoDataFrame(geometry=lines_pnt_geo,crs=project_crs).plot(ax=ax,color ='red')\n",
    "ax= GeoDataFrame(geometry=[LineString(lines_pnt_geo)],crs=project_crs).plot( color ='blue')\n",
    "# GeoDataFrame(geometry=[LineString([pnt_f,pnt_l])],crs=project_crs).plot(ax=ax,color ='orange')\n",
    "# GeoDataFrame(geometry=[pnt_f,pnt_l],crs=project_crs).plot(ax= ax, color='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Store for test\n",
    "data_store =f'{data_folder}/add_more_points_to_line/'\n",
    "lines_in_buffer.to_file(f'{data_store}lines.shp')\n",
    "GeoDataFrame(geometry=[LineString([pnt_f,pnt_l])],crs=project_crs).to_file(f'{data_store}new_line.shp')\n",
    "GeoDataFrame(geometry=[LineString(lines_pnt_geo)],crs=project_crs).to_file(f'{data_store}all_line.shp')\n",
    "GeoDataFrame(geometry=lines_pnt_geo,crs=project_crs).to_file(f'{data_store}pnts.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "# Plot more results\n",
    "ax = data.plot(color ='blue')\n",
    "# ax= lines_in_buffer.plot(color ='yellow')\n",
    "# GeoDataFrame(geometry=[LineString(lines_pnt_geo)],crs=project_crs).plot(ax=ax,color ='orange')\n",
    "# GeoDataFrame(geometry=[one_poly],crs=project_crs).plot(ax=ax,edgecolor='blue', linewidth=2.0, facecolor='none')\n",
    "# GeoDataFrame(geometry=[pnt_f,pnt_l],crs=project_crs).plot(ax= ax, color='red')\n",
    "# #\n",
    "# GeoDataFrame(geometry=[LineString([pnt_f,pnt_l])],crs=project_crs).plot(ax=ax,color ='brown')\n",
    "# lines_in_buffer.plot(ax=ax,color ='yellow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "# More visualizations  for test\n",
    "data_store = f'{data_folder}/add_more_points_to_line/{name}/'\n",
    "os.makedirs(data_store,exist_ok = True)\n",
    "\n",
    "GeoDataFrame(geometry=[LineString(lines_pnt_geo)],crs=project_crs).to_file(f'{data_store}final_line.shp')\n",
    "GeoDataFrame(geometry=[one_poly],crs=project_crs).to_file(f'{data_store}one_poly.shp')\n",
    "\n",
    "# ax= GeoDataFrame(geometry=[one_poly],crs=project_crs).plot(edgecolor='blue', linewidth=2.0, facecolor='none')\n",
    "# GeoDataFrame(geometry=[pnt_f,pnt_l],crs=project_crs).plot(ax= ax, color='red')\n",
    "GeoDataFrame(geometry=lines_pnt_geo,crs=project_crs).to_file(f'{data_store}new_lines_pnt.shp')\n",
    "lines_in_buffer.to_file(f'{data_store}lines_in_buffer.shp')\n",
    "GeoDataFrame(geometry=[LineString([pnt_f,pnt_l])],crs=project_crs).to_file(f'{data_store}first_line.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test intersection - update names\n",
    "# Start from here\n",
    "with open(f'{data_folder}/intersection/network_2.pkl', \"rb\") as f:\n",
    "    df1= pickle.load(f)\n",
    "with open(f'{data_folder}/intersection/extend_lines_f.pkl', \"rb\") as f:\n",
    "    org_gpd= pickle.load(f)\n",
    "\n",
    "# Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "df3 = df1[df1['name'].notna()]\n",
    "# df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "df4 = df1[df1['name'].isna()]\n",
    "# df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "# use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "old_index  ='old_index'\n",
    "\n",
    "df = gpd.sjoin(df4, org_gpd).reset_index(names='old_index')\n",
    "# Create a new dictionary to store the updated data.\n",
    "dic_str_data = {}\n",
    "# Define the relevant columns to store\n",
    "rel_col  =[col for col in df.columns if col.endswith(\"right\")]+['geometry']\n",
    "rel_col.remove('index_right')\n",
    "\n",
    "# nore code should be added here to update name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code to consolidate nearest intersections\n",
    "1. Get the first/start of each line\n",
    "2. Make sure I have the name of the lines associated with these lines\n",
    "3. Use DBSCAN with 20 meters threshold\n",
    "4. For each group\n",
    "   4.1.dffds\n",
    "   4.2 fj;ldsfads\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
