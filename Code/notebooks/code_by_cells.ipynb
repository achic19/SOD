{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from geopandas import GeoDataFrame, GeoSeries\n",
    "from osmnx import io\n",
    "\n",
    "from pandas import DataFrame\n",
    "project_crs = 'epsg:3857'\n",
    "from sklearn.cluster import DBSCAN\n",
    "from shapely.geometry import  Point, LineString, MultiPolygon, MultiPoint\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from momepy import remove_false_nodes,extend_lines\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "import ast # to convert str with list to list of string\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from math import log2\n",
    "def get_five_largest_streets(simplified_network:GeoDataFrame,name_col:str,path:str):\n",
    "    \"\"\"\n",
    "    FOR TEST PURPOSES - get the five largest streets\n",
    "    :param simplified_network:\n",
    "    :param name_col: the column that stores the street name\n",
    "    :param path: the file name to store the longest street\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Dissolve the GeoDataFrame based on 'street_id' to aggregate polylines of the same street\n",
    "    dissolved_gdf = simplified_network[simplified_network['is_simplified']==1].dissolve(by=name_col)\n",
    "    dissolved_gdf['street_length'] = dissolved_gdf['geometry'].length\n",
    "    dissolved_gdf.sort_values(by='street_length',ascending=False).head(5).to_file(f'{path}.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Green;font-size: 30px\">Module 1:Preliminary work</span>\n",
    "<ul> <li>Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.</li>\n",
    " <li>Identify roundabout elements, if any exist, and store them in a separate DataFrame.</li>\n",
    "  <li>Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.</li>\n",
    "   <li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li>\n",
    "   </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"color: Red;font-size: 30px\">Module 2 -3:Detect  parallel streets segments and merge them </span>\n",
    "<ul> <li>For each group of streets with the same name search for parallel segments</li>\n",
    " <li> Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed</li>\n",
    "  <li>The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "    <ul><li>Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.</li></ul>\n",
    "    </li>\n",
    "\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turin\n"
     ]
    },
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Achituv\\\\My_Drive\\\\current_projects\\\\SOD\\\\Code/places/Turin_test'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run when initialise the code\n",
    "# In this example, the data is extracted from OSM by specifying a location's name, but you can also download data using a specified polygon. The code is designed to handle multiple polygons or location names seamlessly.\n",
    "\n",
    "# Download data from OpenStreetMap, project it, and convert it to a GeoDataFrame. OSMnx automatically resolves topology errors and retrieves only the street-related polylines.\n",
    "\n",
    "place = 'Turin'\n",
    "print(place)\n",
    "data_folder  = f'{pjr_loc}/places/{place.replace(\",\",\"_\").replace(\" \",\"_\")}_test'\n",
    "os.makedirs(f'{data_folder}/delete_2_nodes',exist_ok = True)\n",
    "os.makedirs(f'{data_folder}/split_tp_intersection',exist_ok = True)\n",
    "smplfcton_fldr = f'{data_folder}/simplification'\n",
    "data_folder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m     useful_tags_path \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname:en\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhighway\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbearing\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtunnel\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjunction\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      4\u001B[0m     ox\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mconfig(useful_tags_way\u001B[38;5;241m=\u001B[39museful_tags_path)\n\u001B[1;32m----> 5\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43mox\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_from_place\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnetwork_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m graph \u001B[38;5;241m=\u001B[39m ox\u001B[38;5;241m.\u001B[39mbearing\u001B[38;5;241m.\u001B[39madd_edge_bearings(graph, precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m graph_pro \u001B[38;5;241m=\u001B[39m ox\u001B[38;5;241m.\u001B[39mprojection\u001B[38;5;241m.\u001B[39mproject_graph(graph, to_crs\u001B[38;5;241m=\u001B[39mproject_crs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:351\u001B[0m, in \u001B[0;36mgraph_from_place\u001B[1;34m(query, network_type, simplify, retain_all, truncate_by_edge, which_result, buffer_dist, clean_periphery, custom_filter)\u001B[0m\n\u001B[0;32m    348\u001B[0m utils\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConstructed place geometry polygon(s) to query API\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# create graph using this polygon(s) geometry\u001B[39;00m\n\u001B[1;32m--> 351\u001B[0m G \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_from_polygon\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpolygon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnetwork_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnetwork_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43msimplify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimplify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretain_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncate_by_edge\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncate_by_edge\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_periphery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_periphery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_filter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m utils\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgraph_from_place returned graph with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(G)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m nodes and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(G\u001B[38;5;241m.\u001B[39medges)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m edges\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m G\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\graph.py:432\u001B[0m, in \u001B[0;36mgraph_from_polygon\u001B[1;34m(polygon, network_type, simplify, retain_all, truncate_by_edge, clean_periphery, custom_filter)\u001B[0m\n\u001B[0;32m    429\u001B[0m poly_buff, _ \u001B[38;5;241m=\u001B[39m projection\u001B[38;5;241m.\u001B[39mproject_geometry(poly_proj_buff, crs\u001B[38;5;241m=\u001B[39mcrs_utm, to_latlong\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    431\u001B[0m \u001B[38;5;66;03m# download the network data from OSM within buffered polygon\u001B[39;00m\n\u001B[1;32m--> 432\u001B[0m response_jsons \u001B[38;5;241m=\u001B[39m \u001B[43mdownloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_osm_network_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoly_buff\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnetwork_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_filter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;66;03m# create buffered graph from the downloaded data\u001B[39;00m\n\u001B[0;32m    435\u001B[0m bidirectional \u001B[38;5;241m=\u001B[39m network_type \u001B[38;5;129;01min\u001B[39;00m settings\u001B[38;5;241m.\u001B[39mbidirectional_network_types\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\downloader.py:548\u001B[0m, in \u001B[0;36m_osm_network_download\u001B[1;34m(polygon, network_type, custom_filter)\u001B[0m\n\u001B[0;32m    546\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m polygon_coord_str \u001B[38;5;129;01min\u001B[39;00m polygon_coord_strs:\n\u001B[0;32m    547\u001B[0m     query_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moverpass_settings\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m;(way\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mosm_filter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m(poly:\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpolygon_coord_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m);>;);out;\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 548\u001B[0m     response_json \u001B[38;5;241m=\u001B[39m \u001B[43moverpass_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    549\u001B[0m     response_jsons\u001B[38;5;241m.\u001B[39mappend(response_json)\n\u001B[0;32m    550\u001B[0m utils\u001B[38;5;241m.\u001B[39mlog(\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot all network data within polygon from API in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(polygon_coord_strs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m request(s)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    552\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\downloader.py:751\u001B[0m, in \u001B[0;36moverpass_request\u001B[1;34m(data, pause, error_pause)\u001B[0m\n\u001B[0;32m    748\u001B[0m base_endpoint \u001B[38;5;241m=\u001B[39m settings\u001B[38;5;241m.\u001B[39moverpass_endpoint\n\u001B[0;32m    750\u001B[0m \u001B[38;5;66;03m# resolve url to same IP even if there is server round-robin redirecting\u001B[39;00m\n\u001B[1;32m--> 751\u001B[0m \u001B[43m_config_dns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_endpoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    753\u001B[0m \u001B[38;5;66;03m# define the Overpass API URL, then construct a GET-style URL as a string to\u001B[39;00m\n\u001B[0;32m    754\u001B[0m \u001B[38;5;66;03m# hash to look up/save to cache\u001B[39;00m\n\u001B[0;32m    755\u001B[0m url \u001B[38;5;241m=\u001B[39m base_endpoint\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/interpreter\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\momepy_env\\lib\\site-packages\\osmnx\\downloader.py:314\u001B[0m, in \u001B[0;36m_config_dns\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m    312\u001B[0m host \u001B[38;5;241m=\u001B[39m urlparse(url)\u001B[38;5;241m.\u001B[39mnetloc\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 314\u001B[0m     ip \u001B[38;5;241m=\u001B[39m \u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgethostbyname\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;66;03m# this error occurs sometimes when using a proxy. instead, you must\u001B[39;00m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;66;03m# get IP address using google's public JSON API for DNS over HTTPS\u001B[39;00m\n\u001B[0;32m    318\u001B[0m     ip \u001B[38;5;241m=\u001B[39m _get_host_by_name(host)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Download data from OSM\n",
    "if place =='Tel Aviv':\n",
    "    useful_tags_path = ['name:en','highway','length','bearing','tunnel','junction']\n",
    "    ox.utils.config(useful_tags_way=useful_tags_path)\n",
    "graph = ox.graph_from_place(place, network_type='all')\n",
    "graph = ox.bearing.add_edge_bearings(graph, precision=1)\n",
    "graph_pro = ox.projection.project_graph(graph, to_crs=project_crs)\n",
    "io.save_graph_geopackage(graph_pro, filepath=f'{data_folder}/osm_data.gpkg', encoding='utf-8', directed=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Run when initialise the code\n",
    "# find and store roundabout\n",
    "my_gdf = gpd.read_file(f'{data_folder}/osm_data.gpkg',layer = 'edges')# Identify roundabout elements, if any exist, and store them in a separate DataFrame.\n",
    "if place =='Tel Aviv':\n",
    "    my_gdf.rename(columns={'name:en':'name'}, inplace=True)\n",
    "is_junction= True if 'junction' in my_gdf.columns else False\n",
    "if is_junction:\n",
    "    round_about = my_gdf[my_gdf['junction'].isin(['roundabout', 'circular'])]\n",
    "    my_gdf= my_gdf[~((my_gdf['junction'] == 'roundabout') | (my_gdf['junction'] == 'circular'))]\n",
    "\n",
    "# Remove additional irrelevant line objects based on values of the OSM 'tunnel' and 'highway' keys.\n",
    "# if 'tunnel' in my_gdf.columns:\n",
    "#     my_gdf = my_gdf[~((my_gdf['tunnel'] == 'building_passage') | (my_gdf['tunnel'] == 'yes'))]\n",
    "to_remove = my_gdf[~((my_gdf['highway'] == 'motorway') | (my_gdf['highway'] == 'trunk')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'motorway_link')| (my_gdf['highway'] == 'trunk_link'))]\n",
    "\n",
    "\n",
    "# Eliminate polylines that lack a name and calculate angles ranging from 0 to 180 degrees based on the bearing field.\n",
    "df_pro = to_remove.to_crs(project_crs).dropna(subset=['name'])\n",
    "df_pro = df_pro[df_pro['name']!='']\n",
    "df_pro['angle'] = df_pro['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "df_pro['length'] = df_pro.length\n",
    "\n",
    "# Function to convert valid list strings to lists\n",
    "def convert_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)[0]\n",
    "    except (ValueError, SyntaxError,TypeError):\n",
    "        return s  # Return the original string if conversion fails\n",
    "\n",
    "# Apply the function to the DataFrame column so polylines with several street names will return the first name and highway type.\n",
    "df_pro['name'] = df_pro['name'].apply(convert_to_list)\n",
    "df_pro['highway'] = df_pro['highway'].apply(convert_to_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# region\n",
    "\n",
    "# Functions and classes to be utilized - Module 2\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    # See if there are parallel lines\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left')) # we need to offset by both sides since the parallel lines could be in opposite directions\n",
    "    def is_parallel(my_s_join: GeoDataFrame, the_buffer: GeoSeries, geo_field: str):\n",
    "        my_s_join['geometry'] = my_s_join[geo_field]\n",
    "        new_data_0 = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner').reset_index()\n",
    "        if not len(new_data_0):\n",
    "            return False\n",
    "        new_data_1= new_data_0[new_data_0['index'] != new_data_0['index_right']] # Remove overlay of polylines with its buffer\n",
    "        for translated_line in new_data_1.iterrows():\n",
    "            translated_line = translated_line[1]\n",
    "            geo_tr_line =GeoDataFrame(data= pd.DataFrame([translated_line]),crs=project_crs)\n",
    "            overlay = gpd.overlay(geo_tr_line, GeoDataFrame(geometry=the_buffer.loc[geo_tr_line['index_right']], crs=project_crs), how='intersection')\n",
    "            if (overlay.length/translated_line.length).iloc[0]*100>10:\n",
    "                return True\n",
    "        return False\n",
    "    if is_parallel(to_translate, my_buffer, 'geometry_right'):\n",
    "        return True\n",
    "    else:\n",
    "        if is_parallel(to_translate, my_buffer, 'geometry_left'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    This method calculate new line between the farthest points of the simplified polygon\n",
    "    :param one_poly:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lines_in_buffer = data.sjoin(GeoDataFrame(geometry=[one_poly],crs=project_crs)).drop(columns='index_right')\n",
    "\n",
    "    list_pnts_of_line_group= []\n",
    "    def update_list(line_local):\n",
    "        \"\"\"\n",
    "        add the first start/end point into the list\n",
    "        :param line_local:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        list_pnts_of_line_group.extend([Point(line_local.coords[0]), Point(line_local.coords[-1])])\n",
    "\n",
    "    # Get the start/end points of these polylines\n",
    "    lines_in_buffer['geometry'].apply(update_list)\n",
    "\n",
    "    # Find all the unidirectional combinations between each two pair of points\n",
    "    point_combinations = list(combinations(list_pnts_of_line_group, 2))\n",
    "\n",
    "\n",
    "    # Save it into DataFrame frame and calculate distance\n",
    "    df_test= DataFrame()\n",
    "    df_test['point_1'] = [pair[0] for pair in point_combinations]\n",
    "    df_test['point_2'] = [pair[1] for pair in point_combinations]\n",
    "    df_test['dist'] = df_test.apply(lambda x:x['point_1'].distance(x['point_2']),axis=1)\n",
    "\n",
    "\n",
    "    # Calculate  angle (0 and 180)\n",
    "    # Calculate angle using vectorized operations\n",
    "    # Vectorized angle calculation using NumPy\n",
    "    dx = df_test['point_2'].apply(lambda p: p.x)  - df_test['point_1'].apply(lambda p: p.x)\n",
    "    dy = df_test['point_2'].apply(lambda p: p.y)  - df_test['point_1'].apply(lambda p: p.y)\n",
    "    df_test['angle'] = np.degrees(np.arctan2(dy, dx))\n",
    "    df_test['angle'] = np.where(df_test['angle'] > 0, df_test['angle'], df_test['angle'] + 180)\n",
    "\n",
    "\n",
    "    # Calculate the best two points by looking on their distance and angle. we compare the angle to the polylines angles. The angle has less important so the reason for 0.5\n",
    "    avg= lines_in_buffer['angle'].mean()\n",
    "    dis = abs(df_test['angle'] -avg)\n",
    "    df_test['ratio'] = df_test['dist']/df_test['dist'].max() + 0.5*dis /dis.max()\n",
    "    max_points = df_test.sort_values(by='ratio',ascending=False).iloc[0]\n",
    "\n",
    "    # These points will be served to be initial reference in order to find more points\n",
    "    pnt_f = max_points['point_1']\n",
    "    pnt_l = max_points['point_2']\n",
    "\n",
    "    angl_rng = lines_in_buffer['angle'].max() - lines_in_buffer['angle'].min()\n",
    "    if angl_rng <1: # If the angel range is less than 1 degree the line will be based on the first and last points\n",
    "            lines_pnt_geo = [pnt_f]\n",
    "    else:\n",
    "        if angl_rng > 100: # Maximum of length to check is every 10 meters\n",
    "            length_to_check  =10\n",
    "        else:\n",
    "            length_to_check  =75-log2(angl_rng)*10 # The range of  length_to_check (logarithm to create more changes at the beginning)\n",
    "        lines_pnt_geo = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f],length_to_check,lines_in_buffer)\n",
    "    lines_pnt_geo.append(pnt_l)\n",
    "    # Update dic_final\n",
    "    return lines_pnt_geo\n",
    "def add_more_pnts_to_new_lines(pnt_f_loc: Point, pnt_l_loc: Point, line_pnts: list, lngth_chck: float, test_poly:GeoDataFrame) -> list:\n",
    "    \"\"\"\n",
    "    This method checks if more points should be added to the new lines by checking along the new line if the distance to the old network roads are more than 10 meters\n",
    "    :param test_poly: From these polylines find the closet one in each interation\n",
    "    :param lngth_chck: Used latter to find how many checks should be done\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Calculate distance and azimuth between the first and last point\n",
    "    dist = pnt_f_loc.distance(pnt_l_loc)\n",
    "    x_0 = pnt_f_loc.coords[0][0]\n",
    "    y_0 = pnt_f_loc.coords[0][1]\n",
    "    bearing = math.atan2(pnt_l_loc.coords[0][0] - x_0, pnt_l_loc.coords[0][1] - y_0)\n",
    "    bearing = bearing + 2 * math.pi if bearing < 0 else bearing\n",
    "    # Calculate the number of  checks going to carry out\n",
    "    loops = int(dist / lngth_chck)\n",
    "    # Calculate  the first point over the line\n",
    "    for dis_on_line in range(1, loops):\n",
    "        x_new = x_0 + lngth_chck * dis_on_line * math.sin(bearing)\n",
    "        y_new = y_0 + lngth_chck * dis_on_line * math.cos(bearing)\n",
    "        # S_joins to all the network lines (same name and group)\n",
    "        # if the distance is less than 10 meters continue, else: find the projection point and add it to the correct location and run the function agein\n",
    "        one_pnt_df = GeoDataFrame(geometry=[Point(x_new, y_new)], crs=project_crs)\n",
    "        s_join_loc = one_pnt_df.sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "\n",
    "        if s_join_loc['dis'] > 10:\n",
    "            line = data.loc[s_join_loc['index_right']]['geometry']\n",
    "            pnt_med= line.interpolate(line.project( s_join_loc['geometry']))\n",
    "            if pnt_med.distance(pnt_f_loc)<10: # Otherwise the code may stack in endless loops\n",
    "                continue\n",
    "            line_pnts.append(pnt_med)\n",
    "            line_pnts = add_more_pnts_to_new_lines(pnt_med, pnt_l_loc, line_pnts,lngth_chck,test_poly)\n",
    "            return line_pnts\n",
    "    return line_pnts\n",
    "def update_df_with_center_line(new_line,is_simplified=0,group_name= -1):\n",
    "    \"\"\"\n",
    "    update our dictionary with new lines\n",
    "    :param is_simplified:\n",
    "    :param new_line:\n",
    "    :param group_name: According to the DBSCAN algorithm, if no =-1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    # dic_final['geometry'].append(LineString(coordinates=(pnt_list[max_dis[0]], pnt_list[max_dis[1]])))\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "\n",
    "# Initiate dic_final here for @def update_df_with_center_line\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [],'is_simplified':[]}\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Make sure @dic_final is empty before execute this code\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dic_final\u001B[38;5;241m=\u001B[39m {key: [] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdic_final\u001B[49m}\n\u001B[0;32m      3\u001B[0m my_groupby \u001B[38;5;241m=\u001B[39m df_pro\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# group the street segments by street name\u001B[39;00m\n\u001B[0;32m      4\u001B[0m for_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(my_groupby)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dic_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure @dic_final is empty before execute this code\n",
    "dic_final= {key: [] for key in dic_final}\n",
    "my_groupby = df_pro.groupby('name') # group the street segments by street name\n",
    "for_time = len(my_groupby)\n",
    "\n",
    "number_of_parallel = 0 # count the number of polylines were refined\n",
    "with tqdm(total=for_time) as pbar: #  It is used in order to visualise the progress by progress bar\n",
    "    for i, street in enumerate(my_groupby):\n",
    "        res = street[1] # it holds all the streets\n",
    "        name = street[0] # It holds the streets name\n",
    "        pbar.update(1) # for the progress bar\n",
    "        # Remove segments without angle. If less than two segments being left move to the next group.\n",
    "        res = res.dropna(subset=['angle'], axis=0)\n",
    "        if len(res) < 2:\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # Use DBSCAN to classify streets based on their angle, and group each class. Outliers could not consider parallel with any street, thus removed\n",
    "        res['group'] = DBSCAN(eps=10, min_samples=2).fit(res['angle'].to_numpy().reshape(-1, 1)).labels_\n",
    "        # if all is -1, don't touch the element\n",
    "        if (res['group']== -1).all():\n",
    "            data = res\n",
    "            _  = res['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "            continue\n",
    "        # cur_group = res[(res['group'] > -1) | (res.length>20)].groupby('group') # Remove short segments with -1 classification values\n",
    "        # The parallel test is on street segments that  have the same name and belong to the same angle group.\n",
    "        for group in res.groupby('group'):\n",
    "            data = group[1]\n",
    "            if group[0] ==-1: # No need to check if is parallel\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "                continue\n",
    "            if check_parallelism(data.copy()):\n",
    "                # print(group[0])\n",
    "                # Remove unimportant streets which appear more than 10% in the group\n",
    "                min_num_of_polylines = len(data)/10\n",
    "                # Use a single boolean condition for filtering\n",
    "                condition = (data['highway'].isin(['service','unclassified'])) & (data.groupby('highway')['highway'].transform('count') <= min_num_of_polylines)\n",
    "                data = data[~condition]\n",
    "\n",
    "                number_of_parallel+=len(data) # Update the number of parallel polylines\n",
    "\n",
    "                # unify lines to one polygon\n",
    "                buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "                one_buffer = buffers.unary_union\n",
    "                # simplify polygon with simplify function. If one_buffer is multipolygon object simplify each one them separately\n",
    "                if isinstance(one_buffer, MultiPolygon):\n",
    "                    for polygon in one_buffer:\n",
    "                        lines_pnt_geo_final = create_center_line(polygon)\n",
    "                        update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "                else:\n",
    "                    lines_pnt_geo_final =create_center_line(one_buffer)\n",
    "                    # Update dic_final\n",
    "                    update_df_with_center_line(LineString(lines_pnt_geo_final),1,group[0])\n",
    "\n",
    "            else:\n",
    "                _  = data['geometry'].apply(lambda x:update_df_with_center_line(x))\n",
    "\n",
    "print(number_of_parallel)\n",
    "print('create new files')\n",
    "# remove short lines\n",
    "final_cols = ['name', 'geometry', 'highway', 'bearing', 'length']\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "new_network['length']= new_network.length\n",
    "# create network\n",
    "new_network.to_file(f'{smplfcton_fldr}/simp.shp')\n",
    "# save the data as a pickle format\n",
    "\n",
    "# save another file for test\n",
    "get_five_largest_streets(new_network,'name',f'{smplfcton_fldr}/test_simp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "# region\n",
    "# Classes to be employed during the execution of this code.\n",
    "#Intersection\n",
    "#Split in intersection\n",
    "class Intersection:\n",
    "    def __init__(self,network:GeoDataFrame,number:int):\n",
    "        \"\"\"\n",
    "\n",
    "        :param network:\n",
    "        :param number: give a unique name to the files created during the process (this class will be use again in this code)\n",
    "        \"\"\"\n",
    "        self.my_network = network\n",
    "        self.inter_pnt_dic = {'geometry':[],'name':[]}\n",
    "        self.lines_to_delete =[]\n",
    "        self.num = number\n",
    "    def delete_false_intersection(self,name_to_splt='name'):\n",
    "\n",
    "        if 'length' in self.my_network.columns: # To run the code smoothly we need to remove 'length' col if exist\n",
    "            self.my_network.drop(columns='length',inplace= True)\n",
    "\n",
    "        # It should be executed twice in order to clean all\n",
    "        for _  in range(2):\n",
    "            # First clean all the false node\n",
    "            self.my_network = remove_false_nodes(self.my_network)\n",
    "            # the previous function has changed the topology so the length should be updated\n",
    "            self.my_network['length'] =self.my_network.length\n",
    "            self.my_network  =self.my_network.drop_duplicates(subset='length') # remove false intersection duplicate many polyline which should be removed\n",
    "            self.my_network.reset_index(drop=True,inplace= True) # Changes has been made to the geometry so the index should be reset\n",
    "\n",
    "\n",
    "    def intersection_network(self):\n",
    "\n",
    "        # Create buffer around each element\n",
    "        buffer_around_lines= self.my_network['geometry'].buffer(cap_style=3, distance=1, join_style=3)\n",
    "\n",
    "\n",
    "        # s_join between buffer to lines\n",
    "        s_join_0 =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_lines,crs=project_crs),right_df=self.my_network)\n",
    "\n",
    "        # delete lines belong to the buffer\n",
    "        s_join = s_join_0[s_join_0.index!=s_join_0['index_right']]\n",
    "\n",
    "\n",
    "        # Find new intersections that are not at the beginning or end of the line\n",
    "        for_time =len(s_join)\n",
    "        with tqdm(total=for_time) as pbar:\n",
    "            s_join.apply(lambda x: self.find_intersection_points(x,pbar), axis=1)\n",
    "        if len(self.inter_pnt_dic)==0:\n",
    "            return\n",
    "        inter_pnt_gdf = GeoDataFrame(self.inter_pnt_dic,crs=project_crs)\n",
    "\n",
    "        # Split string line by points\n",
    "        segments = {'geometry':[],'org_id':[]}\n",
    "        # Groupby points name (which is the line they should split)\n",
    "        my_groups =  inter_pnt_gdf.groupby('name')\n",
    "        for_time = len(my_groups)\n",
    "        with  tqdm(total=for_time) as pbar:\n",
    "            for group_pnts in my_groups :\n",
    "                pbar.update(1)\n",
    "                points  = group_pnts[1]\n",
    "                points['is_split'] = True\n",
    "\n",
    "                # get the line to split by comparing the name\n",
    "                row = self.my_network.loc[group_pnts[0]]\n",
    "                current = list(row.geometry.coords)\n",
    "                points_line = [Point(x) for x in current]\n",
    "                points_line_gdf = GeoDataFrame(geometry=points_line,crs=project_crs)\n",
    "                points_line_gdf['is_split'] = False\n",
    "\n",
    "                # append all the points together (line points and split points)\n",
    "                line_all_pnts = points_line_gdf.append(points)\n",
    "\n",
    "                # Find the distance of each point form the begining of the line on the line.\n",
    "                line_all_pnts['dis_from_the_start'] = line_all_pnts['geometry'].apply(lambda x:row.geometry.project(x))\n",
    "                line_all_pnts.sort_values('dis_from_the_start',inplace=True)\n",
    "\n",
    "                # split the line\n",
    "                seg =[]\n",
    "                for point in line_all_pnts.iterrows():\n",
    "                    prop = point[1]\n",
    "                    seg.append(prop['geometry'])\n",
    "                    if prop['is_split']:\n",
    "                        segments['geometry'].append(LineString(seg))\n",
    "                        segments['org_id'].append(row.name)\n",
    "                        seg = [prop['geometry']]\n",
    "                # if the split point is the last one, you don't need to create new segment\n",
    "                if len(seg)>1:\n",
    "                    segments['geometry'].append(LineString(seg))\n",
    "                    segments['org_id'].append(row.name)\n",
    "        network_split = GeoDataFrame(data=segments,crs=project_crs)\n",
    "        cols_no_geometry = self.my_network.columns[:-1]\n",
    "        network_split_final = network_split.set_index('org_id')\n",
    "        network_split_final[cols_no_geometry] =self.my_network[cols_no_geometry]\n",
    "\n",
    "        # remove old and redundant line from our network and update with new one\n",
    "        network_split =self.my_network.drop(index=network_split_final.index.unique()).append(network_split_final).drop(index= self.lines_to_delete)\n",
    "        network_split['length'] = network_split.length\n",
    "        self.my_network = network_split\n",
    "        self.my_network.reset_index(drop=True,inplace= True)\n",
    "\n",
    "    def find_intersection_points(self,row,pbar):\n",
    "        r\"\"\"\n",
    "        find the intersection points between the two lines\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pbar.update(1)\n",
    "            line_1 = self.my_network.loc[row.name]\n",
    "            line_2 =  self.my_network.loc[row['index_right']]\n",
    "            pnt = line_1.geometry.intersection(line_2.geometry)\n",
    "            # If there are more than one intersection between two lines, one of the lines should be deleted.\n",
    "            if isinstance(pnt,LineString):\n",
    "                return\n",
    "            if isinstance(pnt,MultiPoint):\n",
    "                temp_line= line_1.name if line_1.length< line_2.length else line_2.name\n",
    "                if temp_line not in self.lines_to_delete:\n",
    "                    self.lines_to_delete.append(temp_line)\n",
    "                return\n",
    "            # If it is first or end continue OR if there is no intersection between the two lines\n",
    "            if len(pnt.coords)==0 or pnt.coords[0]==line_1.geometry.coords[0] or pnt.coords[0]==line_1.geometry.coords[-1]:\n",
    "                return\n",
    "            self.inter_pnt_dic['geometry'].append(pnt)\n",
    "            self.inter_pnt_dic['name'].append(row.name)\n",
    "        except:\n",
    "            print(f\"{row.name},{row['index_right']}:{pnt}\")\n",
    "    def update_names(self, org_gpd:GeoDataFrame):\n",
    "        \"\"\"\n",
    "        It updates the name of those lost their name during the previous process\n",
    "        :param org_gpd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df1 = self.my_network\n",
    "        # Split df1 into two GeoDataFrames: df3 (with names) and df4 (without names)\n",
    "        df3 = df1[df1['name'].notna()]\n",
    "        # df3.to_file(f'{data_folder}/delete_2_nodes/with_name.shp')\n",
    "        df4 = df1[df1['name'].isna()]\n",
    "        # df4.reset_index().to_file(f'{data_folder}/delete_2_nodes/no_name_init.shp')\n",
    "\n",
    "        # use only one polyline from the original dataframe for name even if the algorithm may found more\n",
    "        old_index  ='old_index'\n",
    "        df4_as_buffer= GeoDataFrame(geometry=df4['geometry'].buffer(distance  = 2, cap_style=2),crs=project_crs)\n",
    "        df = gpd.sjoin(df4_as_buffer, org_gpd) # for spatial join use buffer around each polyline.that provide better result\n",
    "        df.index.name = old_index\n",
    "        df['geometry'] = df4['geometry'] # bring the dataframe into linestring format\n",
    "        df.reset_index(inplace=True) # To be consistent with the following code and other dataframe\n",
    "        # Create a new dictionary to store the updated data.\n",
    "        dic_str_data = []\n",
    "\n",
    "\n",
    "        def return_street_name(aplcnts_tst):\n",
    "            \"\"\"\n",
    "            1. \"Count the occurrences of polylines with the same name within each aplcnts_tst.\"\n",
    "            2. \"Return the street if a aplcnts_tst contains only one unique street name.\"\n",
    "            3. \"If a single street name predominates within a aplcnts_tst, return that name.\"\n",
    "            4. \"For groups with multiple names, perform a buffer calculation around the respective polylines and determine the largest overlapping area, returning the name associated with that area.\"\n",
    "            :param aplcnts_tst: group of applicants. Some of them hold the correct street name\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            count_names = aplcnts_tst['name'].value_counts().sort_values(ascending=False)\n",
    "            if len(count_names)==1:\n",
    "                # there is only one name\n",
    "                my_data = aplcnts_tst.iloc[0]\n",
    "            elif count_names[1]- count_names[0]>1:\n",
    "                # The highest number of polylines with the same name are bigger at least in 2:\n",
    "                my_data = aplcnts_tst[aplcnts_tst['name'] == count_names.index[0]].iloc[0]\n",
    "            else:\n",
    "                # otherwise filter those with the most popular name or close to (-1)\n",
    "                str_to_wrk_on  =aplcnts_tst[aplcnts_tst['name'].isin(count_names[count_names - count_names[0] < 2].index)]\n",
    "                buffer_0 = GeoDataFrame(geometry=[str_to_wrk_on.iloc[0]['geometry'].buffer(distance  = 20, cap_style=2)],crs=project_crs) # Buffer around the polyline without name\n",
    "\n",
    "                streets_right_geo = org_gpd[org_gpd.index.isin(str_to_wrk_on['index_right'])].reset_index() # Get all the applicants polylines and create buffer around\n",
    "                buffer_1 =GeoDataFrame(geometry=streets_right_geo.buffer(distance  = 20, cap_style=2))\n",
    "                streets_right_geo['area'] =gpd.overlay(buffer_1, buffer_0, how='intersection').area\n",
    "                groupy = streets_right_geo.groupby('name')\n",
    "                my_data_0 = groupy.get_group(groupy.sum()['area'].sort_values(ascending=False).index[0]).sort_values(by= 'area',ascending=False).iloc[0]\n",
    "                # Get back to the @aplcnts_tst and find the relevant row by comparing index\n",
    "                my_data = aplcnts_tst[aplcnts_tst['index_right'] == my_data_0['index']].iloc[0]\n",
    "            # Populate the new dictionary with relevant data\n",
    "            dic_str_data.append(my_data.to_list())\n",
    "        _ =df.groupby(old_index).apply(return_street_name)\n",
    "        # convert the dictionary into a dataframe.\n",
    "        updated_df = GeoDataFrame(data= dic_str_data,columns=df.columns,crs=project_crs).drop(columns='index_right').set_index(old_index)\n",
    "        updated_df['length'] = updated_df.length\n",
    "        self.my_network = df3.append(updated_df)\n",
    "\n",
    "#Roundabout\n",
    "class EnvEntity:\n",
    "        def __init__(self,network):\n",
    "            self.dead_end_fd = None\n",
    "            self.pnt_dead_end = None\n",
    "            self.pnt_dic = {}\n",
    "            self.first_last_dic = {'geometry': [], 'line_name': [], 'position': []}\n",
    "            self.network = network\n",
    "\n",
    "\n",
    "        def __populate_pnt_dic(self,point: type, name_of_line: str):\n",
    "            \"\"\"\n",
    "            Make \"pnt_dic\" contain a list of all the lines connected to each point.\n",
    "            :param point:\n",
    "            :param name_of_line:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if not point in self.pnt_dic:\n",
    "                self.pnt_dic[point] = []\n",
    "            self.pnt_dic[point].append(name_of_line)\n",
    "\n",
    "        def __send_pnts(self,temp_line: GeoSeries):\n",
    "            \"\"\"\n",
    "            # Send the first and the last points to populate_pnt_dic\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            my_geom = temp_line['geometry']\n",
    "            self.__populate_pnt_dic(my_geom.coords[0], temp_line.name)\n",
    "            self.__populate_pnt_dic(my_geom.coords[-1], temp_line.name)\n",
    "\n",
    "        def get_deadend_gdf(self,delete_short:int =30)-> GeoDataFrame:\n",
    "            self.network.apply(self.__send_pnts, axis=1)\n",
    "\n",
    "            deadend_list = [item[1][0] for item in self.pnt_dic.items() if len(item[1]) == 1]\n",
    "            pnt_dead_end_0 = [item for item in self.pnt_dic.items() if len(item[1]) == 1] # Retain all the line points with deadened\n",
    "            self.pnt_dead_end = [Point(x[0]) for x in pnt_dead_end_0]\n",
    "            # Create shp file of deadened_pnts\n",
    "            geometry,line_name = 'geometry','line_name'\n",
    "            pnt_dead_end_df = GeoDataFrame(data=pnt_dead_end_0)\n",
    "            pnt_dead_end_df[geometry]= pnt_dead_end_df[0].apply(lambda x:Point(x))\n",
    "            pnt_dead_end_df[line_name] = pnt_dead_end_df[1].apply(lambda x:x[0])\n",
    "            pnt_dead_end_df.crs = project_crs\n",
    "            self.dead_end_fd = pnt_dead_end_df\n",
    "\n",
    "            if delete_short>0:\n",
    "                # If it is necessary to eliminate dead-end short segments, it is  important to delete them from the network geodataframe.\n",
    "\n",
    "                deadend_gdf =self.network.loc[deadend_list]\n",
    "                self.network.drop(index=deadend_gdf[deadend_gdf.length<delete_short].index,inplace=True)\n",
    "                return deadend_gdf[deadend_gdf.length>delete_short]\n",
    "            return self.network.loc[deadend_list]\n",
    "\n",
    "        def update_the_current_network(self,temp_network):\n",
    "            r\"\"\"\n",
    "            Update the current network in the new changes\n",
    "            :param temp_network:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            new_network_temp = self.network.drop(index=temp_network.index)\n",
    "            self.network = new_network_temp.append(temp_network)\n",
    "            self.network['length'] = self.network.length\n",
    "            self.network  = self.network[self.network['length']>1]\n",
    "class Roundabout(EnvEntity):\n",
    "    def __init__(self,network: GeoDataFrame):\n",
    "       EnvEntity.__init__(self,network)\n",
    "       self.pnt_dic ={}\n",
    "       self.centroid =self.__from_roundabout_to_centroid()\n",
    "       self.network.rename(columns={'name': 'str_name'}, inplace=True)\n",
    "    def __from_roundabout_to_centroid(self):\n",
    "        # Find the center of each roundabout\n",
    "        # create polygon around each polygon and union\n",
    "        round_about_buffer = round_about.to_crs(project_crs)['geometry'].buffer(cap_style=1, distance=10,\n",
    "                                                                                join_style=1).unary_union\n",
    "        dic_data = {'name': [], 'geometry': []}\n",
    "        if round_about_buffer.type=='Polygon': # In case we have only one polygon\n",
    "            dic_data['name'].append(0)\n",
    "            dic_data['geometry'].append(round_about_buffer.centroid)\n",
    "        else:\n",
    "            for ii, xx in enumerate(round_about_buffer):\n",
    "                dic_data['name'].append(ii)\n",
    "                dic_data['geometry'].append(xx.centroid)\n",
    "        centroid =GeoDataFrame(dic_data, crs=project_crs)\n",
    "        return centroid\n",
    "        # GeoDataFrame(dic_data,crs=project_crs).to_file(f'{path_round_about}/roundabout_union.shp')\n",
    "\n",
    "    def __first_last_pnt_of_line(self,row: GeoSeries):\n",
    "        r\"\"\"\n",
    "        It get geometry of line and fill the first_last_dic with the first and last point and the name of the line\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        geo = list(row['geometry'].coords)\n",
    "        self.first_last_dic['geometry'].extend([Point(geo[0]), Point(geo[-1])])\n",
    "        self.first_last_dic['line_name'].extend([row.name] * 2)\n",
    "        self.first_last_dic['position'].extend([0, -1])\n",
    "    def deadend(self):\n",
    "        r\"\"\"\n",
    "        remove not connected line shorter than 100 meters and then return deadend_list lines and their endpoints (as another file)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Find the first and last points\n",
    "\n",
    "        # Get deadend_gdf\n",
    "        deadend_gdf = self.get_deadend_gdf()\n",
    "\n",
    "        # Create gdf of line points with the reference to the line they belong\n",
    "        deadend_gdf.apply(self.__first_last_pnt_of_line, axis=1)\n",
    "        first_last_gdf = GeoDataFrame(self.first_last_dic, crs=project_crs)\n",
    "\n",
    "\n",
    "        return deadend_gdf, first_last_gdf\n",
    "    def __update_geometry(self,cur,s_join):\n",
    "        r\"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if cur['highway'] == 'footway':\n",
    "            # Don't snap footway to roundabout\n",
    "            return cur['geometry']\n",
    "        # Get only the points that are deadened\n",
    "        points_lines = [item for item in s_join[s_join['line_name'] == cur.name].iterrows()if item[1]['geometry'] in self.pnt_dead_end]\n",
    "        if len(points_lines) == 0:\n",
    "            # No roundabout nearby\n",
    "            return cur['geometry']\n",
    "        # get the line geometry to change the first and/ or last point\n",
    "        geo_cur = list(cur['geometry'].coords)\n",
    "\n",
    "        # iterate over the deadened points  near roundabout\n",
    "        for ind in range(len(points_lines)):\n",
    "            points_line = points_lines[ind]\n",
    "            geo_cur[points_line[1]['position']] = self.centroid.loc[points_line[1]['index_right']]['geometry'].coords[\n",
    "                0]\n",
    "        return LineString(geo_cur)\n",
    "    def my_spatial_join(self,deadend_lines, deadend_pnts,line_name):\n",
    "        # Spatial join between roundabout centroid to nearby dead end lines\n",
    "        # centroid = gpd.read_file(f'{path_round_about}/centroid.shp')\n",
    "        s_join = gpd.sjoin_nearest(left_df=deadend_pnts, right_df=self.centroid, how='left', max_distance=100,\n",
    "                                   distance_col='dist').dropna(subset='dist')\n",
    "\n",
    "        # Deadened lines from both lines should be removed\n",
    "        lines_to_delete_test = s_join['line_name'].unique() # all the Deadened lines close to roundabout\n",
    "\n",
    "        # All deadened lines from both lines\n",
    "        deads_both_side = self.dead_end_fd['line_name'].value_counts()\n",
    "        deads_both_side =deads_both_side[deads_both_side==2]\n",
    "\n",
    "        # Remove this lines from the database\n",
    "        lines_to_delete=deads_both_side[deads_both_side.index.isin(lines_to_delete_test)]\n",
    "\n",
    "        self.network = self.network[~((self.network[line_name].isin(lines_to_delete.index)) & (self.network.length<300))]\n",
    "        deadend_lines = deadend_lines[~((deadend_lines[line_name].isin(lines_to_delete.index)) & (deadend_lines.length<300))]\n",
    "        # Update the geometry so the roundabout will be part of the line geometry\n",
    "        change_geo = deadend_lines.copy()\n",
    "\n",
    "        change_geo['geometry'] = change_geo.apply(lambda x:self.__update_geometry(x,s_join), axis=1)\n",
    "\n",
    "        return change_geo\n",
    "# endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# starting point\n",
    "new_network = gpd.read_file(f'{smplfcton_fldr}/simp.shp').rename(columns={'is_simplif':'is_simplified'})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37215/37215 [00:06<00:00, 6014.04it/s]\n",
      "100%|██████████| 1787/1787 [00:10<00:00, 176.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num=0\n",
    "new_gpd = new_network.copy()\n",
    "obj_intersection = Intersection(new_gpd,num)\n",
    "obj_intersection.delete_false_intersection()\n",
    "obj_intersection.intersection_network()\n",
    "obj_intersection.update_names(new_gpd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# For test\n",
    "obj_intersection.my_network.reset_index().to_file(f'{data_folder}/delete_2_nodes/all_new2.shp')\n",
    "get_five_largest_streets(obj_intersection.my_network,'name',f'{data_folder}/delete_2_nodes/test_all_new2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Achituv\\\\My_Drive\\\\current_projects\\\\SOD\\\\Code/places/Turin_test'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "line_name ='line_name'\n",
    "if is_junction:\n",
    "\n",
    "    exist_data= obj_intersection.my_network.reset_index().reset_index(names=line_name)\n",
    "    my_roundabout=Roundabout(exist_data)\n",
    "    deadend_lines, deadend_pnts = my_roundabout.deadend()\n",
    "\n",
    "    # update the current network\n",
    "    change_geo = my_roundabout.my_spatial_join(deadend_lines, deadend_pnts,line_name)\n",
    "    my_roundabout.update_the_current_network(change_geo)\n",
    "\n",
    "    my_roundabout.network.drop_duplicates(subset=line_name,inplace=True)\n",
    "    # Improve roundabout\n",
    "    # First buffer around centroid\n",
    "    centr_name= 'centr_name'\n",
    "    buffer_around_centroid= my_roundabout.centroid['geometry'].buffer(cap_style=1, distance=30)\n",
    "\n",
    "    # s_join between buffer to lines (reset index to retain the original centroid name which can apper more than one in the results). always stay with data you need and with understandable name\n",
    "    roundabout_with_lines =gpd.sjoin(left_df=GeoDataFrame(geometry=buffer_around_centroid,crs=project_crs).reset_index(),right_df=my_roundabout.network[['geometry',line_name]]).drop_duplicates(subset=['index',line_name]).rename(columns={\"index\":centr_name})[['geometry',line_name,centr_name]]\n",
    "\n",
    "    # To facilitate the searching process\n",
    "    my_roundabout.network.set_index(line_name,inplace=True)\n",
    "    # To facilitate easy access to point centroid geometry data, it is advisable to store the information in an object that provides efficient retrieval.\n",
    "    pnt_centroid_temp = my_roundabout.centroid['geometry']\n",
    "    #  Group the data by centroid\n",
    "    for center_line in roundabout_with_lines.groupby(centr_name):\n",
    "        #  Iterate over each group after performing a groupby() operation\n",
    "        for center in center_line[1].itertuples():\n",
    "            # Find the line that connects to the current centroid and obtain its vertices\n",
    "            line_to_test = my_roundabout.network.loc[center[2]]\n",
    "            vertices_line = list(line_to_test['geometry'].coords)\n",
    "            pnt_test = [vertices_line[0],vertices_line[-1]]\n",
    "            # To determine if the current line is already connected to the current centroid,.\n",
    "            is_connected = my_roundabout.centroid[my_roundabout.centroid['geometry'].isin([Point(pnt_test[0]),Point(pnt_test[-1])])]\n",
    "            if len(is_connected)>0 and center[3] in is_connected['name']:\n",
    "                continue\n",
    "\n",
    "            if len(vertices_line)==2:\n",
    "                vertices_line.insert(1, pnt_centroid_temp[center[3]])\n",
    "            else:\n",
    "                my_list = [pnt_centroid_temp[center[3]].distance(Point(temp)) for temp in vertices_line]\n",
    "                # Find the minimum index\n",
    "                min_index = min(range(len(my_list)), key=my_list.__getitem__)\n",
    "                if min_index ==0:\n",
    "                    vertices_line.insert(0,pnt_centroid_temp[center[3]])\n",
    "                elif min_index == len(my_list)-1:\n",
    "                    vertices_line.append(pnt_centroid_temp[center[3]])\n",
    "                else:\n",
    "                    vertices_line[min_index] = pnt_centroid_temp[center[3]]\n",
    "            new_geo = LineString(vertices_line)\n",
    "            my_roundabout.network.at[center[2],'geometry'] = new_geo\n",
    "# Extend\n",
    "    new_network2 = my_roundabout.network.reset_index()\n",
    "    new_network2.drop(columns='index',inplace=True)\n",
    "else:\n",
    "    new_network2=  obj_intersection.my_network.reset_index()\n",
    "\n",
    "new_network2.to_file(f'{data_folder}/ra_network.shp')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 38918/51678 [00:10<00:04, 2967.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4538,3908:GEOMETRYCOLLECTION (LINESTRING (849242.3779340778 5630513.885474437, 849222.2137190588 5630513.776590061), POINT (849242.0705203642 5630514.464749842))\n",
      "3908,4538:GEOMETRYCOLLECTION (LINESTRING (849242.3779340778 5630513.885474437, 849222.2137190588 5630513.776590061), POINT (849242.0705203642 5630514.464749842))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 41014/51678 [00:10<00:03, 2984.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7431,4132:GEOMETRYCOLLECTION (LINESTRING (855488.3486609876 5638358.625024514, 855496.6251541061 5638349.487165973), POINT (855488.3869064664 5638350.620383657))\n",
      "4132,7431:GEOMETRYCOLLECTION (LINESTRING (855496.6251541061 5638349.487165973, 855488.3486609876 5638358.625024514), POINT (855488.3869064664 5638350.620383657))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51678/51678 [00:14<00:00, 3508.34it/s]\n",
      "100%|██████████| 1029/1029 [00:10<00:00, 97.89it/s]\n"
     ]
    }
   ],
   "source": [
    "extend_lines_f= extend_lines(new_network2,100)\n",
    "extend_lines_f['length'] = extend_lines_f.length\n",
    "new_gpd = extend_lines_f.copy()\n",
    "# extend_lines_f.to_file(f'{data_folder}/extend_network.shp')\n",
    "\n",
    "obj_intersection_1 = Intersection(extend_lines_f.copy(),1)\n",
    "obj_intersection_1.delete_false_intersection('str_name')\n",
    "obj_intersection_1.intersection_network()\n",
    "obj_intersection_1.my_network.rename(columns={'str_name':'name'},inplace=True) # 'str_name' become 'str to be compatible with other previous networks\n",
    "new_gpd.rename(columns={'str_name':'name'},inplace=True)\n",
    "obj_intersection_1.update_names(org_gpd=new_gpd)\n",
    "\n",
    "# Clear short segments\n",
    "final2 = EnvEntity(obj_intersection_1.my_network.reset_index())\n",
    "final2.update_the_current_network(final2.get_deadend_gdf(delete_short=100))\n",
    "final2.network.to_file(f'{data_folder}/final.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# For test\n",
    "get_five_largest_streets(final2.network,'name',f'{data_folder}/delete_2_nodes/test_all_new')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save file for test\n",
    "with open(f'{data_folder}/intersection/network_2.pkl', \"wb\") as f:\n",
    "    pickle.dump(obj_intersection_1.my_network, f)\n",
    "with open(f'{data_folder}/intersection/extend_lines_f.pkl', \"wb\") as f:\n",
    "    pickle.dump(extend_lines_f, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}