{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T07:03:34.772051Z",
     "start_time": "2025-06-29T07:03:27.709848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Run when initialise the code\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point, MultiPolygon,LineString, MultiLineString\n",
    "from shapely.ops import linemerge, unary_union,split\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.prepared import prep\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log2\n",
    "\n",
    "from itertools import groupby,combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "pjr_loc = os.path.dirname(os.getcwd())\n",
    "project_crs = 'epsg:3857'\n",
    "place = 'Turin,Italy'\n",
    "place_folder = f'{pjr_loc}/places'\n",
    "os.makedirs(place_folder , exist_ok=True)\n",
    "data_folder = f'{place_folder}/{place.replace(\",\",\"_\")}'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "test_folder = f'{data_folder}/test'\n",
    "os.makedirs(test_folder, exist_ok=True)"
   ],
   "id": "f588417895c1cfb8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:33:08.739700Z",
     "start_time": "2025-06-30T10:33:08.719812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def get_bbox_from_nominatim(place_name):\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": place_name, \"format\": \"json\", \"limit\": 1}\n",
    "    headers = {\n",
    "        \"User-Agent\": \"GeoResearcher/1.0 (achituv@ariel.ac.il)\"  # Replace with your email\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    results = r.json()\n",
    "    if not results:\n",
    "        raise ValueError(f\"No results found for place: {place_name}\")\n",
    "    bbox = results[0][\"boundingbox\"]\n",
    "    return float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])\n",
    "\n",
    "def download_osm_roads_bbox(place_name, highway_tags=None):\n",
    "    if highway_tags is None:\n",
    "        highway_tags = [ 'primary','footway','pedestrian','cycleway', 'path','secondary', 'tertiary',\n",
    "                        'unclassified', 'residential', 'service', 'living_street','steps']\n",
    "    south, north, west, east = get_bbox_from_nominatim(place_name)\n",
    "    highway_filter = '|'.join(highway_tags)\n",
    "\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    (\n",
    "      way[\"highway\"~\"{highway_filter}\"]({south},{west},{north},{east});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(overpass_url, params={'data': query})\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    roads = []\n",
    "    for element in data['elements']:\n",
    "        if element['type'] == 'way' and 'geometry' in element:\n",
    "            coords = [(p['lon'], p['lat']) for p in element['geometry']]\n",
    "            if len(coords) >= 2:\n",
    "                geom = LineString(coords)\n",
    "                tags = element.get('tags', {})\n",
    "                tags['geometry'] = geom\n",
    "                tags['osmid'] = element['id']\n",
    "                roads.append(tags)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(roads, geometry='geometry', crs='EPSG:4326')\n",
    "    return gdf\n",
    "\n",
    "def remove_unconnected_streets(df_connected, str_name='name_left', con_str_name='name_right'):\n",
    "    \"\"\"\n",
    "    Remove streets from a GeoDataFrame that are not connected to any other streets.\n",
    "\n",
    "    Parameters:\n",
    "    df_connected (GeoDataFrame): The input GeoDataFrame containing street data.\n",
    "    str_name (str): The column name for the street name in the first dataset.\n",
    "    con_str_name (str): The column name for the connected street name in the second dataset.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A new GeoDataFrame with unconnected streets removed.\n",
    "    \"\"\"\n",
    "    df_analysis = df_connected.copy()\n",
    "    s_join_analysis = gpd.sjoin(df_analysis, df_connected)\n",
    "    s_join_analysis2 = s_join_analysis[s_join_analysis[str_name] != s_join_analysis[con_str_name]]\n",
    "    not_connected = set(df_connected['name']) - set(s_join_analysis2.reset_index()[str_name])\n",
    "    df_pro_filtered = df_connected[~df_connected['name'].isin(not_connected)]\n",
    "    return df_pro_filtered\n",
    "\n",
    "def calculate_bearing(line):\n",
    "    if line is None or line.is_empty or len(line.coords) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    start, end = line.coords[0], line.coords[-1]\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "\n",
    "    angle = np.degrees(np.arctan2(dx, dy))\n",
    "    bearing = (angle + 360) % 360\n",
    "    return bearing\n",
    "\n",
    "def merge_lines_by_attributes(gdf, attrs=['name', 'highway']):\n",
    "    \"\"\"\n",
    "    Efficiently merges line geometries in a GeoDataFrame by grouping them based on specified attributes.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame with line geometries.\n",
    "        attrs (list of str): List of attribute names to group by.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: New GeoDataFrame with merged LineStrings or MultiLineStrings split into parts.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    grouped = gdf.groupby(attrs, sort=False)\n",
    "\n",
    "    for key, group in tqdm(grouped, total=len(grouped), desc=\"Merging lines\"):\n",
    "        lines = list(group.geometry)\n",
    "\n",
    "        if len(lines) == 1:\n",
    "            records.append({**dict(zip(attrs, key)), 'geometry': lines[0]})\n",
    "            continue\n",
    "\n",
    "        merged = linemerge(MultiLineString(lines))\n",
    "\n",
    "        if isinstance(merged, LineString):\n",
    "            records.append({**dict(zip(attrs, key)), 'geometry': merged})\n",
    "        elif isinstance(merged, MultiLineString):\n",
    "            for part in merged.geoms:\n",
    "                records.append({**dict(zip(attrs, key)), 'geometry': part})\n",
    "\n",
    "    return gpd.GeoDataFrame(records, crs=gdf.crs)\n",
    "\n",
    "\n",
    "def extract_internal_intersections_spatial_index(geoms):\n",
    "    \"\"\"\n",
    "    Extracts internal intersection points between geometries using spatial indexing (STRtree).\n",
    "\n",
    "    Parameters:\n",
    "        geoms (list of BaseGeometry): List of Shapely geometries (e.g., LineStrings).\n",
    "\n",
    "    Returns:\n",
    "        list of Point: Intersection points where geometries overlap internally.\n",
    "    \"\"\"\n",
    "    geoms = [g for g in geoms if isinstance(g, BaseGeometry) and not g.is_empty]\n",
    "    tree = STRtree(geoms)\n",
    "    all_points = set()\n",
    "\n",
    "    for idx, geom in enumerate(tqdm(geoms, total=len(geoms), desc=\"Extracting intersections\")):\n",
    "        candidate_idxs = tree.query(geom)\n",
    "        candidates = [geoms[i] for i in candidate_idxs]\n",
    "\n",
    "        for other in candidates:\n",
    "            if other is geom or not geom.intersects(other):\n",
    "                continue\n",
    "\n",
    "            inter = geom.intersection(other)\n",
    "            if inter.is_empty:\n",
    "                continue\n",
    "\n",
    "            if inter.geom_type == 'Point':\n",
    "                all_points.add(inter)\n",
    "            elif inter.geom_type.startswith('Multi') or inter.geom_type == 'GeometryCollection':\n",
    "                all_points.update(g for g in inter.geoms if g.geom_type == 'Point')\n",
    "\n",
    "    return list(all_points)\n",
    "\n",
    "\n",
    "def split_lines_by_intersections_fast(gdf, intersections, buffer_eps=1e-6):\n",
    "    \"\"\"\n",
    "    Splits line geometries at intersection points using a buffered boundary union.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame containing line geometries.\n",
    "        intersections (list of Point): Points where lines should be split.\n",
    "        buffer_eps (float): Buffer radius to create boundaries around each intersection point for splitting.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: New GeoDataFrame with split line segments, preserving 'name' and 'highway' attributes.\n",
    "    \"\"\"\n",
    "    if not intersections:\n",
    "        return gdf.copy()\n",
    "\n",
    "    # Build splitting geometry\n",
    "    splitters = unary_union([pt.buffer(buffer_eps).boundary for pt in intersections])\n",
    "    prepared_splitters = prep(splitters)\n",
    "\n",
    "    records = []\n",
    "    for row in gdf.itertuples(index=False):\n",
    "        geom = row.geometry\n",
    "\n",
    "        if prepared_splitters.intersects(geom):\n",
    "            try:\n",
    "                split_parts = split(geom, splitters)\n",
    "                for part in split_parts.geoms:\n",
    "                    if part.length > 0:\n",
    "                        records.append({\n",
    "                            'name': getattr(row, 'name', None),\n",
    "                            'highway': getattr(row, 'highway', None),\n",
    "                            'geometry': part\n",
    "                        })\n",
    "            except Exception:\n",
    "                # Fallback in case of split failure\n",
    "                records.append({\n",
    "                    'name': getattr(row, 'name', None),\n",
    "                    'highway': getattr(row, 'highway', None),\n",
    "                    'geometry': geom\n",
    "                })\n",
    "        else:\n",
    "            records.append({\n",
    "                'name': getattr(row, 'name', None),\n",
    "                'highway': getattr(row, 'highway', None),\n",
    "                'geometry': geom\n",
    "            })\n",
    "\n",
    "    temp_geo = gpd.GeoDataFrame(records, crs=gdf.crs)\n",
    "    return temp_geo[temp_geo.length >1e-4]\n"
   ],
   "id": "80bb1fb7ccc2fd31",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:33:13.992368Z",
     "start_time": "2025-06-30T10:33:09.294630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gdf_roads = download_osm_roads_bbox(place)[['highway','name','osmid','junction','geometry']].to_crs(project_crs )\n",
    "gdf_roads.to_file(f\"{data_folder}/osm_data.shp\")"
   ],
   "id": "8032fb683961c701",
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:33:14.600122Z",
     "start_time": "2025-06-30T10:33:14.264466Z"
    }
   },
   "cell_type": "code",
   "source": "gdf_roads= gpd.read_file(f'{data_folder}/osm_data.shp')",
   "id": "76dc07777375986b",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:34:46.434026Z",
     "start_time": "2025-06-30T10:33:14.826487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create preprocessing object and folder\n",
    "gdf_roads = remove_unconnected_streets(gdf_roads.copy())\n",
    "roundabout = gdf_roads[gdf_roads['junction'].isin(['roundabout', 'circular','mini_roundabout'])]\n",
    "gdf_edges = gdf_roads[~gdf_roads['junction'].isin(['roundabout', 'circular','mini_roundabout'])]\n",
    "# Load and filter the edges from the saved GeoPackage\n",
    "\n",
    "gdf_edges = gdf_edges[~gdf_edges['highway'].str.contains('link', case=False, na=False)]\n",
    "gdf_edges = gdf_edges.dropna(subset=['name'])\n",
    "gdf_edges = gdf_edges[gdf_edges['name'] != '']\n",
    "# Step 1: Merge connected lines with same 'name' and 'highway'\n",
    "gdf_merge= merge_lines_by_attributes(gdf_edges, attrs=['name', 'highway'])\n",
    "gdf_merge = gdf_merge[gdf_merge.geometry.notnull()]\n",
    "\n",
    "# Step 2: Build internal intersection points efficiently\n",
    "internal_points = extract_internal_intersections_spatial_index(list(gdf_merge.geometry))\n",
    "\n",
    "# Step 3: Split merged lines by these points\n",
    "first_results = split_lines_by_intersections_fast(gdf_merge, internal_points)\n",
    "first_results['bearing'] = first_results['geometry'].apply(calculate_bearing)\n",
    "first_results['angle'] = first_results['bearing'].apply(lambda x: x if x < 180 else x - 180)\n",
    "first_results['length'] = first_results.length\n",
    "first_results.to_file(f'{data_folder}/edges.shp')"
   ],
   "id": "af1c21a96babc70",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging lines: 100%|██████████| 4293/4293 [00:00<00:00, 7757.03it/s]\n",
      "Extracting intersections: 100%|██████████| 9210/9210 [00:01<00:00, 5507.75it/s]\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:37:22.606459Z",
     "start_time": "2025-06-30T10:37:22.429123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_deadend_points(gdf_edges):\n",
    "    \"\"\"\n",
    "    Identifies all dead-end points (start or end points that appear only once in the entire edge dataset).\n",
    "\n",
    "    Parameters:\n",
    "        gdf_edges (GeoDataFrame): Edge lines with LineString geometries.\n",
    "\n",
    "    Returns:\n",
    "        Set of Points: All dead-end points.\n",
    "    \"\"\"\n",
    "    endpoint_counter = defaultdict(int)\n",
    "\n",
    "    for geom in gdf_edges.geometry:\n",
    "        coords = list(geom.coords)\n",
    "        endpoint_counter[coords[0]] += 1\n",
    "        endpoint_counter[coords[-1]] += 1\n",
    "    return endpoint_counter\n",
    "\n",
    "def build_street_connection_dict(gdf_streets):\n",
    "    \"\"\"\n",
    "    Build a dictionary of street-to-street connections via unique intersection points.\n",
    "\n",
    "    Parameters:\n",
    "        gdf_streets (GeoDataFrame): A GeoDataFrame containing street segments, with a 'name' column.\n",
    "\n",
    "    Returns:\n",
    "        connection_dict (dict): A dictionary where each key is a tuple of two street names (str, str),\n",
    "                                and each value is a list of unique shapely Points where the streets intersect.\n",
    "    \"\"\"\n",
    "    # Ensure clean index\n",
    "    gdf = gdf_streets.reset_index(drop=True)\n",
    "\n",
    "    # Step 1: Perform spatial join to find intersecting features\n",
    "    joined = gpd.sjoin(gdf, gdf, how='inner', predicate='intersects')\n",
    "    joined = joined[joined.index != joined['index_right']]  # Remove self-intersections\n",
    "\n",
    "    # Step 2: Initialize output containers\n",
    "    connection_dict = defaultdict(list)\n",
    "    seen_edges = set()\n",
    "\n",
    "    # Step 3: Process each intersecting pair\n",
    "    for idx, row in tqdm(joined.iterrows(), total=len(joined)):\n",
    "        idx2 = row['index_right']\n",
    "        name1 = row['name_left']\n",
    "        name2 = row['name_right']\n",
    "\n",
    "        # Skip connections within the same street\n",
    "        if name1 == name2:\n",
    "            continue\n",
    "\n",
    "        pair_key = tuple(sorted((name1, name2)))\n",
    "        geom1 = row.geometry\n",
    "        geom2 = gdf.loc[idx2].geometry\n",
    "        intersection = geom1.intersection(geom2)\n",
    "\n",
    "        # Skip if no actual intersection\n",
    "        if intersection.is_empty:\n",
    "            continue\n",
    "\n",
    "        # Normalize intersection geometry into list of points\n",
    "        if intersection.geom_type == 'Point':\n",
    "            inter_points = [intersection]\n",
    "        elif intersection.geom_type == 'MultiPoint':\n",
    "            inter_points = list(intersection.geoms)\n",
    "        else:\n",
    "            continue  # Ignore non-point intersections\n",
    "\n",
    "        # Step 4: Store only new intersection points\n",
    "        existing_coords = {tuple(pt.coords)[0] for pt in connection_dict[pair_key]}\n",
    "        new_points = [pt for pt in inter_points if tuple(pt.coords)[0] not in existing_coords]\n",
    "\n",
    "        if new_points:\n",
    "            connection_dict[pair_key].extend(new_points)\n",
    "            seen_edges.add(pair_key)\n",
    "\n",
    "    return connection_dict\n",
    "conn_dict = build_street_connection_dict(first_results)"
   ],
   "id": "205b17d5487d47ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 17706.34it/s]\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:37:33.505616Z",
     "start_time": "2025-06-30T10:37:33.328925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merge roundabout geometries, identify which streets intersect them, and extract roundabout center points for further spatial analysis.\n",
    "\n",
    "\n",
    "# --------------------`----------------------------------------\n",
    "# STEP 1: Merge roundabout edges into clean geometries\n",
    "# - Union geometries to dissolve boundaries\n",
    "# - Linemerge to connect continuous segments\n",
    "# - Export merged geometries as shapefile\n",
    "# ------------------------------------------------------------\n",
    "roundabout_edges = roundabout.copy()  # GeoDataFrame with roundabout segments\n",
    "\n",
    "# Merge all geometries into a MultiLineString\n",
    "merged_geom = unary_union(roundabout_edges.geometry)\n",
    "\n",
    "# Connect continuous line segments\n",
    "connected_lines = linemerge(merged_geom)\n",
    "\n",
    "# Normalize to list of LineStrings\n",
    "if isinstance(connected_lines, LineString):\n",
    "    lines_list = [connected_lines]\n",
    "elif isinstance(connected_lines, MultiLineString):\n",
    "    lines_list = list(connected_lines.geoms)\n",
    "else:\n",
    "    lines_list = []\n",
    "\n",
    "# Save merged roundabout geometries for inspection\n",
    "gdf_roundabout = gpd.GeoDataFrame(geometry=lines_list, crs=roundabout_edges.crs)\n",
    "gdf_roundabout.to_file(f'{data_folder}/roundabout.shp')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 2: Create a buffer around merged roundabouts and find intersecting street edges\n",
    "# ------------------------------------------------------------\n",
    "# Buffer each merged roundabout line (e.g., 5 meters)\n",
    "roundabout_buffer = gdf_roundabout.copy()\n",
    "roundabout_buffer['geometry'] = roundabout_buffer.geometry.buffer(5)\n",
    "\n",
    "# Perform spatial join: find street edges that intersect the roundabout buffer\n",
    "intersections = gpd.sjoin(first_results, roundabout_buffer, how='inner', predicate='intersects')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 3: Build dictionary mapping roundabout index to connected street names\n",
    "# ------------------------------------------------------------\n",
    "roundabout_to_streets = defaultdict(set)\n",
    "\n",
    "for _, row in intersections.iterrows():\n",
    "    roundabout_idx = row['index_right']  # index of the matched roundabout in gdf_merged\n",
    "    street_name = row['name']\n",
    "    roundabout_to_streets[roundabout_idx].add(street_name)\n",
    "\n",
    "# Save the original roundabout edge segments for reference\n",
    "roundabout_edges.to_file(f'{data_folder}/roundabout_edges.shp')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 4: Calculate center point for each roundabout polygon buffer\n",
    "# - Buffer again with clean style, then extract centroid\n",
    "# ------------------------------------------------------------\n",
    "# Buffer with styling for rounded shapes, then get centroids\n",
    "roundabout_centers = gdf_roundabout['geometry'].buffer(distance=1, cap_style=1, join_style=1).centroid\n",
    "\n",
    "# Compute distance from centroid to the boundary of the buffer\n",
    "distances = [\n",
    "    centroid.distance(line)\n",
    "    for centroid, line in zip(roundabout_centers, gdf_roundabout['geometry'])\n",
    "]\n",
    "\n",
    "# Save as GeoDataFrame\n",
    "gdf_roundabouts = gpd.GeoDataFrame(geometry=roundabout_centers, crs=roundabout_edges.crs)\n",
    "# Compute distance from centroid to buffer boundary\n",
    "gdf_roundabouts['dist'] = distances\n",
    "gdf_roundabouts.reset_index(drop=True).to_file(f'{data_folder}/center_roundabout.shp')\n",
    "\n"
   ],
   "id": "ae9c08b2c555ecfa",
   "outputs": [],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:41:43.203513Z",
     "start_time": "2025-06-30T10:38:37.505652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# This code performs the main part of the simplification\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def check_parallelism(to_translate: GeoDataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether a group of line segments contain any parallel segments\n",
    "    by offsetting each line and checking buffer intersections.\n",
    "    \"\"\"\n",
    "    my_buffer = to_translate['geometry'].buffer(cap_style=2, distance=30, join_style=3)\n",
    "    to_translate['geometry_right'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'right'))\n",
    "    to_translate['geometry_left'] = to_translate['geometry'].apply(lambda x: x.parallel_offset(35, 'left'))\n",
    "\n",
    "    def is_parallel(my_s_join: GeoDataFrame, the_buffer, geo_field: str):\n",
    "        my_s_join['geometry'] = my_s_join[geo_field]\n",
    "        sjoin = my_s_join.sjoin(GeoDataFrame(geometry=the_buffer, crs=project_crs), how='inner')\n",
    "        sjoin = sjoin[sjoin.index != sjoin['index_right']]\n",
    "        for _, row in sjoin.iterrows():\n",
    "            overlay = gpd.overlay(\n",
    "                GeoDataFrame([row], crs=project_crs),\n",
    "                GeoDataFrame(geometry=[the_buffer[row['index_right']]], crs=project_crs),\n",
    "                how='intersection')\n",
    "            if (overlay.length / row.geometry.length).iloc[0] * 100 > 10:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    return is_parallel(to_translate, my_buffer, 'geometry_right') or is_parallel(to_translate, my_buffer, 'geometry_left')\n",
    "\n",
    "def circular_distance(angle1, angle2):\n",
    "    \"\"\"Compute minimum circular angle difference between two angles.\"\"\"\n",
    "    diff = np.abs(angle1 - angle2) % 180\n",
    "    return np.minimum(diff, 180 - diff)\n",
    "\n",
    "def add_more_pnts_to_new_lines(pnt_f, pnt_l, line_pnts, lngth_chck, test_poly):\n",
    "    \"\"\"\n",
    "    Recursively add points along a line if they're more than 10m from existing road segments.\n",
    "    \"\"\"\n",
    "    dist = pnt_f.distance(pnt_l)\n",
    "    x0, y0 = pnt_f.x, pnt_f.y\n",
    "    bearing = math.atan2(pnt_l.x - x0, pnt_l.y - y0)\n",
    "    if bearing < 0:\n",
    "        bearing += 2 * math.pi\n",
    "    loops = int(dist / lngth_chck)\n",
    "\n",
    "    for step in range(1, loops):\n",
    "        x_new = x0 + lngth_chck * step * math.sin(bearing)\n",
    "        y_new = y0 + lngth_chck * step * math.cos(bearing)\n",
    "        new_point = Point(x_new, y_new)\n",
    "        nearest = GeoDataFrame(geometry=[new_point], crs=project_crs).sjoin_nearest(test_poly, distance_col='dis').iloc[0]\n",
    "        if nearest['dis'] > 10:\n",
    "            line = data.loc[nearest['index_right']]['geometry']\n",
    "            projected = line.interpolate(line.project(new_point))\n",
    "            if projected.distance(pnt_f) < 10:\n",
    "                continue\n",
    "            line_pnts.append(projected)\n",
    "            return add_more_pnts_to_new_lines(projected, pnt_l, line_pnts, lngth_chck, test_poly)\n",
    "    return line_pnts\n",
    "\n",
    "def create_center_line(one_poly):\n",
    "    \"\"\"\n",
    "    Construct a center line through a polygon by identifying its farthest endpoints\n",
    "    and interpolating new points as needed based on angular continuity.\n",
    "    \"\"\"\n",
    "    lines = data.sjoin(GeoDataFrame(geometry=[one_poly], crs=project_crs)).drop(columns='index_right')\n",
    "    endpoints = []\n",
    "    lines['geometry'].apply(lambda line: endpoints.extend([Point(line.coords[0]), Point(line.coords[-1])]))\n",
    "    combos = list(combinations(endpoints, 2))\n",
    "\n",
    "    df = DataFrame({\n",
    "        'point_1': [a for a, _ in combos],\n",
    "        'point_2': [b for _, b in combos],\n",
    "    })\n",
    "    df['dist'] = df.apply(lambda x: x['point_1'].distance(x['point_2']), axis=1)\n",
    "    dx = df['point_2'].apply(lambda p: p.x) - df['point_1'].apply(lambda p: p.x)\n",
    "    dy = df['point_2'].apply(lambda p: p.y) - df['point_1'].apply(lambda p: p.y)\n",
    "    df['angle'] = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "    avg_angle = lines['angle'].mean()\n",
    "    df['ratio'] = df['dist'] / df['dist'].max() + 0.5 * np.abs(df['angle'] - avg_angle) / np.abs(df['angle'] - avg_angle).max()\n",
    "\n",
    "    pnt_f, pnt_l = df.sort_values(by='ratio', ascending=False).iloc[0][['point_1', 'point_2']]\n",
    "    angle_range = lines['angle'].max() - lines['angle'].min()\n",
    "\n",
    "    if angle_range < 1:\n",
    "        new_line_pts = [pnt_f]\n",
    "    else:\n",
    "        step = 8.5 if angle_range > 100 else 75 - log2(angle_range) * 10\n",
    "        new_line_pts = add_more_pnts_to_new_lines(pnt_f, pnt_l, [pnt_f], step, lines)\n",
    "    new_line_pts.append(pnt_l)\n",
    "    return new_line_pts\n",
    "\n",
    "def update_df_with_center_line(new_line, is_simplified=0, group_name=-1):\n",
    "    \"\"\"Append a new line feature to the output dictionary.\"\"\"\n",
    "    dic_final['name'].append(name)\n",
    "    dic_final['geometry'].append(new_line)\n",
    "    dic_final['highway'].append(data.iloc[0]['highway'])\n",
    "    dic_final['bearing'].append(data['angle'].mean())\n",
    "    dic_final['group'].append(group_name)\n",
    "    dic_final['is_simplified'].append(is_simplified)\n",
    "\n",
    "# --- Main logic ---\n",
    "dic_final = {'name': [], 'geometry': [], 'highway': [], 'bearing': [], 'group': [], 'is_simplified': []}\n",
    "df_pro = first_results.copy()\n",
    "grouped = df_pro.groupby('name')\n",
    "\n",
    "with tqdm(total=len(grouped)) as pbar:\n",
    "    for name, group_df in grouped:\n",
    "        pbar.update(1)\n",
    "        group_df = group_df.dropna(subset=['angle'])\n",
    "        if len(group_df) < 2:\n",
    "            data = group_df\n",
    "            _ = group_df['geometry'].apply(lambda geom: update_df_with_center_line(geom))\n",
    "            continue\n",
    "\n",
    "        angles = group_df['angle'].to_numpy()\n",
    "        dists = np.array([[circular_distance(a1, a2) for a2 in angles] for a1 in angles])\n",
    "        dbscan = DBSCAN(eps=10, min_samples=2, metric='precomputed')\n",
    "        group_df['group'] = dbscan.fit_predict(dists)\n",
    "\n",
    "        if (group_df['group'] == -1).all():\n",
    "            data = group_df\n",
    "            _ = group_df['geometry'].apply(lambda geom: update_df_with_center_line(geom))\n",
    "            continue\n",
    "\n",
    "        for group_id, sub_group in group_df.groupby('group'):\n",
    "            data = sub_group\n",
    "            if group_id == -1:\n",
    "                _ = data['geometry'].apply(lambda geom: update_df_with_center_line(geom))\n",
    "                continue\n",
    "            if check_parallelism(data.copy()):\n",
    "                min_polylines = len(data) / 15\n",
    "                condition = (data['highway'].isin(['service', 'unclassified'])) & (\n",
    "                    data.groupby('highway')['highway'].transform('count') <= min_polylines)\n",
    "                data = data[~condition]\n",
    "\n",
    "                buffers = data.buffer(cap_style=3, distance=30, join_style=3)\n",
    "                unified = buffers.unary_union\n",
    "\n",
    "                if isinstance(unified, MultiPolygon):\n",
    "                    for poly in unified.geoms:\n",
    "                        center_pts = create_center_line(poly)\n",
    "                        update_df_with_center_line(LineString(center_pts), 1, group_id)\n",
    "                else:\n",
    "                    center_pts = create_center_line(unified)\n",
    "                    update_df_with_center_line(LineString(center_pts), 1, group_id)\n",
    "            else:\n",
    "                _ = data['geometry'].apply(lambda geom: update_df_with_center_line(geom))\n",
    "\n",
    "# Finalize and export\n",
    "print(f'number_of_parallel: {sum(dic_final[\"is_simplified\"])}')\n",
    "print('create new files')\n",
    "new_network = GeoDataFrame(dic_final, crs=project_crs)\n",
    "new_network['length'] = new_network.length\n",
    "new_network.to_file(f'{data_folder}/simp.shp')\n"
   ],
   "id": "6a641c47de411326",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3177/3177 [03:05<00:00, 17.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_parallel: 1253\n",
      "create new files\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:37:37.957489Z",
     "start_time": "2025-06-30T10:37:37.700479Z"
    }
   },
   "cell_type": "code",
   "source": "new_network = gpd.read_file(f'{data_folder}/simp.shp')",
   "id": "8f0cb829a822b21f",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:52:45.689637Z",
     "start_time": "2025-06-30T10:52:38.728315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Simplify and Clean Network\n",
    "# -----------------------------\n",
    "\n",
    "def simplify_and_clean_street_network(new_network, roundabout_to_streets):\n",
    "    \"\"\"\n",
    "    Cleans and simplifies a street network by:\n",
    "    - Removing short terminal lines that are weakly connected.\n",
    "    - Ensuring intersections occur at endpoints.\n",
    "    - Eliminating redundant coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        new_network (GeoDataFrame): The initial simplified network.\n",
    "        roundabout_to_streets (dict): Mapping of roundabout IDs to connected street names.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Cleaned and simplified network.\n",
    "    \"\"\"\n",
    "\n",
    "    gdf = new_network.copy()\n",
    "    # gdf =gdf[gdf.length>10]\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 1: Precompute useful attributes\n",
    "    # --------------------------------------------\n",
    "\n",
    "    gdf['name_count'] = gdf['name'].map(gdf['name'].value_counts())\n",
    "\n",
    "    # Count endpoint appearances across all geometries\n",
    "    endpoint_counts = defaultdict(int)\n",
    "    for geom in gdf.geometry:\n",
    "        coords = list(geom.coords)\n",
    "        endpoint_counts[coords[0]] += 1\n",
    "        endpoint_counts[coords[-1]] += 1\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 2: Filter for lines connected to roundabouts\n",
    "    # --------------------------------------------\n",
    "\n",
    "    connected_streets = {name for names in roundabout_to_streets.values() for name in names}\n",
    "    gdf_connected = gdf[gdf['name'].isin(connected_streets)].copy()\n",
    "\n",
    "    # Flag short lines with only one connected endpoint\n",
    "    lines_to_remove = set(gdf_connected[\n",
    "        (gdf_connected['length'] < 30) &\n",
    "        (gdf_connected['name_count'] > 1) &\n",
    "        (gdf_connected.geometry.apply(lambda geom: endpoint_counts[geom.coords[0]] == 1 and\n",
    "                                                      endpoint_counts[geom.coords[-1]] == 1))\n",
    "    ].index)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 3: Identify short, isolated terminal lines\n",
    "    # --------------------------------------------\n",
    "\n",
    "    candidates = gdf_connected[\n",
    "        (gdf_connected['length'] < 100) &\n",
    "        (gdf_connected['name_count'] > 1) &\n",
    "        (gdf_connected.geometry.apply(lambda geom: endpoint_counts[geom.coords[0]] == 1 and\n",
    "                                                      endpoint_counts[geom.coords[-1]] == 1))\n",
    "    ].copy()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 4: Find actual intersections (excluding self-intersection)\n",
    "    # --------------------------------------------\n",
    "\n",
    "    intersections = gpd.sjoin(candidates, gdf, how='inner', predicate='intersects')\n",
    "    intersections = intersections[intersections.index != intersections['index_right']]\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 5: Determine which candidates are not validly connected\n",
    "    # --------------------------------------------\n",
    "\n",
    "    for idx, row in candidates.iterrows():\n",
    "        temp_line = row.geometry\n",
    "        start, end = Point(temp_line.coords[0]), Point(temp_line.coords[-1])\n",
    "        matching = intersections.loc[intersections.index == idx]\n",
    "\n",
    "        if matching.empty:\n",
    "            lines_to_remove.add(idx)\n",
    "            continue\n",
    "\n",
    "        # Check if intersection occurs exactly at an endpoint\n",
    "        valid = False\n",
    "        for other_idx in matching['index_right']:\n",
    "            inter = temp_line.intersection(gdf.loc[other_idx].geometry)\n",
    "            if inter.is_empty:\n",
    "                continue\n",
    "\n",
    "            # Normalize to list of points\n",
    "            inter_points = [inter] if isinstance(inter, Point) else list(inter.geoms)\n",
    "            if any(pt.equals(start) or pt.equals(end) for pt in inter_points):\n",
    "                valid = True\n",
    "                break\n",
    "\n",
    "        if not valid:\n",
    "            lines_to_remove.add(idx)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Step 6: Remove flagged lines and clean geometry\n",
    "    # --------------------------------------------\n",
    "\n",
    "    gdf_simplified = gdf.drop(index=lines_to_remove).copy()\n",
    "\n",
    "    # Remove duplicate consecutive points in LineString\n",
    "    gdf_simplified['geometry'] = gdf_simplified['geometry'].apply(\n",
    "        lambda geom: LineString([pt for pt, _ in groupby(geom.coords)]) if len(set(geom.coords)) > 1 else None\n",
    "    )\n",
    "\n",
    "    # Drop lines with invalid or null geometries\n",
    "    gdf_simplified = gdf_simplified[gdf_simplified['geometry'].notnull()].copy()\n",
    "\n",
    "    return gdf_simplified\n",
    "gdf_edges = merge_lines_by_attributes(new_network.copy(), attrs=['name', 'highway'])\n",
    "\n",
    "gdf_edges = gdf_edges[gdf_edges.geometry.notnull()]\n",
    "\n",
    "# Step 2: Build internal intersection points efficiently\n",
    "internal_points = extract_internal_intersections_spatial_index(list(gdf_edges.geometry))\n",
    "\n",
    "# Step 3: Split merged lines by these points\n",
    "gdf_edges = split_lines_by_intersections_fast(gdf_edges, internal_points)\n",
    "\n",
    "gdf_edges['length'] = gdf_edges.length\n",
    "gdf_simplified = simplify_and_clean_street_network(gdf_edges, roundabout_to_streets)\n",
    "gdf_simplified.to_file(f'{test_folder}/gdf_simplified_fix0.shp')"
   ],
   "id": "e00ecb6a75fea371",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging lines: 100%|██████████| 3557/3557 [00:00<00:00, 4663.83it/s]\n",
      "Extracting intersections: 100%|██████████| 14724/14724 [00:00<00:00, 23043.25it/s]\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:53:50.329672Z",
     "start_time": "2025-06-30T10:53:47.158816Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # This code is enhancing the geometry of a street network by snapping street segments to nearby roundabouts to ensure cleaner, more topologically correct connections.\n",
    "\n",
    "# --- Parameters ---\n",
    "  # Threshold to treat the roundabout as coincident with a vertex\n",
    "\n",
    "# --- Working copy ---\n",
    "gdf_simplified_fix1 = gdf_simplified.copy()\n",
    "existing_connections = defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "# --- Iterate roundabouts and their connected streets ---\n",
    "for ridx, streets in roundabout_to_streets.items():\n",
    "    roundabout_point = gdf_roundabouts.geometry.iloc[ridx]\n",
    "    roundabout_dist = gdf_roundabouts['dist'].iloc[ridx]\n",
    "    TOLERANCE_NEAR_VERTEX = roundabout_dist *1.5\n",
    "    for street in streets:\n",
    "        street_edges = gdf_simplified_fix1[gdf_simplified_fix1['name'] == street]\n",
    "        if street_edges.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute distances from each edge to the roundabout\n",
    "        dists = street_edges.geometry.distance(roundabout_point)\n",
    "        min_dist = dists.min()\n",
    "        closest_edges = street_edges[dists == min_dist]\n",
    "\n",
    "        for edge_idx, edge_row in closest_edges.iterrows():\n",
    "            line = edge_row.geometry\n",
    "            new_coords = list(line.coords)\n",
    "\n",
    "            # Project the roundabout point onto the line\n",
    "            nearest_pt = line.interpolate(line.project(roundabout_point))\n",
    "            p_start = Point(new_coords[0])\n",
    "            p_end = Point(new_coords[-1])\n",
    "\n",
    "            round_coord = roundabout_point.coords[0]\n",
    "\n",
    "            # --- Insertion logic ---\n",
    "            # Find the closest existing vertex on the line\n",
    "            insert_idx = np.argmin([Point(c).distance(nearest_pt) for c in new_coords])\n",
    "            rpt = Point(new_coords[insert_idx])\n",
    "            def dist(p1, p2):\n",
    "                return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "            dist_to_start = dist(new_coords[insert_idx], new_coords[0])\n",
    "            dist_to_end = dist(new_coords[insert_idx], new_coords[-1])\n",
    "            # Efficient conditional logic\n",
    "            if dist_to_start < 30 and dist_to_end < 30:\n",
    "                if dist_to_start < dist_to_end:\n",
    "                    rpt, insert_idx = p_start, 0\n",
    "                else:\n",
    "                    rpt, insert_idx = p_end, len(new_coords) - 1\n",
    "            elif dist_to_start < 30:\n",
    "                rpt, insert_idx = p_start, 0\n",
    "            elif dist_to_end < 30:\n",
    "                rpt, insert_idx = p_end, len(new_coords) - 1\n",
    "            if insert_idx == 0 and roundabout_point.distance(rpt) < TOLERANCE_NEAR_VERTEX and all(not geom.contains(rpt) for geom in gdf_roundabouts.geometry.buffer(1)):\n",
    "                    new_coords = [round_coord] + new_coords[1:]\n",
    "            elif insert_idx == len(new_coords) - 1 and roundabout_point.distance(rpt) < TOLERANCE_NEAR_VERTEX and all(not geom.contains(rpt) for geom in gdf_roundabouts.geometry.buffer(1)):\n",
    "                    new_coords = new_coords[:-1] + [round_coord]\n",
    "            elif roundabout_point.distance(rpt) < TOLERANCE_NEAR_VERTEX and all(not geom.contains(rpt) for geom in gdf_roundabouts.geometry.buffer(1)):\n",
    "                new_coords = new_coords[:insert_idx] + [round_coord] + new_coords[insert_idx + 1:]\n",
    "            else:\n",
    "                # Check angle between segments to ensure smooth connection\n",
    "                def bearing(p1, p2):\n",
    "                    dx, dy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "                    return np.degrees(np.arctan2(dy, dx)) % 360\n",
    "                if insert_idx==0:\n",
    "                    az1 = bearing(new_coords[insert_idx], new_coords[insert_idx +1])\n",
    "                    az2 = bearing(new_coords[insert_idx], round_coord)\n",
    "                else:\n",
    "                    az1 = bearing(new_coords[insert_idx - 1], new_coords[insert_idx])\n",
    "                    az2 = bearing(new_coords[insert_idx], round_coord)\n",
    "                angle_diff = abs((az1 - az2 + 180) % 360 - 180)\n",
    "                if 110 <= angle_diff <= 250:\n",
    "                    new_coords = new_coords[:insert_idx] + [round_coord] + new_coords[insert_idx:]\n",
    "                else:\n",
    "                    new_coords = new_coords[:insert_idx + 1] + [round_coord] + new_coords[insert_idx + 1:]\n",
    "\n",
    "\n",
    "            gdf_simplified_fix1.at[edge_idx,'geometry'] = LineString(new_coords)\n",
    "            existing_connections[(ridx,street)].append(edge_idx)\n",
    "\n",
    "# --- Apply updated geometries to GeoDataFrame ---\n",
    "gdf_simplified_fix1.drop(columns='coords', inplace=True, errors='ignore')\n",
    "gdf_simplified_fix1.to_file(f'{test_folder}/gdf_simplified_fix1.shp')\n"
   ],
   "id": "47170e42b4d211c4",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:41:57.741919Z",
     "start_time": "2025-06-30T10:41:57.737930Z"
    }
   },
   "cell_type": "code",
   "source": "### TEST AREA ####",
   "id": "d9f315a7b550573e",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:41:58.009641Z",
     "start_time": "2025-06-30T10:41:58.006974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "### TEST AREA  END###"
   ],
   "id": "3ea8a8b70f61cad3",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T13:58:35.744562Z",
     "start_time": "2025-06-30T13:58:33.582171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_azimuth(p1, p2):\n",
    "    dx, dy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return np.degrees(np.arctan2(dy, dx)) % 360\n",
    "\n",
    "def angle_difference(az1, az2):\n",
    "    diff = abs(az1 - az2)\n",
    "    return min(diff, 360 - diff)\n",
    "def trim_roundabout_point(coords, rcoord,test_all=True):\n",
    "    \"\"\"\n",
    "    Removes the roundabout coordinate from the coordinate list if it exists.\n",
    "    This function supports cases where the roundabout is not necessarily at the start or end.\n",
    "    \"\"\"\n",
    "    if len(coords) < 3:\n",
    "        return coords\n",
    "    if test_all:\n",
    "        return [pt for pt in coords if pt != rcoord]\n",
    "    if coords[0] == rcoord:\n",
    "        return coords[1:]\n",
    "    elif coords[-1] == rcoord:\n",
    "        return coords[:-1]\n",
    "    print('error')\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "def snap_dead_ends_with_angle_filter(\n",
    "    gdf_streets,\n",
    "    gdf_roundabouts,\n",
    "    roundabout_to_streets,\n",
    "    existing_connections,\n",
    "    tolerance=120\n",
    "):\n",
    "    gdf_fixed = gdf_streets.copy()\n",
    "    node_count = defaultdict(int)\n",
    "\n",
    "    for _, row in tqdm(gdf_fixed.iterrows(), total=len(gdf_fixed)):\n",
    "        coords = list(row.geometry.coords)\n",
    "        node_count[coords[0]] += 1\n",
    "        node_count[coords[-1]] += 1\n",
    "    for pt in gdf_roundabouts.geometry:\n",
    "        node_count[pt.coords[0]] += 1\n",
    "\n",
    "    lines_to_drop = set()\n",
    "\n",
    "    for ridx, streets in roundabout_to_streets.items():\n",
    "        roundabout_row = gdf_roundabouts.loc[ridx]\n",
    "        rpt = roundabout_row.geometry\n",
    "        rcoord = rpt.coords[0]\n",
    "        rbuffer = rpt.buffer(roundabout_row['dist'] * 1.5)\n",
    "\n",
    "        for street in streets:\n",
    "            street_edges = gdf_fixed[gdf_fixed[\"name\"] == street]\n",
    "\n",
    "            for idx, row in street_edges.iterrows():\n",
    "                if idx in existing_connections[(ridx, street)]:\n",
    "                    continue\n",
    "\n",
    "                coords = list(row.geometry.coords)\n",
    "                start, end = Point(coords[0]), Point(coords[-1])\n",
    "                snapped = False\n",
    "\n",
    "                if rbuffer.contains(start) and node_count[coords[0]] == 1:\n",
    "                    coords[0] = rcoord\n",
    "                    snapped = True\n",
    "                elif rbuffer.contains(end) and node_count[coords[-1]] == 1:\n",
    "                    coords[-1] = rcoord\n",
    "                    snapped = True\n",
    "\n",
    "                if not snapped:\n",
    "                    continue\n",
    "\n",
    "                new_line = LineString(coords)\n",
    "                new_is_start = coords[0] == rcoord\n",
    "                new_az = compute_azimuth(rcoord, coords[1] if new_is_start else coords[-2])\n",
    "                redundant = False\n",
    "\n",
    "                for ex_id in existing_connections.get((ridx, street), []):\n",
    "                    ex_geom = gdf_fixed.loc[ex_id].geometry\n",
    "                    ex_coords = list(ex_geom.coords)\n",
    "\n",
    "                    # Remove roundabout from new and existing line\n",
    "                    trimmed_new = LineString(trim_roundabout_point(coords, rcoord,False))\n",
    "\n",
    "                    ex_trimmed_coords = trim_roundabout_point(ex_coords, rcoord)\n",
    "                    trimmed_existing = LineString(ex_trimmed_coords)\n",
    "\n",
    "                    # Distance check\n",
    "                    if trimmed_new.distance(trimmed_existing) > 50:\n",
    "                        existing_connections[(ridx, street)].append(idx)\n",
    "                        gdf_fixed.at[idx, 'geometry'] = new_line\n",
    "                    else:\n",
    "                        ex_is_start = ex_coords[0] == rcoord\n",
    "                        ex_az = compute_azimuth(rcoord, ex_coords[1] if ex_is_start else ex_coords[-2])\n",
    "                        az_diff = angle_difference(new_az, ex_az)\n",
    "\n",
    "                        if az_diff < tolerance:\n",
    "                            if new_line.length > ex_geom.length:\n",
    "                                lines_to_drop.add(ex_id)\n",
    "                                existing_connections[(ridx, street)].remove(ex_id)\n",
    "                                existing_connections[(ridx, street)].append(idx)\n",
    "                                gdf_fixed.at[idx, 'geometry'] = new_line\n",
    "                            else:\n",
    "                                lines_to_drop.add(idx)\n",
    "                            redundant = True\n",
    "                            break\n",
    "\n",
    "                if not redundant:\n",
    "                    existing_connections[(ridx, street)].append(idx)\n",
    "                    gdf_fixed.at[idx, 'geometry'] = new_line\n",
    "\n",
    "    return gdf_fixed.drop(index=list(lines_to_drop)).copy()\n",
    "gdf_simplified_fix2= snap_dead_ends_with_angle_filter(gdf_simplified_fix1,gdf_roundabouts,roundabout_to_streets,\n",
    "    existing_connections)\n",
    "gdf_simplified_fix2.to_file(f'{test_folder}/gdf_simplified_fix2.shp')"
   ],
   "id": "3c35fc00edacb7e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16674/16674 [00:00<00:00, 19826.49it/s]\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:01.309814Z",
     "start_time": "2025-06-30T10:42:01.306981Z"
    }
   },
   "cell_type": "code",
   "source": "#### TEST AREA",
   "id": "33581e7a4e65e96",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:01.558148Z",
     "start_time": "2025-06-30T10:42:01.555106Z"
    }
   },
   "cell_type": "code",
   "source": "### TEAT AREA - END",
   "id": "5e32ba624d523711",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:54:03.200418Z",
     "start_time": "2025-06-30T10:54:02.365377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This code removes duplicate street segments (with same endpoints but different geometries) around roundabouts, keeping only the longest one per pair.\n",
    "\n",
    "\n",
    "# Make a working copy of the dataset\n",
    "gdf_simplified_fix3 = gdf_simplified_fix2.copy()\n",
    "\n",
    "# Set to store indices of duplicate edges to remove\n",
    "duplicate_removal_indices = set()\n",
    "\n",
    "# --- Helper function: Normalize edge as unordered pair of start/end points ---\n",
    "def normalize_edge(pt1, pt2):\n",
    "    return tuple(sorted([tuple(pt1), tuple(pt2)]))\n",
    "\n",
    "# --- Step 1: Loop through roundabouts and their connected streets ---\n",
    "for ridx, streets in roundabout_to_streets.items():\n",
    "    # Collect all edges from the connected streets\n",
    "    street_edges = gdf_simplified_fix3[gdf_simplified_fix3['name'].isin(streets)]\n",
    "\n",
    "    # Group edges by normalized endpoint pair (to catch reversed duplicates)\n",
    "    edge_groups = defaultdict(list)\n",
    "\n",
    "    for idx, row in street_edges.iterrows():\n",
    "        coords = list(row.geometry.coords)\n",
    "        edge_key = normalize_edge(coords[0], coords[-1])\n",
    "        edge_groups[edge_key].append((idx, row.geometry.length))\n",
    "\n",
    "    # --- Step 2: Within each group, retain only the longest edge ---\n",
    "    for edges in edge_groups.values():\n",
    "        if len(edges) > 1:\n",
    "            # Sort by length in descending order\n",
    "            edges_sorted = sorted(edges, key=lambda x: -x[1])\n",
    "            # Keep the longest (first); mark others for removal\n",
    "            for edge_idx, _ in edges_sorted[1:]:\n",
    "                duplicate_removal_indices.add(edge_idx)\n",
    "\n",
    "# --- Step 3: Remove marked duplicates and export result ---\n",
    "gdf_simplified_fix3 = gdf_simplified_fix3[~gdf_simplified_fix3.index.isin(duplicate_removal_indices)].copy()\n",
    "gdf_simplified_fix3.to_file(f'{data_folder}/gdf_fix_ra.shp')\n",
    "\n"
   ],
   "id": "5dae4cbf58cc51bc",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:54:05.436514Z",
     "start_time": "2025-06-30T10:54:05.402594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Test for the roundabout\n",
    "joined = gpd.sjoin(gdf_roundabouts, gdf_simplified_fix3, how='left', predicate='intersects')\n",
    "\n",
    "def compare_roundabout_dicts(old_dict, new_dict):\n",
    "    differences = {}\n",
    "\n",
    "    all_keys = set(old_dict.keys()).union(new_dict.keys())\n",
    "\n",
    "    for key in all_keys:\n",
    "        old = set(old_dict.get(key, []))\n",
    "        new = set(new_dict.get(key, []))\n",
    "\n",
    "        if old != new:\n",
    "            differences[key] = {\n",
    "                'added': new - old,\n",
    "                'removed': old - new\n",
    "            }\n",
    "\n",
    "    return differences\n",
    "roundabout_to_streets_new = defaultdict(set)\n",
    "\n",
    "for ridx, row in joined.iterrows():\n",
    "    roundabout_idx = row.name  # index of the roundabout\n",
    "    street_name = row['name']\n",
    "    if pd.notnull(street_name):\n",
    "        roundabout_to_streets_new[roundabout_idx].add(street_name)\n",
    "diffs = compare_roundabout_dicts(roundabout_to_streets, roundabout_to_streets_new)\n",
    "\n",
    "if not diffs:\n",
    "    print(\"✅ Roundabout-street mapping is unchanged.\")\n",
    "else:\n",
    "    print(\"❌ Differences found:\")\n",
    "    for ridx, change in diffs.items():\n",
    "        print(f\"\\nRoundabout {ridx}:\")\n",
    "        if change['added']:\n",
    "            print(f\"  ➕ Streets added: {sorted(change['added'])}\")\n",
    "        if change['removed']:\n",
    "            print(f\"  ➖ Streets removed: {sorted(change['removed'])}\")\n",
    "\n"
   ],
   "id": "58a7d521924b077e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Differences found:\n",
      "\n",
      "Roundabout 51:\n",
      "  ➖ Streets removed: ['Circonvallazione di Borgaretto']\n",
      "\n",
      "Roundabout 59:\n",
      "  ➖ Streets removed: ['Circonvallazione di Borgaretto']\n",
      "\n",
      "Roundabout 107:\n",
      "  ➖ Streets removed: ['Circonvallazione di Borgaretto']\n",
      "\n",
      "Roundabout 155:\n",
      "  ➖ Streets removed: ['Via Pietro Motrassino']\n",
      "\n",
      "Roundabout 178:\n",
      "  ➖ Streets removed: ['Largo Tirreno']\n",
      "\n",
      "Roundabout 225:\n",
      "  ➖ Streets removed: ['Piazza Muzio Scevola']\n",
      "\n",
      "Roundabout 226:\n",
      "  ➖ Streets removed: ['Via Francesco Cirio']\n",
      "\n",
      "Roundabout 227:\n",
      "  ➖ Streets removed: ['Via Francesco Cirio']\n",
      "\n",
      "Roundabout 237:\n",
      "  ➖ Streets removed: ['Strada Comunale del Villaretto']\n",
      "\n",
      "Roundabout 239:\n",
      "  ➖ Streets removed: ['Piazza Carlo Emanuele Secondo']\n",
      "\n",
      "Roundabout 252:\n",
      "  ➖ Streets removed: ['Piazzale Cornelio Tacito']\n",
      "\n",
      "Roundabout 260:\n",
      "  ➖ Streets removed: ['Piazza Antonio Fontanesi']\n",
      "\n",
      "Roundabout 278:\n",
      "  ➖ Streets removed: ['Largo Odoardo Tabacchi']\n",
      "\n",
      "Roundabout 285:\n",
      "  ➖ Streets removed: ['Piazza Giovanni dalle Bande Nere']\n",
      "\n",
      "Roundabout 316:\n",
      "  ➖ Streets removed: ['Piazza Mochino']\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:03.734879Z",
     "start_time": "2025-06-30T10:42:03.271122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conn_dict2 = build_street_connection_dict(gdf_simplified_fix3)\n",
    "\n",
    "def compare_connection_dicts(original_connection_dict, updated_connection_dict):\n",
    "    \"\"\"\n",
    "    Compare two street connection dictionaries. Identify connections that existed in the original\n",
    "    version but are missing in the updated version.\n",
    "\n",
    "    Parameters:\n",
    "        original_connection_dict (dict): { (street1, street2): [Point, ...] }\n",
    "        updated_connection_dict (dict): { (street1, street2): [Point, ...] }\n",
    "\n",
    "    Returns:\n",
    "        missing_connections (dict): connections from the original missing in the updated\n",
    "    \"\"\"\n",
    "    missing_connections = {}\n",
    "\n",
    "    for conn_pair, orig_points in original_connection_dict.items():\n",
    "        if conn_pair not in updated_connection_dict:\n",
    "            missing_connections[conn_pair] = orig_points\n",
    "        else:\n",
    "            # Optionally, you can check if some intersection points are missing\n",
    "            updated_coords = {tuple(pt.coords)[0] for pt in updated_connection_dict[conn_pair]}\n",
    "            missing_pts = [pt for pt in orig_points if tuple(pt.coords)[0] not in updated_coords]\n",
    "            if missing_pts:\n",
    "                missing_connections[conn_pair] = missing_pts\n",
    "\n",
    "    return missing_connections\n",
    "\n",
    "# Example usage\n",
    "missing = compare_connection_dicts(conn_dict, conn_dict2 )\n",
    "\n",
    "# Print or log missing entries\n",
    "for pair, pts in missing.items():\n",
    "    print(f\"Missing connection between {pair[0]} and {pair[1]} at {len(pts)} point(s)\")\n"
   ],
   "id": "530ef3cce64c2bf3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1610/1610 [00:00<00:00, 5464.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing connection between Piazza Madama Cristina and Via Claudio Luigi Berthollet at 1 point(s)\n",
      "Missing connection between Piazza Eugenio Montale and Viale dei Mughetti at 1 point(s)\n",
      "Missing connection between Piazza Dante Livio Bianco and Via Don Giovanni Grioli at 1 point(s)\n",
      "Missing connection between Piazza Corpus Domini and Via Porta Palatina at 1 point(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:03.938243Z",
     "start_time": "2025-06-30T10:42:03.932995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_intersection_points(points, tolerance=70):\n",
    "    \"\"\"\n",
    "    Filters a list of intersection points based on spatial proximity.\n",
    "\n",
    "    Logic:\n",
    "    - If only 1 or 0 points: return as-is with empty representative mapping.\n",
    "    - If 2 points: if close (< tolerance), return one and map both to it.\n",
    "    - If >2: use DBSCAN to group points, select one per group, and store mapping from all grouped points to representative.\n",
    "\n",
    "    Ensures the representative point is not duplicated in its own value list.\n",
    "\n",
    "    Parameters:\n",
    "        points (list of shapely.geometry.Point)\n",
    "        tolerance (float): distance threshold in meters for clustering\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - list of representative points (filtered)\n",
    "            - dict: {representative Point: [all merged Point(s) excluding rep]}\n",
    "    \"\"\"\n",
    "    if len(points) <= 1:\n",
    "        return points, {}\n",
    "\n",
    "    if len(points) == 2:\n",
    "        if points[0].distance(points[1]) < tolerance:\n",
    "            return [points[0]], {points[0]: [points[1]]}\n",
    "        else:\n",
    "            return points, {}\n",
    "\n",
    "    # More than two points → use DBSCAN\n",
    "    coords = np.array([[pt.x, pt.y] for pt in points])\n",
    "    db = DBSCAN(eps=tolerance-10, min_samples=1).fit(coords)\n",
    "    labels = db.labels_\n",
    "\n",
    "    result = []\n",
    "    group_map = defaultdict(list)\n",
    "\n",
    "    label_to_rep = {}\n",
    "    for label in np.unique(labels):\n",
    "        rep_idx = np.where(labels == label)[0][0]\n",
    "        rep = points[rep_idx]\n",
    "        label_to_rep[label] = rep\n",
    "        result.append(rep)\n",
    "\n",
    "    for pt, label in zip(points, labels):\n",
    "        rep = label_to_rep[label]\n",
    "        if pt != rep:\n",
    "            group_map[rep].append(pt)\n",
    "\n",
    "    return result, group_map\n"
   ],
   "id": "3a98156729f68693",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:04.165288Z",
     "start_time": "2025-06-30T10:42:04.158609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def find_nearest_line_from_list(lines: list, point: Point):\n",
    "    tree = STRtree(lines)\n",
    "    nearest_geom = tree.nearest(point)\n",
    "    return nearest_geom\n",
    "\n",
    "def get_closest_endpoint_to_line(line_a: LineString, line_b: LineString):\n",
    "    \"\"\"Returns the endpoint of line_a closest to line_b and its distance.\"\"\"\n",
    "    endpoints = [Point(line_a.coords[0]), Point(line_a.coords[-1])]\n",
    "    distances = [pt.distance(line_b) for pt in endpoints]\n",
    "    return np.argmin(distances),distances[np.argmin(distances)]\n",
    "\n",
    "def extend_line_to_target(line: LineString, endpoint_index: int, target: LineString) -> LineString:\n",
    "    \"\"\"\n",
    "    Extend a line by projecting its start or end point onto a target line.\n",
    "\n",
    "    Parameters:\n",
    "        line (LineString): The line to extend.\n",
    "        endpoint_index (int): Use 0 to extend from start, -1 from end.\n",
    "        target (LineString): The line to project the endpoint onto.\n",
    "\n",
    "    Returns:\n",
    "        LineString: Extended line.\n",
    "    \"\"\"\n",
    "    coords = list(line.coords)\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return line  # Can't extend a single-point line\n",
    "\n",
    "    # Get reference segment\n",
    "    if endpoint_index == 0:\n",
    "        p2 = Point(coords[0])\n",
    "    else:\n",
    "        p2 = Point(coords[-1])\n",
    "\n",
    "    # Get projected point from p2 to target\n",
    "    proj_distance = target.project(p2)\n",
    "    projected_point = target.interpolate(proj_distance)\n",
    "\n",
    "    # Extend line to this projected point\n",
    "    if endpoint_index == 0:\n",
    "        new_coords = [projected_point.coords[0]] + coords\n",
    "    else:\n",
    "        new_coords = coords + [projected_point.coords[0]]\n",
    "\n",
    "    return LineString(new_coords)\n",
    "\n",
    "def connect_lines(line1, line2):\n",
    "    \"\"\"\n",
    "    Returns a modified line1 or line2 extended to intersect with the other.\n",
    "    \"\"\"\n",
    "    lg1  = line1.geometry\n",
    "    lg2 = line2.geometry\n",
    "    idx1, dist1 = get_closest_endpoint_to_line(lg1, lg2)\n",
    "    idx2, dist2 = get_closest_endpoint_to_line(lg2, lg1)\n",
    "\n",
    "    if dist1 < dist2:\n",
    "        return extend_line_to_target(lg1, idx1,lg2), line1.name\n",
    "\n",
    "    else:\n",
    "        return extend_line_to_target(lg2, idx2,lg1), line2.name\n",
    "\n",
    "def connect_nearest_lines(lines1, lines2, point, gdf_streets):\n",
    "    \"\"\"\n",
    "    Finds the nearest line in each set to a given point, connects them using a custom function,\n",
    "    and updates the geometry in gdf_streets.\n",
    "\n",
    "    Parameters:\n",
    "        lines1 (GeoDataFrame): First set of candidate lines (subset of gdf_streets).\n",
    "        lines2 (GeoDataFrame): Second set of candidate lines (subset of gdf_streets).\n",
    "        point (shapely.geometry.Point): The reference point to determine nearest lines.\n",
    "        gdf_streets (GeoDataFrame): The full street dataset to be updated.\n",
    "        connect_lines_func (function): A function that takes two lines and returns (new_line, index_to_update).\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Updated gdf_streets with the new connected line geometry.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_nearest_line(lines, pt):\n",
    "        return lines.iloc[find_nearest_line_from_list(list(lines.geometry), pt)] if len(lines) > 1 else lines.iloc[0]\n",
    "\n",
    "    line1 = get_nearest_line(lines1, point)\n",
    "    line2 = get_nearest_line(lines2, point)\n",
    "    new_line,new_idx = connect_lines(line1,line2)\n",
    "    gdf_streets.at[new_idx, 'geometry'] = new_line\n",
    "    return gdf_streets\n",
    "\n"
   ],
   "id": "981a310159a3447d",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:04.369286Z",
     "start_time": "2025-06-30T10:42:04.364698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def find_nearest_intersections_overlay(gdf1, gdf2, ref_pnt):\n",
    "    \"\"\"\n",
    "    Vectorized approach: find the minimum distance from each gdf1 line to the nearest intersection\n",
    "    with lines in gdf2, measured from a reference point on each gdf1 line.\n",
    "\n",
    "    Parameters:\n",
    "        gdf1 (GeoDataFrame): GeoDataFrame with LineString geometries.\n",
    "        gdf2 (GeoDataFrame): Another GeoDataFrame with LineString geometries.\n",
    "        ref (str): Reference point on gdf1 geometry. Options: 'centroid', 'start', or 'end'.\n",
    "\n",
    "    Returns:\n",
    "        list of float: Distances from each gdf1 feature to its nearest intersection point, or 35 if no intersection.\n",
    "        :param ref_pnt:\n",
    "    \"\"\"\n",
    "\n",
    "    gdf1['geometry'] = gdf1['geometry'].buffer(0.1)\n",
    "    gdf2['geometry'] =  gdf1['geometry'].buffer(0.1)\n",
    "    intersections = gpd.overlay(gdf1, gdf2, how='intersection')\n",
    "\n",
    "\n",
    "    if intersections.empty:\n",
    "        return 35\n",
    "    # Map reference points to intersections\n",
    "        # Compute distance to a given point (pt_coords should be a shapely.geometry.Point)\n",
    "    intersections['distance'] = intersections.geometry.distance(ref_pnt)\n",
    "    return intersections['distance'].min()\n"
   ],
   "id": "676cbd7f6eb32c75",
   "outputs": [],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:04.954455Z",
     "start_time": "2025-06-30T10:42:04.596995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "TOLERANCE_NEAREST = 30\n",
    "\n",
    "# Main loop\n",
    "def snap_missing_points_to_streets(gdf_streets, missing):\n",
    "    for (street1, street2), points_0 in tqdm(missing.items(), total=len(missing)):\n",
    "\n",
    "        points, points_map = filter_intersection_points(points_0)\n",
    "        lines1 = gdf_streets[gdf_streets['name'] == street1].copy()\n",
    "        lines2 = gdf_streets[gdf_streets['name'] == street2].copy()\n",
    "        if len(points)==1:\n",
    "            if len(lines1.sjoin(lines2))>0:\n",
    "                continue\n",
    "            else:\n",
    "                gdf_streets= connect_nearest_lines(lines1, lines2, points[0], gdf_streets)\n",
    "        else:\n",
    "            for pt in points:\n",
    "                if find_nearest_intersections_overlay(lines1.copy(),lines2.copy(),pt) < TOLERANCE_NEAREST:\n",
    "                    continue\n",
    "                else:\n",
    "                    gdf_streets= connect_nearest_lines(lines1.copy(), lines2.copy(), pt, gdf_streets)\n",
    "    return gdf_streets\n",
    "\n",
    "gdf_fixed = snap_missing_points_to_streets(gdf_simplified_fix3.copy(), missing)\n",
    "gdf_fixed.reset_index().to_file(f'{data_folder}/network_final.shp')\n"
   ],
   "id": "d419f1f953ed2bc5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 133.66it/s]\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T10:42:15.243490Z",
     "start_time": "2025-06-30T10:42:05.194552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_edges_by_deadend_criteria(gdf_edges, preserved_old,preserved_new, min_length=30):\n",
    "    \"\"\"\n",
    "    Removes edges that meet the following condition:\n",
    "    - One endpoint is a deadend\n",
    "    - The line is shorter than `min_length`\n",
    "    - That deadend is NOT in the `preserved_deadends` set\n",
    "\n",
    "    Parameters:\n",
    "        gdf_edges (GeoDataFrame): Input edge layer with LineString geometries.\n",
    "        preserved_deadends (set of shapely.geometry.Point): Dead-ends to preserve.\n",
    "        min_length (float): Length threshold below which certain edges may be removed.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Filtered edge layer with unwanted short deadend connections removed.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for idx, row in tqdm(gdf_edges.iterrows(),total=len(gdf_edges)):\n",
    "        geom = row.geometry\n",
    "        coords = list(geom.coords)\n",
    "        p_start = coords[0]\n",
    "        p_end = coords[-1]\n",
    "\n",
    "        # Condition: one deadend, short, and that end not in preserved set\n",
    "        if geom.length < min_length:\n",
    "            if p_start in preserved_new and p_start not in preserved_old:\n",
    "                continue\n",
    "            if p_end  in preserved_new and p_end  not in preserved_old:\n",
    "                continue\n",
    "\n",
    "        # Keep otherwise\n",
    "        filtered.append(row)\n",
    "\n",
    "    return gpd.GeoDataFrame(filtered, columns=gdf_edges.columns, crs=gdf_edges.crs)\n",
    "# ----------------------------\n",
    "# ⚙️ Full Efficient Workflow\n",
    "# ----------------------------\n",
    "\n",
    "# Step 1: Merge connected lines with same 'name' and 'highway'\n",
    "merged_0 = merge_lines_by_attributes(gdf_fixed, attrs=['name', 'highway'])\n",
    "merged = merged_0[merged_0.geometry.notnull()]\n",
    "\n",
    "# Step 2: Build internal intersection points efficiently\n",
    "internal_points = extract_internal_intersections_spatial_index(list(merged.geometry))\n",
    "\n",
    "# Step 3: Split merged lines by these points\n",
    "final_result = split_lines_by_intersections_fast(merged, internal_points)\n",
    "\n",
    "\n",
    "# List of keys with value == 1\n",
    "preserved_deadends_old= [key for key, value in extract_deadend_points(first_results).items() if value == 1]\n",
    "preserved_deadends_new= [key for key, value in extract_deadend_points(final_result).items() if value == 1]\n",
    "new_gdf  =filter_edges_by_deadend_criteria(final_result,preserved_deadends_old,preserved_deadends_new)\n",
    "new_gdf['length'] = new_gdf.length\n",
    "new_gdf.reset_index(drop=True).to_file(f'{data_folder}/network_final2.shp')\n"
   ],
   "id": "a9cbd74df51960f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging lines: 100%|██████████| 3505/3505 [00:01<00:00, 3213.34it/s]\n",
      "Extracting intersections: 100%|██████████| 16603/16603 [00:00<00:00, 48770.16it/s]\n",
      "100%|██████████| 17023/17023 [00:06<00:00, 2549.77it/s]\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Test Area",
   "id": "e9bbfb81dae3144a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:59:59.115961Z",
     "start_time": "2025-06-30T07:59:59.106732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final_result[final_result['name']=='Via Giovanni Somis']\n",
    "idx = 25789\n",
    "min_length = 30\n",
    "row = final_result.loc[idx]\n",
    "geom = row.geometry\n",
    "coords = list(geom.coords)\n",
    "p_start = coords[0]\n",
    "p_end = coords[-1]\n",
    "geom.length < min_length\n",
    "preserved_new= preserved_deadends_new\n",
    "preserved_old= preserved_deadends_old\n",
    "p_end  not in preserved_old"
   ],
   "id": "f1de6b77787baeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T07:57:21.733655Z",
     "start_time": "2025-06-30T07:57:21.727564Z"
    }
   },
   "cell_type": "code",
   "source": "p_end",
   "id": "73a8bd076b9257b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853499.0593547426, 5633341.5780822905)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T09:26:10.782956Z",
     "start_time": "2025-06-30T09:26:09.711882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_deadend_points(gdf_edges):\n",
    "    \"\"\"\n",
    "    Identifies all dead-end points (start or end points that appear only once in the entire edge dataset).\n",
    "\n",
    "    Parameters:\n",
    "        gdf_edges (GeoDataFrame): Edge lines with LineString geometries.\n",
    "\n",
    "    Returns:\n",
    "        Set of Points: All dead-end points.\n",
    "    \"\"\"\n",
    "    endpoint_counter = defaultdict(int)\n",
    "\n",
    "    for geom in gdf_edges.geometry:\n",
    "        coords = list(geom.coords)\n",
    "        endpoint_counter[coords[0]] += 1\n",
    "        endpoint_counter[coords[-1]] += 1\n",
    "    return endpoint_counter\n",
    "test = extract_deadend_points(first_results)\n",
    "test[(853499.0593547426, 5633341.5780822905)]"
   ],
   "id": "8b867afc94305138",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    filtered = []\n",
    "\n",
    "    for idx, row in tqdm(gdf_edges.iterrows(),total=len(gdf_edges)):\n",
    "        geom = row.geometry\n",
    "        coords = list(geom.coords)\n",
    "        p_start = coords[0]\n",
    "        p_end = coords[-1]\n",
    "\n",
    "        # Condition: one deadend, short, and that end not in preserved set\n",
    "        if geom.length < min_length:\n",
    "            if p_start in preserved_new and p_start not in preserved_old:\n",
    "                continue\n",
    "            if p_end  in preserved_new and p_end  not in preserved_old:\n",
    "                continue\n",
    "\n",
    "        # Keep otherwise\n",
    "        filtered.append(row)\n",
    "\n",
    "    return gpd.GeoDataFrame(filtered, columns=gdf_edges.columns, crs=gdf_edges.crs)"
   ],
   "id": "637d715cadaea840"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Test Area - TEST",
   "id": "1ea3c61ed585a1ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
